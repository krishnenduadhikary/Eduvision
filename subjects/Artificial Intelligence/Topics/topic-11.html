<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natural Language Processing (NLP)</title>
    <meta name="description"
        content="Dive into Natural Language Processing: text preprocessing, language models (N-grams, Word2Vec, GloVe, BERT), NER, POS tagging, sentiment analysis, Transformers, chatbots, and text generation.">
    <meta name="keywords"
        content="NLP, Natural Language Processing, Tokenization, Stemming, Lemmatization, N-grams, Word Embeddings, Word2Vec, GloVe, BERT, NER, POS Tagging, Sentiment Analysis, Transformers, Chatbots, Text Generation, AI">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <style>
        /* CSS from topic-1.html - For brevity, imagine all the CSS from the previous example is here */
        /* Key structural CSS will be included, specific element styles might be summarized */
        :root {
            --primary-color-light: #3498db;
            /* Blue */
            --secondary-color-light: #2ecc71;
            /* Green */
            --accent-color-light: #e67e22;
            /* Orange */
            --background-color-light: #f4f7f6;
            --text-color-light: #333;
            --card-bg-light: #ffffff;
            --border-color-light: #e0e0e0;
            --code-bg-light: #eef;

            --primary-color-dark: #5dade2;
            /* Lighter Blue */
            --secondary-color-dark: #58d68d;
            /* Lighter Green */
            --accent-color-dark: #f5b041;
            /* Lighter Orange */
            --background-color-dark: #1e272e;
            --text-color-dark: #f0f0f0;
            --card-bg-dark: #2c3a47;
            --border-color-dark: #444;
            --code-bg-dark: #2a2a40;

            --primary-color: var(--primary-color-light);
            --secondary-color: var(--secondary-color-light);
            --accent-color: var(--accent-color-light);
            --background-color: var(--background-color-light);
            --text-color: var(--text-color-light);
            --card-bg: var(--card-bg-light);
            --border-color: var(--border-color-light);
            --code-bg: var(--code-bg-light);

            --font-sans: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            --font-mono: 'Courier New', Courier, monospace;
            --font-logo: 'Nunito', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-mode {
            --primary-color: var(--primary-color-dark);
            --secondary-color: var(--secondary-color-dark);
            --accent-color: var(--accent-color-dark);
            --background-color: var(--background-color-dark);
            --text-color: var(--text-color-dark);
            --card-bg: var(--card-bg-dark);
            --border-color: var(--border-color-dark);
            --code-bg: var(--code-bg-dark);
        }

        /* Eduvision Logo (Same as topic-1) */
        .eduvishion-logo {
            position: absolute;
            top: 18px;
            left: 18px;
            z-index: 1050;
            text-shadow: 0 2px 12px #6a82fb33;
        }

        .eduvishion-logo .text-2xl {
            font-family: var(--font-logo);
            font-size: 1.7rem;
            font-weight: 900;
            display: flex;
            align-items: center;
            gap: 2px;
            letter-spacing: 0.01em;
        }

        .eduvishion-logo .text-white {
            color: #fff !important;
        }

        .eduvishion-logo .text-yellow-300 {
            color: #fde047 !important;
        }

        .eduvishion-logo .group:hover .text-yellow-300 {
            color: #fef08a !important;
        }

        .eduvishion-logo a {
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 2px;
        }

        .eduvishion-logo svg {
            display: inline-block;
            vertical-align: middle;
            height: 1.5em;
            width: 1.5em;
            margin: 0 2px;
        }

        /* Navbar (Same as topic-1) */
        .navbar {
            background-color: var(--card-bg);
            color: var(--text-color);
            padding: 1rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-left-spacer {
            flex-basis: 50px;
            flex-shrink: 0;
        }

        .navbar-brand {
            font-size: 1.8rem;
            font-weight: bold;
            color: var(--primary-color);
            text-decoration: none;
            text-align: center;
            flex-grow: 1;
        }

        .navbar-brand i {
            margin-right: 0.5rem;
        }

        .theme-toggle {
            cursor: pointer;
            font-size: 1.5rem;
            background: none;
            border: none;
            color: var(--text-color);
            flex-basis: 50px;
            flex-shrink: 0;
            text-align: right;
        }

        /* Sidebar (Same as topic-1) */
        .sidebar {
            position: fixed;
            top: 77px;
            left: 0;
            width: 280px;
            height: calc(100vh - 77px);
            background-color: var(--card-bg);
            padding: 20px;
            overflow-y: auto;
            border-right: 1px solid var(--border-color);
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.05);
        }

        .sidebar h3 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.2rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }

        .sidebar ul {
            list-style: none;
            padding: 0;
        }

        .sidebar ul li a {
            display: block;
            padding: 8px 0;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.2s, padding-left 0.2s, background-color 0.2s;
            border-radius: 4px;
        }

        .sidebar ul li a.sub-item {
            padding-left: 15px;
            font-size: 0.9rem;
        }

        .sidebar ul li a.sub-item:hover {
            padding-left: 25px;
        }

        .sidebar ul li a.sub-item.active {
            padding-left: 25px;
        }

        .sidebar ul li a:hover {
            color: var(--accent-color);
            padding-left: 10px;
        }

        .sidebar ul li a.active {
            color: var(--accent-color);
            padding-left: 10px;
            font-weight: bold;
            background-color: rgba(0, 0, 0, 0.05);
        }

        .dark-mode .sidebar ul li a.active {
            background-color: rgba(255, 255, 255, 0.08);
        }

        /* Main Content Area (Same as topic-1) */
        .main-content {
            margin-left: 300px;
            padding: 2rem 3rem;
        }

        .hero-section {
            text-align: center;
            padding: 4rem 1rem;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .hero-section h1 {
            font-size: 3.5rem;
            margin-bottom: 0.5rem;
        }

        .hero-section p {
            font-size: 1.3rem;
            opacity: 0.9;
        }

        /* Syllabus Bar */
        .syllabus-bar-container {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 0.75rem;
            background-color: var(--card-bg);
            border-radius: 0.5rem;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.07);
            border: 1px solid var(--border-color);
        }

        .syllabus-bar-back-link {
            display: flex;
            align-items: center;
            font-size: 0.875rem;
            color: #2563eb;
            font-weight: 500;
            padding: 0.5rem 0.75rem;
            border-radius: 0.375rem;
            text-decoration: none;
            transition: background-color 0.15s, color 0.15s;
            margin-bottom: 0.5rem;
        }

        .dark-mode .syllabus-bar-back-link {
            color: #5dade2;
        }

        .dark-mode .syllabus-bar-back-link:hover {
            color: #8ecae6;
            background-color: rgba(255, 255, 255, 0.1);
        }

        .syllabus-bar-back-link:hover {
            color: #1d4ed8;
            background-color: #eff6ff;
        }

        .syllabus-bar-back-link svg {
            height: 1.25rem;
            width: 1.25rem;
            margin-right: 0.375rem;
            fill: currentColor;
        }

        .syllabus-bar-topic-badge {
            background-color: #2563eb;
            color: white;
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.375rem 1rem;
            border-radius: 9999px;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }

        @media (min-width: 640px) {
            .syllabus-bar-container {
                flex-direction: row;
            }

            .syllabus-bar-back-link {
                margin-bottom: 0;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.875rem;
            }
        }

        /* General Content Styles (Same as topic-1) */
        section {
            margin-bottom: 3rem;
            padding-top: 70px;
            margin-top: -70px;
        }

        h2 {
            /* Main sections: Clustering, Association, Dimensionality */
            font-size: 2.2rem;
            color: var(--primary-color);
            border-bottom: 3px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            /* Algorithm names: K-Means, Apriori, PCA */
            font-size: 1.8rem;
            /* Slightly larger for main algorithm names */
            color: var(--secondary-color);
            /* Using secondary for algorithm names */
            margin-top: 2.5rem;
            /* More space before a new algorithm */
            margin-bottom: 1.2rem;
            border-bottom: 1px dashed var(--border-color);
            padding-bottom: 0.4rem;
        }

        h4 {
            /* Sub-headings within an algorithm: How it works, Python Implementation */
            font-size: 1.4rem;
            color: var(--accent-color);
            margin-top: 1.8rem;
            margin-bottom: 1rem;
        }

        h5 {
            /* Further sub-headings: Step 1, Pros/Cons */
            font-size: 1.2rem;
            color: var(--primary-color);
            opacity: 0.9;
            margin-top: 1.2rem;
            margin-bottom: 0.7rem;
        }

        .formula {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 0.8rem 1rem;
            border-radius: 4px;
            margin: 1rem auto;
            display: block;
            text-align: center;
            font-size: 1em;
            overflow-x: auto;
        }


        p,
        li {
            font-size: 1.05rem;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        ul,
        ol {
            padding-left: 25px;
            margin-bottom: 1rem;
        }

        ul li,
        ol li {
            margin-bottom: 0.5rem;
        }

        .callout {
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 5px solid var(--accent-color);
            background-color: var(--card-bg);
            border-radius: 0 5px 5px 0;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
        }

        .callout.info {
            border-left-color: var(--primary-color);
        }

        .callout.success {
            border-left-color: var(--secondary-color);
        }

        .callout.warning {
            border-left-color: #f39c12;
        }

        details {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 5px;
            margin-bottom: 1rem;
            padding: 0.5rem 1rem;
        }

        summary {
            font-weight: bold;
            cursor: pointer;
            color: var(--primary-color);
            padding: 0.5rem 0;
        }

        summary::marker {
            color: var(--accent-color);
        }

        pre {
            background-color: var(--code-bg);
            color: var(--text-color);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9em;
            /* Slightly smaller for better fit */
            border: 1px solid var(--border-color);
            margin-top: 0.5rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
        }

        code {
            /* Inline code */
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 0.1em 0.3em;
            border-radius: 3px;
            font-size: 0.9em;
        }


        /* Diagram Styles (similar to agent-diagram) */
        .algorithm-diagram,
        .output-plot {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 15px;
            margin: 1.5rem auto;
            max-width: 600px;
            /* Can adjust per diagram */
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.07);
        }

        .algorithm-diagram img,
        .output-plot img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-top: 10px;
            border: 1px solid var(--border-color);
        }

        .algorithm-diagram p,
        .output-plot p {
            font-size: 0.9em;
            margin-top: 0.5em;
            color: var(--text-color);
            opacity: 0.8;
        }

        .diagram-placeholder {
            border: 2px dashed var(--border-color);
            padding: 20px;
            min-height: 100px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--text-color);
            opacity: 0.7;
            font-style: italic;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            font-size: 0.95em;
        }

        th,
        td {
            border: 1px solid var(--border-color);
            padding: 8px 10px;
            text-align: left;
        }

        th {
            background-color: var(--code-bg);
            /* Light background for headers */
            font-weight: bold;
        }

        /* Responsive table */
        .table-container {
            overflow-x: auto;
            margin-bottom: 1.5rem;
        }


        /* New Footer Styles (Same as topic-1, with link updates) */
        .new-footer-container {
            margin-top: 4rem;
            padding-top: 2.5rem;
            border-top: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            padding-bottom: 1rem;
        }

        .new-footer-buttons-wrapper {
            display: flex;
            flex-direction: column;
            width: 100%;
            align-items: center;
        }

        .new-footer-button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 0.75rem 1.5rem;
            background-color: #2563eb;
            color: white !important;
            border-radius: 9999px;
            text-decoration: none;
            transition: background-color 0.15s, box-shadow 0.15s;
            font-weight: 500;
            font-size: 0.875rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            min-width: 280px;
            text-align: center;
            margin-bottom: 0.5rem;
        }

        .new-footer-button:hover {
            background-color: #1d4ed8;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        .new-footer-button:focus {
            outline: 2px solid #3b82f6;
            outline-offset: 2px;
        }



        @media (min-width: 640px) {
            .new-footer-buttons-wrapper {
                flex-direction: row;
                justify-content: space-between;
                gap: 1rem;
            }

            .new-footer-button {
                margin-bottom: 0;
            }
        }

        /* Quiz Styles (Same as topic-1) */
        .quiz-container {
            background-color: var(--card-bg);
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .quiz-question {
            margin-bottom: 15px;
        }

        .quiz-question p strong {
            color: var(--text-color);
        }


        .quiz-options label {
            display: block;
            margin-bottom: 8px;
            cursor: pointer;
        }

        .quiz-options input {
            margin-right: 8px;
        }

        .quiz-feedback {
            margin-top: 10px;
            font-weight: bold;
        }

        /* Responsive Adjustments (Same as topic-1) */
        @media (max-width: 992px) {
            .sidebar {
                width: 100%;
                height: auto;
                position: static;
                border-right: none;
                border-bottom: 1px solid var(--border-color);
                box-shadow: none;
                top: auto;
            }

            .main-content {
                margin-left: 0;
                padding: 1.5rem;
            }

            .navbar {
                padding: 0.8rem 1rem;
            }

            .navbar-brand {
                font-size: 1.5rem;
            }

            .nav-left-spacer,
            .theme-toggle {
                flex-basis: 40px;
            }
        }

        @media (max-width: 600px) {
            .eduvishion-logo {
                top: 12px;
                left: 12px;
            }

            .eduvishion-logo .text-2xl {
                font-size: 1.3rem;
            }

            .eduvishion-logo svg {
                height: 1.2em;
                width: 1.2em;
            }

            .navbar {
                padding: 1rem;
                justify-content: center;
                position: relative;
            }

            .nav-left-spacer {
                display: none;
            }

            .navbar-brand {
                flex-grow: 0;
                margin-right: auto;
                margin-left: 50px;
                /* Adjusted for Eduvision logo space */
                font-size: 1.3rem;
            }

            .theme-toggle {
                position: absolute;
                right: 1rem;
                top: 50%;
                transform: translateY(-50%);
                flex-basis: auto;
                font-size: 1.2rem;
            }

            .main-content {
                padding: 1rem;
            }

            .hero-section h1 {
                font-size: 2.5rem;
            }

            .hero-section p {
                font-size: 1.1rem;
            }

            h2 {
                font-size: 1.8rem;
            }

            h3 {
                font-size: 1.5rem;
            }

            h4 {
                font-size: 1.2rem;
            }

            h5 {
                font-size: 1.1rem;
            }


            .syllabus-bar-container {
                padding: 0.5rem;
            }

            .syllabus-bar-back-link {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.7rem;
                padding: 0.3rem 0.8rem;
            }

            .new-footer-button {
                font-size: 0.8rem;
                padding: 0.6rem 1.2rem;
                min-width: auto;
                width: 90%;
            }

            .new-footer-buttons-wrapper {
                flex-direction: column;
            }

            .new-footer-buttons-wrapper .new-footer-button:first-child {
                margin-bottom: 0.5rem;
            }

            .algorithm-diagram,
            .output-plot {
                max-width: 100%;
                padding: 10px;
            }
        }

        /* Print Styles (Same as topic-1) */
        @media print {
            body {
                font-size: 10pt;
                color: #000 !important;
                background-color: #fff !important;
            }

            .navbar,
            .sidebar,
            .theme-toggle,
            .hero-section .btn,
            .quiz-container,
            .new-footer-container,
            details summary::marker,
            .eduvishion-logo,
            .syllabus-bar-container,
            .new-footer-print-link {
                display: none;
            }

            .main-content {
                margin-left: 0;
                padding: 0;
            }

            section {
                padding-top: 0;
                margin-top: 0;
                margin-bottom: 1.5rem;
                page-break-after: auto;
            }

            h1,
            h2,
            h3,
            h4,
            h5 {
                color: #000 !important;
                border: none !important;
                page-break-after: avoid;
            }

            .callout {
                border-left: 3px solid #ccc !important;
                background-color: #f9f9f9 !important;
            }

            a {
                text-decoration: none;
                color: #000 !important;
            }

            a[href^="http"]:after {
                content: " (" attr(href) ")";
            }

            pre,
            .formula {
                background-color: #f0f0f0 !important;
                border: 1px solid #ccc !important;
                color: #000 !important;
                page-break-inside: avoid;
                white-space: pre-wrap;
                /* Ensure code wraps in print */
                word-wrap: break-word;
            }

            table,
            .algorithm-diagram,
            .output-plot {
                page-break-inside: avoid;
            }

            .algorithm-diagram img,
            .output-plot img {
                max-width: 80% !important;
                /* Control image size in print */
            }
        }
    </style>
</head>

<body>

    <div class="eduvishion-logo">
        <div class="text-2xl font-bold">
            <a href="../../../index.html" class="flex items-center group">
                <span class="text-white">Edu</span>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"
                    fill="none">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
                    <path stroke-linecap="round" stroke-linejoin="round"
                        d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z" />
                </svg>
                <span class="text-white">ision</span>
            </a>
        </div>
    </div>

    <nav class="navbar">
        <div class="nav-left-spacer"></div>
        <a href="#" class="navbar-brand"><i class="fas fa-language"></i> Natural Language Processing</a>
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
            <i class="fas fa-moon"></i>
        </button>
    </nav>

    <aside class="sidebar" id="sidebar">
        <h3>Table of Contents</h3>
        <ul id="toc"></ul>
    </aside>

    <main class="main-content">
        <header class="hero-section">
            <h1>Natural Language Processing (NLP)</h1>
            <p>Enabling computers to understand, interpret, and generate human language.</p>
        </header>

        <div class="syllabus-bar-container">
            <a href="../ai.html" class="syllabus-bar-back-link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20">
                    <path fill-rule="evenodd"
                        d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z"
                        clip-rule="evenodd" />
                </svg>
                Back to Syllabus
            </a>
            <div class="syllabus-bar-topic-badge">
                Topic 11
            </div>
        </div>

        <section id="intro-nlp">
            <h2>Introduction to NLP</h2>
            <p>Natural Language Processing (NLP) refers to AI method of communicating with an intelligent systems using
                a natural language such as English. Processing of Natural Language is required when you want an
                intelligent system like robot to perform as per your instructions, when you want to hear decision from a
                dialogue based clinical expert system, etc.</p>
            <p>The field of NLP involves making computers to perform useful tasks with the natural languages humans use.
                The input and output of an NLP system can be:</p>
            <ul>
                <li>Speech</li>
                <li>Written Text</li>
            </ul>
            <h4>Components of NLP</h4>
            <p>There are two components of NLP as given:</p>
            <ul>
                <li><strong>Natural Language Understanding (NLU):</strong> Understanding involves the following tasks:
                    <ul>
                        <li>Mapping the given input in natural language into useful representations.</li>
                        <li>Analyzing different aspects of the language.</li>
                    </ul>
                </li>
                <li><strong>Natural Language Generation (NLG):</strong> It is the process of producing meaningful
                    phrases and sentences in the form of natural language from some internal representation. It
                    involves:
                    <ul>
                        <li><strong>Text planning</strong> – It includes retrieving the relevant content from knowledge
                            base.</li>
                        <li><strong>Sentence planning</strong> – It includes choosing required words, forming meaningful
                            phrases, setting tone of the sentence.</li>
                        <li><strong>Text Realization</strong> – It is mapping sentence plan into sentence structure.
                        </li>
                    </ul>
                </li>
            </ul>
            <p>The NLU is harder than NLG.</p>
            <h4>Difficulties in NLU</h4>
            <p>NL has an extremely rich form and structure. It is very ambiguous. There can be different levels of
                ambiguity:</p>
            <ul>
                <li><strong>Lexical ambiguity</strong> – It is at very primitive level such as word-level. For example,
                    treating the word "board" as noun or verb?</li>
                <li><strong>Syntax Level ambiguity</strong> – A sentence can be parsed in different ways. For example,
                    “He lifted the beetle with red cap.” – Did he use cap to lift the beetle or he lifted a beetle that
                    had red cap?</li>
                <li><strong>Referential ambiguity</strong> – Referring to something using pronouns. For example, Rima
                    went to Gauri. She said, “I am tired.” – Exactly who is tired?</li>
                <li>One input can mean different meanings.</li>
                <li>Many inputs can mean the same thing.</li>
            </ul>
            <h4>NLP Terminology</h4>
            <ul>
                <li><strong>Phonology</strong> – It is study of organizing sound systematically.</li>
                <li><strong>Morphology</strong> – It is a study of construction of words from primitive meaningful
                    units.</li>
                <li><strong>Morpheme</strong> – It is primitive unit of meaning in a language.</li>
                <li><strong>Syntax</strong> – It refers to arranging words to make a sentence. It also involves
                    determining the structural role of words in the sentence and in phrases.</li>
                <li><strong>Semantics</strong> – It is concerned with the meaning of words and how to combine words into
                    meaningful phrases and sentences.</li>
                <li><strong>Pragmatics</strong> – It deals with using and understanding sentences in different
                    situations and how the interpretation of the sentence is affected.</li>
                <li><strong>Discourse</strong> – It deals with how the immediately preceding sentence can affect the
                    interpretation of the next sentence.</li>
                <li><strong>World Knowledge</strong> – It includes the general knowledge about the world.</li>
            </ul>
        </section>

        <section id="nlp-steps">
            <h2>Steps in NLP</h2>
            <p>There are general five steps involved in Natural Language Processing:</p>
            <ol>
                <li><strong>Lexical Analysis</strong> – It involves identifying and analyzing the structure of words.
                    Lexicon of a language means the collection of words and phrases in a language. Lexical analysis is
                    dividing the whole chunk of text into paragraphs, sentences, and words.</li>
                <li><strong>Syntactic Analysis (Parsing)</strong> – It involves analysis of words in the sentence for
                    grammar and arranging words in a manner that shows the relationship among the words. The sentence
                    such as “The school goes to boy” is rejected by English syntactic analyzer.</li>
                <li><strong>Semantic Analysis</strong> – It draws the exact meaning or the dictionary meaning from the
                    text. The text is checked for meaningfulness. It is done by mapping syntactic structures and objects
                    in the task domain. The semantic analyzer disregards sentence such as “hot ice-cream”.</li>
                <li><strong>Discourse Integration</strong> – The meaning of any sentence depends upon the meaning of the
                    sentence just before it. In addition, it also brings about the meaning of immediately succeeding
                    sentence.</li>
                <li><strong>Pragmatic Analysis</strong> – During this, what was said is re-interpreted on what it
                    actually meant. It involves deriving those aspects of language which require real world knowledge.
                </li>
            </ol>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Figure 3.1: NLP Steps Flowchart: Lexical Analysis -> Syntactic Analysis
                    -> Semantic Analysis -> Disclosure Integration (Discourse Integration) -> Pragmatic Analysis.</div>
                <p>Figure 3.1: NLP Steps</p>
            </div>
        </section>

        <section id="text-preprocessing">
            <h2>Text Preprocessing</h2>
            <p>Text preprocessing is a crucial initial step in any NLP pipeline. It involves cleaning and transforming
                raw text data into a format that can be easily understood and processed by machine learning models. Key
                techniques include:</p>

            <h3 id="tokenization">Tokenization</h3>
            <p>Tokenization refers to breaking down the text into smaller units. It entails splitting paragraphs into
                sentences and sentences into words. It is one of the initial steps of any NLP pipeline.</p>
            <h4>Work Tokenization (Word Tokenization)</h4>
            <p>It involves breaking down the text into words.</p>
            <div class="callout info">
                <p>Example: "I study Machine Learning on GeeksforGeeks." will be word-tokenized as
                    <br><code>['I', 'study', 'Machine', 'Learning', 'on', 'GeeksforGeeks', '.']</code>
                </p>
            </div>
            <h4>Sentence Tokenization</h4>
            <p>It involves breaking down the text into individual sentences.</p>
            <div class="callout info">
                <p>Example: "I study Machine learning on GeeksforGeeks. Currently, I'm studying NLP." will be
                    sentence-tokenized as
                    <br><code>['I study Machine learning on GeeksforGeeks.', "Currently, I'm studying NLP." ]</code>
                </p>
            </div>
            <h5>Python Implementation (NLTK):</h5>
            <pre><code>from nltk.tokenize import word_tokenize, sent_tokenize
sent = "GeeksforGeeks is a great learning platform. It is one of the best for Computer Science students."
print(word_tokenize(sent))
print(sent_tokenize(sent))</code></pre>
            <div class="output-plot">
                <pre><code>['GeeksforGeeks', 'is', 'a', 'great', 'learning', 'platform', '.', 'It', 'is', 'one', 'of', 'the', 'best', 'for', 'Computer', 'Science', 'students', '.']
['GeeksforGeeks is a great learning platform.', 'It is one of the best for Computer Science students.']</code></pre>
            </div>

            <h3 id="stemming">Stemming</h3>
            <p>Stemming generates the base word from the inflected word by removing the affixes of the word. It has a
                set of predefined rules that govern the dropping of these affixes. It must be noted that stemmers might
                not always result in semantically meaningful base words. Stemmers are faster and computationally less
                expensive than lemmatizers.</p>
            <h5>Python Implementation (Porter Stemmer with NLTK):</h5>
            <pre><code>from nltk.stem import PorterStemmer
porter = PorterStemmer()
words = ["play", "playing", "plays", "played", "communication"]
for word in words:
    print(f"{word} -> {porter.stem(word)}")</code></pre>
            <div class="output-plot">
                <pre><code>play -> play
playing -> play
plays -> play
played -> play
communication -> commun</code></pre>
            </div>
            <p>The stemmer reduces the word ‘communication’ to a base word ‘commun’ which is meaningless in itself.</p>

            <h3 id="lemmatization">Lemmatization</h3>
            <p>Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out
                to the base form of any word which will be meaningful in nature. The base from here is called the Lemma.
                Lemmatizers are slower and computationally more expensive than stemmers. It often requires
                Part-of-Speech (POS) information for accuracy.</p>
            <h5>Python Implementation (WordNet Lemmatizer with NLTK):</h5>
            <pre><code>from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
words = ["plays", "playing", "played", "mice", "corpora"]
# POS tags: 'v' for verb, 'n' for noun
print(f"plays (verb) -> {lemmatizer.lemmatize('plays', 'v')}")
print(f"playing (verb) -> {lemmatizer.lemmatize('playing', 'v')}")
print(f"mice (noun) -> {lemmatizer.lemmatize('mice', 'n')}")
print(f"corpora (noun) -> {lemmatizer.lemmatize('corpora', 'n')}")</code></pre>
            <div class="output-plot">
                <pre><code>plays (verb) -> play
playing (verb) -> play
mice (noun) -> mouse
corpora (noun) -> corpus</code></pre>
            </div>
            <p>Please note that in lemmatizers, we need to pass the Part of Speech of the word along with the word as a
                function argument.</p>
            <h4>Other Preprocessing Steps:</h4>
            <ul>
                <li><strong>Stopword Removal:</strong> Removing common words (like "and", "the", "is") that may not
                    carry significant meaning for certain tasks.</li>
                <li><strong>Text Normalization:</strong> Standardizing text, including case normalization (e.g.,
                    converting to lowercase), removing punctuation, and correcting spelling errors.</li>
            </ul>
        </section>

        <section id="language-models">
            <h2>Language Models</h2>
            <p>Language modeling is the way of determining the probability of any sequence of words. Language modeling
                is used in various applications such as Speech Recognition, Spam filtering, etc. Language modeling is
                the key aim behind implementing many state-of-the-art Natural Language Processing models.</p>
            <h4>Methods of Language Modeling</h4>
            <ol>
                <li><strong>Statistical Language Modeling:</strong> Statistical Language Modeling, or Language Modeling,
                    is the development of probabilistic models that can predict the next word in the sequence given the
                    words that precede. Examples such as N-gram language modeling.</li>
                <li><strong>Neural Language Modeling:</strong> Neural network methods are achieving better results than
                    classical methods both on standalone language models and when models are incorporated into larger
                    models on challenging tasks like speech recognition and machine translation. A way of performing a
                    neural language model is through word embeddings.</li>
            </ol>

            <h3 id="n-grams">N-grams</h3>
            <p>N-gram can be defined as the contiguous sequence of n items from a given sample of text or speech. The
                items can be letters, words, or base pairs according to the application. The N-grams typically are
                collected from a text or speech corpus (A long text dataset).</p>
            <p>For instance, N-grams can be unigrams like ("This", "article", "is", "on", "NLP") or bigrams ("This
                article", "article is", "is on", "on NLP").</p>
            <p>An N-gram language model predicts the probability of a given N-gram within any sequence of words in a
                language. A well-crafted N-gram model can effectively predict the next word in a sentence, which is
                essentially determining the value of P(w|h), where h is the history or context and w is the word to
                predict.</p>
            <h5>Metrics for Language Modelling</h5>
            <ul>
                <li><strong>Entropy:</strong> Entropy, as a measure of the amount of information conveyed by Claude
                    Shannon. H(p) = Σ<sub>x</sub> p(x) · (-log[p(x)]). H(p) is always greater than equal to 0.</li>
                <li><strong>Cross-Entropy:</strong> It measures the ability of the trained model to represent test data.
                    H(p) = Σ<sup>N</sup><sub>i=1</sub> 1/N (-log<sub>2</sub>(p(w<sub>i</sub> |
                    w<sub>i-1</sub><sup>1</sup>))). The cross-entropy is always greater than or equal to Entropy i.e the
                    model uncertainty can be no less than the true uncertainty.</li>
                <li><strong>Perplexity:</strong> Perplexity is a measure of how good a probability distribution predicts
                    a sample. It can be understood as a measure of uncertainty. The perplexity can be calculated by
                    cross-entropy to the exponent of 2. <br> PP(W) = 2<sup>Cross-Entropy</sup> = (
                    Π<sup>N</sup><sub>i=1</sub> P(w<sub>i</sub> | w<sub>1</sub>...w<sub>i-1</sub>) )<sup>-1/N</sup></li>
            </ul>

            <h3 id="word-embeddings">Word Embeddings</h3>
            <p>Word Embeddings are numeric representations of words in a lower-dimensional space, capturing semantic and
                syntactic information. They play a vital role in Natural Language Processing (NLP) tasks. Word Embedding
                or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows
                words with similar meanings to have a similar representation.</p>
            <p>Word Embeddings are a method of extracting features out of text so that we can input those features into
                a machine learning model to work with text data. They try to preserve syntactical and semantic
                information. The methods such as Bag of Words (BOW), CountVectorizer and TFIDF rely on the word count in
                a sentence but do not save any syntactical or semantic information. Word Embeddings give a solution to
                these problems.</p>
            <h4>Need for Word Embedding?</h4>
            <ul>
                <li>To reduce dimensionality</li>
                <li>To use a word to predict the words around it.</li>
                <li>Inter-word semantics must be captured.</li>
            </ul>

            <h4 id="word2vec">Word2Vec</h4>
            <p>Word2Vec is a widely used method in natural language processing (NLP) that allows words to be represented
                as vectors in a continuous vector space. Word2Vec utilizes two architectures:</p>
            <ul>
                <li><strong>CBOW (Continuous Bag of Words):</strong> The CBOW model predicts the current word given
                    context words within a specific window.</li>
                <li><strong>Skip Gram:</strong> Skip gram predicts the surrounding context words within specific window
                    given current word.</li>
            </ul>
            <div class="algorithm-diagram" style="display: flex; justify-content: space-around;">
                <div style="width:48%;">
                    <div class="diagram-placeholder">CBOW Model Architecture (context words as input, target word as
                        output).</div>
                    <p>CBOW Model</p>
                </div>
                <div style="width:48%;">
                    <div class="diagram-placeholder">Skip-gram Model Architecture (target word as input, context words
                        as output).</div>
                    <p>Skip-gram Model</p>
                </div>
            </div>
            <p>Word2Vec's mechanics involve training shallow neural network models (CBOW and Skip-gram) to learn vector
                representations that effectively capture semantic relationships between words. The resulting vectors
                provide meaningful and efficient word representations in the vector space.</p>
            <h5>Advantages of Word2Vec:</h5>
            <ul>
                <li>Captures semantic relationships effectively.</li>
                <li>Efficient for large datasets.</li>
                <li>Provides meaningful word representations.</li>
            </ul>
            <h5>Disadvantages of Word2Vec:</h5>
            <ul>
                <li>May struggle with rare words.</li>
                <li>Ignores word order beyond the context window.</li>
            </ul>

            <h4 id="glove">GloVe (Global Vectors for Word Representation)</h4>
            <p>Global Vectors for Word Representation (GloVe) is a powerful word embedding technique that captures the
                semantic relationships between words by considering their co-occurrence probabilities within a corpus.
                The key to GloVe's effectiveness lies in the construction of a word-context matrix and the subsequent
                factorization process.</p>
            <ol>
                <li><strong>Word-Context Matrix Formation:</strong> This matrix is designed to represent the likelihood
                    of a given word appearing near another across the entire corpus.</li>
                <li><strong>Factorization for Word Vectors:</strong> GloVe decomposes this high-dimensional matrix into
                    two smaller matrices – one representing words and the other contexts.</li>
                <li><strong>Vector Representations:</strong> Once trained, GloVe provides each word with a dense vector
                    that captures not just local context but global word usage patterns.</li>
            </ol>
            <h5>Advantages of GloVe:</h5>
            <ul>
                <li>Efficiently captures global statistics of the corpus.</li>
                <li>Good at representing both semantic and syntactic relationships.</li>
                <li>Effective in capturing word analogies.</li>
            </ul>
            <h5>Disadvantages of GloVe:</h5>
            <ul>
                <li>Requires more memory for storing co-occurrence matrices.</li>
                <li>Less effective with very small corpora.</li>
            </ul>

            <h4 id="bert-embeddings">BERT (Bidirectional Encoder Representations from Transformers)</h4>
            <p>A new model based on transformers was made by Google AI called BERT (Bidirectional Encoder
                Representations from Transformers). BERT changed the field of natural language processing (NLP) forever
                when it came up with contextual embeddings, which look at the whole sentence context of a word.</p>
            <p>BERT undergoes pre-training using extensive quantities of textual material, which facilitates the
                acquisition of a profound understanding of linguistic nuances. During this phase, the system is taught
                to predict absent words in sentences, a process known as masked language modeling (MLM) and next
                sentence prediction (NSP).</p>
            <p>BERT can subsequently be fine-tuned for specific following tasks by adapting its pre-trained knowledge to
                various NLP applications such as sentiment analysis, question answering, and language translation.</p>

            <h4>Advantages and Disadvantages of Word Embeddings (General)</h4>
            <h5>Advantages</h5>
            <ul>
                <li>Much faster to train than hand-built models like WordNet (which uses graph embeddings).</li>
                <li>Almost all modern NLP applications start with an embedding layer.</li>
                <li>Stores an approximation of meaning.</li>
            </ul>
            <h5>Disadvantages</h5>
            <ul>
                <li>Can be memory intensive.</li>
                <li>Is corpus dependent. Any underlying bias will have an effect on your model.</li>
                <li>Cannot distinguish between homophones. Eg: brake/break, cell/sell, weather/whether etc. (Note:
                    Contextual embeddings like BERT address this).</li>
            </ul>
        </section>

        <section id="core-nlp-tasks">
            <h2>Core NLP Tasks</h2>

            <h3 id="pos-tagging">Part-of-Speech (POS) Tagging</h3>
            <p>Part of Speech (POS) tagging refers to assigning each word of a sentence to its part of speech. It is
                significant as it helps to give a better syntactic overview of a sentence. Common tags include Noun
                (NN), Verb (VB), Adjective (JJ), Adverb (RB), etc.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Example: "The delicious meal" -> The (DT), delicious (JJ), meal (NN).
                </div>
            </div>
            <h5>Implementation (NLTK):</h5>
            <pre><code>import nltk
from nltk.tokenize import word_tokenize
# nltk.download('averaged_perceptron_tagger') # if not already downloaded
text = "GeeksforGeeks is a Computer Science platform."
tokenized_text = word_tokenize(text)
tags = nltk.pos_tag(tokenized_text)
print(tags)</code></pre>
            <div class="output-plot">
                <pre><code>[('GeeksforGeeks', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Computer', 'NNP'), ('Science', 'NNP'), ('platform', 'NN'), ('.', '.')]</code></pre>
            </div>

            <h3 id="ner">Named Entity Recognition (NER)</h3>
            <p>Named Entity Recognition (NER) serves as a bridge between unstructured text and structured data, enabling
                machines to sift through vast amounts of textual information and extract nuggets of valuable data in
                categorized forms. By pinpointing specific entities within a sea of words, NER transforms the way we
                process and utilize textual data. Its primary objective is to identify specific chunks as named entities
                and classify them into predefined categories such as Person, Organization, Location, Date, etc.</p>
            <h4>How it works:</h4>
            <ol>
                <li><strong>Tokenization:</strong> Text is split into tokens.</li>
                <li><strong>Entity identification:</strong> Potential named entities are detected using linguistic rules
                    or statistical methods.</li>
                <li><strong>Entity classification:</strong> Identified entities are categorized.</li>
                <li><strong>Contextual analysis:</strong> Surrounding context improves accuracy.</li>
                <li><strong>Post-processing:</strong> Results are refined.</li>
            </ol>
            <h4>NER Methods:</h4>
            <ul>
                <li><strong>Rule-based Methods:</strong> Use manually crafted rules, regular expressions, or
                    dictionaries.</li>
                <li><strong>Statistical Methods:</strong> Employ models like Hidden Markov Models (HMM) or Conditional
                    Random Fields (CRF).</li>
                <li><strong>Machine Learning Methods:</strong> Use algorithms like decision trees or SVMs, learning from
                    labeled data.</li>
                <li><strong>Deep Learning Methods:</strong> Harness RNNs and Transformers.</li>
                <li><strong>Hybrid Methods:</strong> Combine multiple approaches.</li>
            </ul>
            <h5>Implementation (spaCy):</h5>
            <pre><code>import spacy
# python -m spacy download en_core_web_sm # Run in terminal if model not downloaded
nlp = spacy.load("en_core_web_sm")
text = "Apple is looking at buying U.K. startup for $1 billion in London."
doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)</code></pre>
            <div class="output-plot">
                <pre><code>Apple ORG
U.K. GPE
$1 billion MONEY
London GPE</code></pre>
            </div>


            <h3 id="sentiment-analysis">Sentiment Analysis</h3>
            <p>Sentiment analysis is a branch of natural language processing (NLP) that focuses on identifying and
                categorizing opinions expressed in textual data. It determines whether the sentiment of a piece of text
                is positive, negative, or neutral. Widely used in business, social media monitoring, and customer
                feedback analysis, sentiment analysis helps organizations understand public perception and make
                data-driven decisions.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Sentiment Analysis images: smiley faces for positive, neutral,
                    negative. Example sentences for each.</div>
            </div>
            <h4>Techniques of Sentiment Analysis:</h4>
            <ul>
                <li><strong>Lexicon-Based Analysis:</strong> Uses predefined dictionaries of words associated with
                    positive, negative, or neutral sentiments.</li>
                <li><strong>Machine Learning-Based Analysis:</strong> Involves training machine learning models on
                    labeled datasets to classify sentiments.</li>
                <li><strong>Deep Learning-Based Analysis:</strong> Uses advanced neural networks (RNNs, LSTMs,
                    Transformers) to extract sentiment from text.</li>
                <li><strong>Hybrid Techniques:</strong> Combines lexicon-based and machine learning techniques.</li>
            </ul>
        </section>

        <section id="transformers-attention">
            <h2>Transformers and Attention Mechanisms</h2>
            <p>Transformers and attention mechanisms have revolutionized the field of deep learning, offering a powerful
                way to process sequential data and capture long-range dependencies. The advent of attention mechanisms
                has been nothing short of revolutionary in the realm of deep learning. Attention allows models to
                dynamically focus on pertinent parts of the input data, akin to the way humans pay attention to certain
                aspects of a visual scene or conversation.</p>
            <h4>Key Takeaways:</h4>
            <ul>
                <li>Attention mechanisms are crucial in transformers, allowing different tokens to be weighted based on
                    their importance, enhancing model context and output quality.</li>
                <li>Transformers operate on self-attention, enabling the capture of long-range dependencies without
                    sequential processing.</li>
                <li>Multi-head attention in transformers enhances model performance by allowing the model to focus on
                    different aspects of the input data simultaneously.</li>
                <li>Transformers outperform RNNs and LSTMs in handling sequential data due to their parallel processing
                    capabilities.</li>
            </ul>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Transformer Encoder-Decoder architecture high-level view. Input: "I
                    like science", Output: "Ich mag Wissenschaften".</div>
                <p>Evolution of Transformers in Deep Learning</p>
            </div>
            <p>The Transformer architecture, a groundbreaking innovation in the field of deep learning, has
                revolutionized our approach to sequence-to-sequence tasks. At its core, the Transformer consists of two
                primary components: an <strong>encoder</strong> and a <strong>decoder</strong>. Each component is
                designed to perform distinct yet complementary functions in the processing of input and generation of
                output.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Detailed Transformer Architecture (Encoder-Decoder with multi-head
                    attention, feed-forward layers, positional encoding, etc.).</div>
                <p>Transformer Architecture</p>
            </div>
            <h4>Key Components of Transformers:</h4>
            <ul>
                <li><strong>Self-Attention Mechanism:</strong> A pivotal innovation enabling the model to weigh the
                    significance of different parts of the input independently of their position in the sequence.</li>
                <li><strong>Multi-Head Attention:</strong> Allows the model to concurrently process data through
                    multiple self-attention layers, capturing richer representations by attending to different parts of
                    the input sequence from different perspectives.</li>
                <li><strong>Feed-Forward Neural Networks:</strong> Process the information gleaned from attention
                    mechanisms, adding non-linearity and refining token representations.</li>
                <li><strong>Positional Encoding:</strong> Since Transformers process tokens in parallel, they lack
                    inherent sequence order information. Positional encodings are added to the input embeddings to
                    provide this.</li>
            </ul>
            <p>Transformers excel in scenarios where the ability to process large sequences in parallel and capture
                long-range dependencies is important.</p>
        </section>

        <section id="nlp-applications">
            <h2>Applications: Chatbots and Text Generation</h2>
            <h3 id="text-generation">Text Generation</h3>
            <p>Text generation is the process of using artificial intelligence (AI) to produce human-like written
                content. By leveraging advanced machine learning techniques, particularly Natural Language Processing
                (NLP), AI models can generate coherent and contextually relevant text. This technology is transforming
                how we create content, making writing faster and more efficient.</p>
            <h4>How Text Generation Works?</h4>
            <p>Text generation works by training AI models on vast datasets of text. These models analyse thousands, if
                not millions, of documents to recognize patterns, sentence structures, and word relationships. Just as a
                student learns a language by reading books and practicing conversations, AI models learn from text data
                to generate coherent and meaningful responses.</p>
            <h4>Different Approaches to Text Generation:</h4>
            <ul>
                <li><strong>Autoregressive models:</strong> These models, such as GPT-4, generate text by predicting one
                    word at a time based on the sequence of words that came before it.</li>
                <li><strong>Seq2Seq models:</strong> These models are commonly used in tasks like machine translation,
                    where an input sequence (such as a sentence in one language) is transformed into an output sequence
                    (the translated sentence in another language).</li>
                <li><strong>Fine-tuned models:</strong> Pre-trained AI models can be further customized using specific
                    datasets to specialize in particular domains, such as generating medical reports, legal documents,
                    or financial summaries.</li>
            </ul>

            <h3 id="chatbots">Chatbots</h3>
            <p>A chatbot is an artificial intelligence (AI) software that can simulate a conversation (or a chat) with a
                user in natural language through messaging applications, websites, mobile apps or through the telephone.
                Chatbots are computer programs that simulate human conversation, written or spoken.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Chatbot workflow: User request -> Analyze request -> Identify intent
                    and entities -> Compose reply.</div>
            </div>
            <h4>How do chatbots work?</h4>
            <p>Many contemporary chatbots rely on a combination of machine learning, natural language processing (NLP),
                and automated rules. NLP enables chatbots to “understand” human language—including syntax, semantics,
                and intent—and provide accurate responses to human prompts.</p>
            <h4>Types of chatbots:</h4>
            <ul>
                <li><strong>Declarative chatbots:</strong> Perform one function. These chatbots use NLP, defined rules,
                    and ML to generate automated responses when you ask a question. They are common in customer support.
                </li>
                <li><strong>Predictive chatbots:</strong> More sophisticated and personalized. Often considered
                    conversational chatbots, or virtual agents, these AI- and data-driven chatbots are much more
                    interactive and aware. They utilize NLP and more complicated ML, along with NLU to continue learning
                    about the user.</li>
            </ul>
            <h4>Benefits and limitations of chatbots:</h4>
            <p><strong>Benefits:</strong> 24/7 availability, instant responses, cost savings, data collection,
                personalization, multilingual support.
                <br><strong>Limitations:</strong> Hallucinations, lack of emotional intelligence, limited understanding
                of complex context, dependence on training data, user frustration.
            </p>
        </section>

        <section id="interactive-quiz-nlp">
            <h2>Mini Quiz: Test Your NLP Knowledge!</h2>
            <div class="quiz-container">
                <div id="quiz">
                    <div class="quiz-question" data-question="1">
                        <p><strong>1. What is the main difference between Stemming and Lemmatization?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q1" value="a"> Stemming is faster but may not produce a
                                real word, while lemmatization is slower but produces a dictionary word.</label>
                            <label><input type="radio" name="q1" value="b"> Lemmatization removes prefixes, while
                                stemming removes suffixes.</label>
                            <label><input type="radio" name="q1" value="c"> Stemming uses machine learning, while
                                lemmatization uses rule-based approaches.</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q1"></p>
                    </div>
                    <div class="quiz-question" data-question="2">
                        <p><strong>2. Which word embedding technique is known for considering global co-occurrence
                                statistics of words in a corpus?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q2" value="a"> Word2Vec (CBOW)</label>
                            <label><input type="radio" name="q2" value="b"> Word2Vec (Skip-gram)</label>
                            <label><input type="radio" name="q2" value="c"> GloVe</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q2"></p>
                    </div>
                    <div class="quiz-question" data-question="3">
                        <p><strong>3. What does NER stand for in NLP, and what is its primary task?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q3" value="a"> Neural Entity Routing; routing text to
                                appropriate neural networks.</label>
                            <label><input type="radio" name="q3" value="b"> Named Entity Recognition; identifying and
                                classifying entities like persons, organizations, locations.</label>
                            <label><input type="radio" name="q3" value="c"> Natural Element Retrieval; retrieving basic
                                elements of language.</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q3"></p>
                    </div>
                    <div class="quiz-question" data-question="4">
                        <p><strong>4. The Transformer architecture primarily relies on which mechanism to process
                                sequences?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q4" value="a"> Recurrent connections</label>
                            <label><input type="radio" name="q4" value="b"> Convolutional filters</label>
                            <label><input type="radio" name="q4" value="c"> Self-attention mechanisms</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q4"></p>
                    </div>
                    <button id="submitQuiz">Submit Answers</button>
                    <p id="quizScore" style="margin-top:15px; font-weight:bold;"></p>
                </div>
            </div>
        </section>


        <footer class="new-footer-container">
            <div class="new-footer-buttons-wrapper">
                <a href="topic-10.html" class="new-footer-button">
                    ← Previous Topic: Neural Networks & Deep Learning
                </a>
                <a href="topic-12.html" class="new-footer-button">
                    Next Topic: Computer Vision →
                </a>
            </div>

            <p style="font-size: 0.9rem; color: var(--text-color); margin-top: 0.5rem;">© <span id="currentYear"></span>
                Natural Language Processing (NLP). All rights reserved.</p>
        </footer>

    </main>

    <script>
        // JavaScript from topic-1.html (Theme toggle, Throttled TOC highlighting, Footer year)
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        const prefersDarkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        function setTheme(theme) { if (theme === 'dark') { body.classList.add('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-sun"></i>'; localStorage.setItem('theme', 'dark'); } else { body.classList.remove('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-moon"></i>'; localStorage.setItem('theme', 'light'); } }
        const localTheme = localStorage.getItem('theme'); if (localTheme) { setTheme(localTheme); } else { setTheme(prefersDarkScheme.matches ? 'dark' : 'light'); }
        themeToggle.addEventListener('click', () => { setTheme(body.classList.contains('dark-mode') ? 'light' : 'dark'); });
        prefersDarkScheme.addEventListener('change', (e) => { if (!localStorage.getItem('theme')) { setTheme(e.matches ? 'dark' : 'light'); } });

        // Table of Contents Generation & Active Scrolling
        const tocContainer = document.getElementById('toc');
        const mainContent = document.querySelector('.main-content');
        const mainSections = Array.from(mainContent.querySelectorAll('section[id]'));
        let tocLinkElements = [];

        function throttle(func, limit) { let inThrottle; return function () { const args = arguments; const context = this; if (!inThrottle) { func.apply(context, args); inThrottle = true; setTimeout(() => inThrottle = false, limit); } } }

        function updateTocAndSectionData() {
            const navbar = document.querySelector('.navbar');
            const navbarHeight = navbar ? navbar.offsetHeight : 0;
            tocContainer.innerHTML = '';
            tocLinkElements = [];

            mainSections.forEach(section => {
                const sectionTitleElement = section.querySelector('h2');
                if (sectionTitleElement) {
                    const listItem = document.createElement('li');
                    const link = document.createElement('a');
                    link.textContent = sectionTitleElement.textContent;
                    link.href = `#${section.id}`;
                    link.dataset.sectionId = section.id;
                    listItem.appendChild(link);
                    tocContainer.appendChild(listItem);
                    tocLinkElements.push(link);

                    const subSectionsH3 = Array.from(section.querySelectorAll('h3[id]'));
                    if (subSectionsH3.length > 0) {
                        const subListH3 = document.createElement('ul');
                        subSectionsH3.forEach(subSectionH3 => {
                            const subListItemH3 = document.createElement('li');
                            const subLinkH3 = document.createElement('a');
                            subLinkH3.textContent = subSectionH3.textContent;
                            subLinkH3.href = `#${subSectionH3.id}`;
                            subLinkH3.dataset.sectionId = subSectionH3.id;
                            subLinkH3.classList.add('sub-item');
                            subListItemH3.appendChild(subLinkH3);
                            subListH3.appendChild(subListItemH3);
                            tocLinkElements.push(subLinkH3);
                        });
                        listItem.appendChild(subListH3);
                    }
                }
                const allNavigableElements = Array.from(section.querySelectorAll('h2[id], h3[id]'));
                allNavigableElements.forEach(el => {
                    el.dataset.effectiveOffsetTop = el.getBoundingClientRect().top + window.scrollY - navbarHeight - 30;
                });
            });
        }

        function highlightActiveTocLink() {
            const scrollPosition = window.scrollY;
            let currentlyActiveSectionId = null;
            const allLinkableElements = tocLinkElements.map(link => document.getElementById(link.dataset.sectionId)).filter(el => el);

            for (let i = allLinkableElements.length - 1; i >= 0; i--) {
                const element = allLinkableElements[i];
                if (element && scrollPosition >= parseFloat(element.dataset.effectiveOffsetTop || '0')) {
                    currentlyActiveSectionId = element.id;
                    break;
                }
            }

            tocLinkElements.forEach(link => {
                if (link.dataset.sectionId === currentlyActiveSectionId) {
                    link.classList.add('active');
                    if (link.classList.contains('sub-item')) {
                        const parentLi = link.closest('ul').closest('li');
                        if (parentLi) {
                            const parentLink = parentLi.querySelector('a:not(.sub-item)');
                            if (parentLink) parentLink.classList.add('active');
                        }
                    }
                } else {
                    link.classList.remove('active');
                }
            });
        }

        if (mainSections.length > 0) { updateTocAndSectionData(); highlightActiveTocLink(); window.addEventListener('scroll', throttle(highlightActiveTocLink, 100)); window.addEventListener('resize', throttle(() => { updateTocAndSectionData(); highlightActiveTocLink(); }, 150)); }

        // Mini Quiz Logic
        const submitQuizButton = document.getElementById('submitQuiz');
        const quizQuestionElements = document.querySelectorAll('#interactive-quiz-nlp .quiz-question');
        const quizCorrectAnswers = {
            q1: { value: 'a', text: 'Stemming is faster but may not produce a real word, while lemmatization is slower but produces a dictionary word.' },
            q2: { value: 'c', text: 'GloVe' },
            q3: { value: 'b', text: 'Named Entity Recognition; identifying and classifying entities like persons, organizations, locations.' },
            q4: { value: 'c', text: 'Self-attention mechanisms' },
        };

        if (submitQuizButton) {
            submitQuizButton.addEventListener('click', () => {
                let score = 0;
                quizQuestionElements.forEach(questionElement => {
                    const questionId = 'q' + questionElement.dataset.question;
                    const selectedOption = document.querySelector(`#interactive-quiz-nlp input[name="${questionId}"]:checked`);
                    const feedbackElement = document.getElementById(`feedback-${questionId}`);
                    const correctAnswerInfo = quizCorrectAnswers[questionId];

                    if (selectedOption) {
                        if (selectedOption.value === correctAnswerInfo.value) {
                            score++;
                            feedbackElement.textContent = "Correct!";
                            feedbackElement.style.color = "var(--secondary-color)";
                        } else {
                            feedbackElement.textContent = `Incorrect. The correct answer is: ${correctAnswerInfo.text}.`;
                            feedbackElement.style.color = "var(--accent-color)";
                        }
                    } else {
                        feedbackElement.textContent = "Please select an answer.";
                        feedbackElement.style.color = "var(--accent-color)";
                    }
                });
                const quizScoreEl = document.getElementById('quizScore');
                if (quizScoreEl) {
                    quizScoreEl.textContent = `You scored ${score} out of ${quizQuestionElements.length}.`;
                }
            });
        }

        // Footer current year
        const currentYearSpan = document.getElementById('currentYear');
        if (currentYearSpan) {
            currentYearSpan.textContent = new Date().getFullYear();
        }
    </script>
</body>

</html>