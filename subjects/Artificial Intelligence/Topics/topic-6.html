<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Fundamentals</title>
    <meta name="description"
        content="Comprehensive overview of Machine Learning: Types (Supervised, Unsupervised, Reinforcement), ML workflow, model evaluation, overfitting, underfitting, bias-variance, feature engineering, and data preprocessing.">
    <meta name="keywords"
        content="Machine Learning, ML Fundamentals, Supervised Learning, Unsupervised Learning, Reinforcement Learning, ML Workflow, Model Evaluation, Overfitting, Underfitting, Bias-Variance Tradeoff, Feature Engineering, Data Preprocessing">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <style>
        /* CSS from previous topic pages - For brevity, imagine all the CSS is here */
        :root {
            --primary-color-light: #3498db;
            /* Blue */
            --secondary-color-light: #2ecc71;
            /* Green */
            --accent-color-light: #e67e22;
            /* Orange */
            --background-color-light: #f4f7f6;
            --text-color-light: #333;
            --card-bg-light: #ffffff;
            --border-color-light: #e0e0e0;
            --code-bg-light: #eef;

            --primary-color-dark: #5dade2;
            /* Lighter Blue */
            --secondary-color-dark: #58d68d;
            /* Lighter Green */
            --accent-color-dark: #f5b041;
            /* Lighter Orange */
            --background-color-dark: #1e272e;
            --text-color-dark: #f0f0f0;
            --card-bg-dark: #2c3a47;
            --border-color-dark: #444;
            --code-bg-dark: #2a2a40;

            --primary-color: var(--primary-color-light);
            --secondary-color: var(--secondary-color-light);
            --accent-color: var(--accent-color-light);
            --background-color: var(--background-color-light);
            --text-color: var(--text-color-light);
            --card-bg: var(--card-bg-light);
            --border-color: var(--border-color-light);
            --code-bg: var(--code-bg-light);

            --font-sans: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            --font-mono: 'Courier New', Courier, monospace;
            --font-logo: 'Nunito', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-mode {
            --primary-color: var(--primary-color-dark);
            --secondary-color: var(--secondary-color-dark);
            --accent-color: var(--accent-color-dark);
            --background-color: var(--background-color-dark);
            --text-color: var(--text-color-dark);
            --card-bg: var(--card-bg-dark);
            --border-color: var(--border-color-dark);
            --code-bg: var(--code-bg-dark);
        }

        /* Eduvision Logo */
        .eduvishion-logo {
            position: absolute;
            top: 18px;
            left: 18px;
            z-index: 1050;
            text-shadow: 0 2px 12px #6a82fb33;
        }

        .eduvishion-logo .text-2xl {
            font-family: var(--font-logo);
            font-size: 1.7rem;
            font-weight: 900;
            display: flex;
            align-items: center;
            gap: 2px;
            letter-spacing: 0.01em;
        }

        .eduvishion-logo .text-white {
            color: #fff !important;
        }

        .eduvishion-logo .text-yellow-300 {
            color: #fde047 !important;
        }

        .eduvishion-logo .group:hover .text-yellow-300 {
            color: #fef08a !important;
        }

        .eduvishion-logo a {
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 2px;
        }

        .eduvishion-logo svg {
            display: inline-block;
            vertical-align: middle;
            height: 1.5em;
            width: 1.5em;
            margin: 0 2px;
        }

        /* Navbar */
        .navbar {
            background-color: var(--card-bg);
            color: var(--text-color);
            padding: 1rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-left-spacer {
            flex-basis: 50px;
            flex-shrink: 0;
        }

        .navbar-brand {
            font-size: 1.8rem;
            font-weight: bold;
            color: var(--primary-color);
            text-decoration: none;
            text-align: center;
            flex-grow: 1;
        }

        .navbar-brand i {
            margin-right: 0.5rem;
        }

        .theme-toggle {
            cursor: pointer;
            font-size: 1.5rem;
            background: none;
            border: none;
            color: var(--text-color);
            flex-basis: 50px;
            flex-shrink: 0;
            text-align: right;
        }

        /* Sidebar */
        .sidebar {
            position: fixed;
            top: 77px;
            left: 0;
            width: 280px;
            height: calc(100vh - 77px);
            background-color: var(--card-bg);
            padding: 20px;
            overflow-y: auto;
            border-right: 1px solid var(--border-color);
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.05);
        }

        .sidebar h3 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.2rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }

        .sidebar ul {
            list-style: none;
            padding: 0;
        }

        .sidebar ul li a {
            display: block;
            padding: 8px 0;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.2s, padding-left 0.2s, background-color 0.2s;
            border-radius: 4px;
        }

        .sidebar ul li a:hover {
            color: var(--accent-color);
            padding-left: 10px;
        }

        .sidebar ul li a.active {
            color: var(--accent-color);
            padding-left: 10px;
            font-weight: bold;
            background-color: rgba(0, 0, 0, 0.05);
        }

        .dark-mode .sidebar ul li a.active {
            background-color: rgba(255, 255, 255, 0.08);
        }

        /* Main Content Area */
        .main-content {
            margin-left: 300px;
            padding: 2rem 3rem;
        }

        .hero-section {
            text-align: center;
            padding: 4rem 1rem;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .hero-section h1 {
            font-size: 3.5rem;
            margin-bottom: 0.5rem;
        }

        .hero-section p {
            font-size: 1.3rem;
            opacity: 0.9;
        }

        /* Syllabus Bar */
        .syllabus-bar-container {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 0.75rem;
            background-color: var(--card-bg);
            border-radius: 0.5rem;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.07);
            border: 1px solid var(--border-color);
        }

        .syllabus-bar-back-link {
            display: flex;
            align-items: center;
            font-size: 0.875rem;
            color: #2563eb;
            font-weight: 500;
            padding: 0.5rem 0.75rem;
            border-radius: 0.375rem;
            text-decoration: none;
            transition: background-color 0.15s, color 0.15s;
            margin-bottom: 0.5rem;
        }

        .dark-mode .syllabus-bar-back-link {
            color: #5dade2;
        }

        .dark-mode .syllabus-bar-back-link:hover {
            color: #8ecae6;
            background-color: rgba(255, 255, 255, 0.1);
        }

        .syllabus-bar-back-link:hover {
            color: #1d4ed8;
            background-color: #eff6ff;
        }

        .syllabus-bar-back-link svg {
            height: 1.25rem;
            width: 1.25rem;
            margin-right: 0.375rem;
            fill: currentColor;
        }

        .syllabus-bar-topic-badge {
            background-color: #2563eb;
            color: white;
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.375rem 1rem;
            border-radius: 9999px;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }

        @media (min-width: 640px) {
            .syllabus-bar-container {
                flex-direction: row;
            }

            .syllabus-bar-back-link {
                margin-bottom: 0;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.875rem;
            }
        }

        /* General Content Styles */
        section {
            margin-bottom: 3rem;
            padding-top: 70px;
            margin-top: -70px;
        }

        h2 {
            font-size: 2.2rem;
            color: var(--primary-color);
            border-bottom: 3px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            font-size: 1.7rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        h4 {
            font-size: 1.3rem;
            color: var(--accent-color);
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
        }

        h5 {
            font-size: 1.1rem;
            color: var(--primary-color);
            opacity: 0.9;
            margin-top: 1.2rem;
            margin-bottom: 0.7rem;
        }

        p,
        li {
            font-size: 1.05rem;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        ul,
        ol {
            padding-left: 25px;
        }

        .callout {
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 5px solid var(--accent-color);
            background-color: var(--card-bg);
            border-radius: 0 5px 5px 0;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
        }

        .callout.info {
            border-left-color: var(--primary-color);
        }

        .callout.success {
            border-left-color: var(--secondary-color);
        }

        .callout.warning {
            border-left-color: #f39c12;
        }

        pre {
            background-color: var(--code-bg);
            color: var(--text-color);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9em;
            border: 1px solid var(--border-color);
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
        }

        .highlight {
            background-color: var(--accent-color);
            color: white;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-weight: bold;
        }

        /* Diagram Styles */
        .diagram {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin: 1.5rem auto;
            max-width: 600px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.07);
        }

        .diagram img,
        .diagram-placeholder {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-top: 10px;
        }

        .diagram-placeholder {
            border: 2px dashed var(--border-color);
            padding: 20px;
            min-height: 150px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--text-color);
            opacity: 0.7;
        }

        .table-like-diagram {
            width: 100%;
            overflow-x: auto;
        }

        .table-like-diagram table {
            width: 100%;
            min-width: 500px;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        .table-like-diagram th,
        .table-like-diagram td {
            border: 1px solid var(--border-color);
            padding: 8px;
            text-align: left;
        }

        .table-like-diagram th {
            background-color: var(--secondary-color);
            color: white;
        }

        /* Footer Styles */
        .new-footer-container {
            margin-top: 4rem;
            padding-top: 2.5rem;
            border-top: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            padding-bottom: 1rem;
        }

        .new-footer-buttons-wrapper {
            display: flex;
            flex-direction: column;
            width: 100%;
            align-items: center;
        }

        .new-footer-button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 0.75rem 1.5rem;
            background-color: #2563eb;
            color: white !important;
            border-radius: 9999px;
            text-decoration: none;
            transition: background-color 0.15s, box-shadow 0.15s;
            font-weight: 500;
            font-size: 0.875rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            min-width: 280px;
            text-align: center;
            margin-bottom: 0.5rem;
        }

        .new-footer-button:hover {
            background-color: #1d4ed8;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        .new-footer-button:focus {
            outline: 2px solid #3b82f6;
            outline-offset: 2px;
        }

        .new-footer-print-link {
            margin-top: 1rem;
            color: var(--primary-color);
            text-decoration: none;
            font-size: 0.9rem;
        }

        .new-footer-print-link:hover {
            text-decoration: underline;
        }

        @media (min-width: 640px) {
            .new-footer-buttons-wrapper {
                flex-direction: row;
                justify-content: space-between;
                gap: 1rem;
            }

            .new-footer-button {
                margin-bottom: 0;
            }
        }

        /* Quiz Styles */
        .quiz-container {
            background-color: var(--card-bg);
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .quiz-question {
            margin-bottom: 15px;
        }

        .quiz-options label {
            display: block;
            margin-bottom: 8px;
            cursor: pointer;
        }

        .quiz-options input {
            margin-right: 8px;
        }

        .quiz-feedback {
            margin-top: 10px;
            font-weight: bold;
        }

        /* Responsive & Print Styles (Same as previous, condensed for brevity) */
        @media (max-width: 992px) {
            .sidebar {
                width: 100%;
                height: auto;
                position: static;
                border-right: none;
                border-bottom: 1px solid var(--border-color);
                box-shadow: none;
                top: auto;
            }

            .main-content {
                margin-left: 0;
                padding: 1.5rem;
            }

            .navbar {
                padding: 0.8rem 1rem;
            }

            .navbar-brand {
                font-size: 1.5rem;
            }

            .nav-left-spacer,
            .theme-toggle {
                flex-basis: 40px;
            }
        }

        @media (max-width: 600px) {
            .eduvishion-logo {
                top: 12px;
                left: 12px;
            }

            .eduvishion-logo .text-2xl {
                font-size: 1.3rem;
            }

            .eduvishion-logo svg {
                height: 1.2em;
                width: 1.2em;
            }

            .navbar {
                padding: 1rem;
                justify-content: center;
                position: relative;
            }

            .nav-left-spacer {
                display: none;
            }

            .navbar-brand {
                flex-grow: 0;
                margin-right: auto;
                margin-left: 50px;
                font-size: 1.3rem;
            }

            .theme-toggle {
                position: absolute;
                right: 1rem;
                top: 50%;
                transform: translateY(-50%);
                flex-basis: auto;
                font-size: 1.2rem;
            }

            .main-content {
                padding: 1rem;
            }

            h2 {
                font-size: 1.8rem;
            }

            h3 {
                font-size: 1.5rem;
            }

            .syllabus-bar-container {
                padding: 0.5rem;
            }

            .syllabus-bar-back-link {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.7rem;
                padding: 0.3rem 0.8rem;
            }

            .new-footer-button {
                font-size: 0.8rem;
                padding: 0.6rem 1.2rem;
                min-width: auto;
                width: 90%;
            }

            .new-footer-buttons-wrapper {
                flex-direction: column;
            }

            .new-footer-buttons-wrapper .new-footer-button:first-child {
                margin-bottom: 0.5rem;
            }
        }

        @media print {
            body {
                font-size: 10pt;
                color: #000 !important;
                background-color: #fff !important;
            }

            .navbar,
            .sidebar,
            .theme-toggle,
            .hero-section .btn,
            .quiz-container,
            .new-footer-container,
            .eduvishion-logo,
            .syllabus-bar-container,
            .new-footer-print-link {
                display: none;
            }

            .main-content {
                margin-left: 0;
                padding: 0;
            }

            section {
                padding-top: 0;
                margin-top: 0;
                margin-bottom: 1.5rem;
                page-break-after: auto;
            }

            h1,
            h2,
            h3,
            h4 {
                color: #000 !important;
                border: none !important;
                page-break-after: avoid;
            }

            .callout {
                border-left: 3px solid #ccc !important;
                background-color: #f9f9f9 !important;
            }

            a {
                text-decoration: none;
                color: #000 !important;
            }

            a[href^="http"]:after {
                content: " (" attr(href) ")";
            }

            pre {
                background-color: #f0f0f0 !important;
                border: 1px solid #ccc !important;
                color: #000 !important;
            }

            .diagram,
            .diagram img,
            .table-like-diagram table {
                page-break-inside: avoid;
            }
        }
    </style>
</head>

<body>

    <div class="eduvishion-logo">
        <div class="text-2xl font-bold">
            <a href="../../index.html" class="flex items-center group">
                <span class="text-white">Edu</span>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"
                    fill="none">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
                    <path stroke-linecap="round" stroke-linejoin="round"
                        d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z" />
                </svg>
                <span class="text-white">ision</span>
            </a>
        </div>
    </div>

    <nav class="navbar">
        <div class="nav-left-spacer"></div>
        <a href="#" class="navbar-brand"><i class="fas fa-cogs"></i> Machine Learning Fundamentals</a>
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
            <i class="fas fa-moon"></i>
        </button>
    </nav>

    <aside class="sidebar" id="sidebar">
        <h3>Table of Contents</h3>
        <ul id="toc"></ul>
    </aside>

    <main class="main-content">
        <header class="hero-section">
            <h1>Machine Learning Fundamentals</h1>
            <p>Understanding the core concepts, types, and processes that power Machine Learning.</p>
        </header>

        <div class="syllabus-bar-container">
            <a href="../ai.html" class="syllabus-bar-back-link"> <!-- Ensure this path is correct -->
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20">
                    <path fill-rule="evenodd"
                        d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z"
                        clip-rule="evenodd" />
                </svg>
                Back to Syllabus
            </a>
            <div class="syllabus-bar-topic-badge">
                Topic 6
            </div>
        </div>

        <section id="intro-ml">
            <h2>Introduction to Machine Learning</h2>
            <p>Machine learning is a branch of Artificial Intelligence that focuses on developing models and algorithms
                that let computers learn from data without being explicitly programmed for every task. In simple words,
                ML teaches the systems to think and understand like humans by learning from the data.</p>
        </section>

        <section id="types-of-ml">
            <h2>Types of Machine Learning</h2>
            <p>Machine Learning can be broadly categorized into four main types:</p>
            <div class="diagram">
                <h4>Types of Machine Learning</h4>
                <div class="diagram-placeholder">Image: Tree diagram showing Supervised, Unsupervised, Semi-Supervised,
                    Reinforcement Learning.</div>
                <!-- You would replace the placeholder with: <img src="path/to/types_of_ml_diagram.png" alt="Types of Machine Learning"> -->
            </div>
            <ul>
                <li><strong class="highlight">Supervised Learning:</strong> Trains models on labeled data to predict or
                    classify new, unseen data.</li>
                <li><strong class="highlight">Unsupervised Learning:</strong> Finds patterns or groups in unlabeled
                    data, like clustering or dimensionality reduction.</li>
                <li><strong class="highlight">Reinforcement Learning:</strong> Learns through trial and error to
                    maximize rewards, ideal for decision-making tasks.</li>
                <li><strong class="highlight">Semi-Supervised Learning:</strong> The model generates its own labels from
                    the data, or uses a small amount of labeled data with a large amount of unlabeled data. (Note: Often
                    considered a hybrid approach, it has become a major category in deep learning and fields like NLP
                    and computer vision).</li>
            </ul>

            <!-- Content for Supervised, Unsupervised, Reinforcement Learning now directly visible -->
            <h3>1. Supervised Learning</h3>
            <p>Supervised machine learning is a fundamental approach that involves training a model using labeled data,
                where each input comes with a corresponding correct output. The process is like a teacher guiding a
                student.</p>
            <div class="diagram">
                <h4>Supervised Machine Learning Workflow</h4>
                <div class="diagram-placeholder">Image: Labeled Data (Input Raw Data with Labels) → Algorithm →
                    Processing → Output (Predicted classifications).</div>
            </div>
            <h4>How Supervised Learning Works:</h4>
            <ol>
                <li><strong>Training Data:</strong> The model is provided with a training dataset that includes input
                    data (features) and corresponding output data (labels or target variables).</li>
                <li><strong>Learning Process:</strong> The algorithm processes the training data, learning the
                    relationships between the input features and the output labels. This is achieved by adjusting the
                    model's parameters to minimize the difference between its predictions and the actual labels.</li>
            </ol>
            <p>After training, the model is evaluated using a <strong class="highlight">test dataset</strong> to measure
                its accuracy and performance. Techniques like <strong class="highlight">cross-validation</strong> are
                used to balance bias and variance, ensuring the model generalizes well to new, unseen data. While
                training the model, data is usually split in the ratio of 80:20 i.e. 80% as training data and the rest
                as testing data.</p>
            <div class="diagram">
                <h4>Supervised Learning: Training vs. Testing</h4>
                <div class="diagram-placeholder">Image: INPUT → Algorithm (TRAINING) → LOGIC; INPUT → LOGIC (TESTING) →
                    OUTPUT.</div>
            </div>
            <h4>Types of Supervised Learning Problems:</h4>
            <div class="diagram">
                <div class="diagram-placeholder">Image: Supervised Learning branching to Classification (defined Labels)
                    and Regression (no Labels defined / continuous output).</div>
            </div>
            <ul>
                <li><strong>Classification:</strong> Where the output is a categorical variable (e.g., spam vs. non-spam
                    emails, yes vs. no).</li>
                <li><strong>Regression:</strong> Where the output is a continuous variable (e.g., predicting house
                    prices, stock prices).</li>
            </ul>
            <div class="callout info">
                <h4>Example Datasets for Supervised Learning:</h4>
                <p><strong>Figure A (Classification - Shopping):</strong> A dataset of a shopping store that is useful
                    in predicting whether a customer will purchase a particular product under consideration or not based
                    on his/her gender, age, and salary. Input: Gender, Age, Salary. Output: Purchased (0 or 1).</p>
                <p><strong>Figure B (Regression - Meteorological):</strong> A meteorological dataset that serves the
                    purpose of predicting wind speed based on different parameters. Input: Dew Point, Temperature,
                    Pressure, Relative Humidity, Wind Direction. Output: Wind Speed.</p>
                <div class="diagram table-like-diagram">
                    <div class="diagram-placeholder">Image: Table showing User ID, Gender, Age, Salary, Purchased
                        (Figure A) and Temp, Pressure, etc., Wind Speed (Figure B).</div>
                </div>
            </div>

            <h4>Deep Dive: Classification</h4>
            <p>Classification teaches a machine to sort things into categories. It learns by looking at examples with
                labels (like emails marked "spam" or "not spam"). After learning, it can decide which category new items
                belong to, like identifying if a new email is spam or not. For example, a classification model might be
                trained on a dataset of images labeled as either dogs or cats and it can be used to predict the class of
                new and unseen images as dogs or cats based on their features such as color, texture, and shape.</p>
            <div class="diagram">
                <h4>Classification: Example of Dogs vs. Cats</h4>
                <div class="diagram-placeholder">Image: Scatter plot with Feature 1 (e.g., Color/Texture) and Feature 2
                    (e.g., Shape/Size) showing decision boundaries for dog/cat classification.</div>
                <p style="font-size:0.9em; margin-top:5px;">Explaining classification in ML, the horizontal axis
                    represents the combined values of color and texture features. The vertical axis represents the
                    combined values of shape and size features. Each colored dot represents an individual image. The
                    shaded areas show the decision boundary, which is the line or region that the model uses to decide
                    which category an image belongs to.</p>
            </div>
            <h5>Types of Classification:</h5>
            <ol>
                <li><strong>Binary Classification:</strong> Sorts data into two distinct categories (e.g., spam or not
                    spam). It works by looking at different features of the email like certain keywords or sender
                    details, and decides whether it's spam or not.</li>
                <li><strong>Multiclass Classification:</strong> Sorts data into more than two categories (e.g., cat,
                    dog, bird). Basically, the machine looks at the features in the image (like shape, color, or
                    texture) and chooses which animal the picture is most likely to be based on the training it
                    received.</li>
                <li><strong>Multi-Label Classification:</strong> A single piece of data can belong to multiple
                    categories at once (e.g., a movie recommendation system could tag a movie as both action and
                    comedy). This is relevant in specific use cases but not as crucial for a starting overview.</li>
            </ol>
            <div class="diagram">
                <h4>Binary vs. Multiclass Classification</h4>
                <div class="diagram-placeholder">Image: Two plots side-by-side, one showing binary classification (e.g.,
                    squares vs. circles) and another showing multiclass classification (e.g., squares, circles,
                    triangles).</div>
            </div>
            <h5>How Classification in Machine Learning Works:</h5>
            Classification involves training a model using a labeled dataset, where each input is paired with its
            correct output label. The model learns patterns and relationships from this data, so it can later predict
            labels for new, unseen inputs.
            <ol>
                <li><strong>Data Collection:</strong> You start with a dataset where each item is labeled with the
                    correct class (for example, "cat" or "dog").</li>
                <li><strong>Feature Extraction:</strong> The system identifies features (like color, shape, or texture)
                    that help distinguish one class from another. These features are what the model uses to make
                    predictions.</li>
                <li><strong>Model Training:</strong> The classification machine learning algorithm uses the labeled data
                    to learn how to map the features to the correct class. It looks for patterns and relationships in
                    the data.</li>
                <li><strong>Model Evaluation:</strong> Once the model is trained, it's tested on new, unseen data to
                    check how accurately it can classify these items.</li>
                <li><strong>Prediction:</strong> After being trained and evaluated, the model can be used to predict the
                    class of new data based on the features it has learned.</li>
                <li><strong>Model Evaluation (ongoing):</strong> Evaluating a classification model is a key step. It
                    helps us check how well the model performs and how good it is at handling new, unseen data.
                    Depending on the problem and needs, we can use different metrics to measure its performance.</li>
            </ol>
            <div class="diagram">
                <h4>ML Classification Workflow</h4>
                <div class="diagram-placeholder">Image: Flowchart: Training Data (y, X) → Feature Extraction (h(x)) → ML
                    Model (W) ↔ (Feedback from) ML Algorithm ↔ (Input to) Quality Metric ← (Output ŷ from) ML Model.
                </div>
            </div>
            <h5>Examples of Machine Learning Classification in Real Life:</h5>
            <ul>
                <li>Email spam filtering</li>
                <li>Credit risk assessment</li>
                <li>Medical diagnosis</li>
                <li>Image classification (facial recognition, autonomous driving)</li>
                <li>Sentiment analysis</li>
                <li>Fraud detection</li>
                <li>Recommendation systems</li>
            </ul>
            <h5>Classification Modeling in Machine Learning:</h5>
            Key characteristics of Classification Models:
            <ol>
                <li><strong>Class Separation:</strong> Relies on distinguishing between distinct classes.</li>
                <li><strong>Decision Boundaries:</strong> The model draws boundaries (linear or non-linear) in the
                    feature space.</li>
                <li><strong>Sensitivity to Data Quality:</strong> Performance depends on the quality and quantity of
                    labeled data.</li>
                <li><strong>Handling Imbalanced Data:</strong> Special techniques (e.g., resampling, weighting) may be
                    needed if classes are underrepresented.</li>
                <li><strong>Interpretability:</strong> Some algorithms (e.g., Decision Trees) offer higher
                    interpretability.</li>
            </ol>
            <h5>Classification Algorithms:</h5>
            <p>For implementation of any classification model, it is essential to understand algorithms like Logistic
                Regression. Some common ones include:</p>
            <p><strong>Linear Classifiers:</strong> Create a linear decision boundary.</p>
            <ul>
                <li>Logistic Regression</li>
                <li>Support Vector Machines (SVM) having kernel = 'linear'</li>
                <li>Single-layer Perceptron</li>
                <li>Stochastic Gradient Descent (SGD) Classifier</li>
            </ul>
            <p><strong>Non-linear Classifiers:</strong> Create a non-linear decision boundary.</p>
            <ul>
                <li>K-Nearest Neighbors (KNN)</li>
                <li>Kernel SVM</li>
                <li>Naive Bayes</li>
                <li>Decision Tree Classification</li>
                <li>Ensemble learning classifiers: Random Forests, AdaBoost, Bagging Classifier, Voting Classifier,
                    Extra Trees Classifier</li>
                <li>Multi-layer Artificial Neural Networks</li>
            </ul>

            <h4>Deep Dive: Regression</h4>
            <p>Regression in machine learning refers to a supervised learning technique where the goal is to predict a
                continuous numerical value based on one or more independent features. It finds relationships between
                variables so that predictions can be made.</p>
            <ul>
                <li><strong>Dependent Variable (Target):</strong> The variable we are trying to predict (e.g., house
                    price, salary, weight).</li>
                <li><strong>Independent Variables (Features):</strong> The input variables that influence the prediction
                    (e.g., locality, number of rooms for house price).</li>
            </ul>
            <h5>Types of Regression:</h5>
            <ol>
                <li><strong>Simple Linear Regression:</strong> Models linear relationship between one independent and
                    one dependent variable.</li>
                <li><strong>Multiple Linear Regression:</strong> Uses multiple independent variables.</li>
                <li><strong>Polynomial Regression:</strong> Models non-linear relationships using polynomial terms.</li>
                <li><strong>Ridge & Lasso Regression:</strong> Regularized versions of linear regression to prevent
                    overfitting by penalizing large coefficients.</li>
                <li><strong>Support Vector Regression (SVR):</strong> Based on SVM, finds a hyperplane that minimizes
                    the sum of squared residuals between predicted and actual values.</li>
                <li><strong>Decision Tree Regression:</strong> Uses a tree-like structure to make decisions.</li>
                <li><strong>Random Forest Regression:</strong> An ensemble method building multiple decision trees.</li>
            </ol>
            <div class="callout info">
                <h5>Python Code Example: Linear Regression for Housing Prices</h5>
                <pre><code>
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg') # Or another backend like 'Qt5Agg', 'Agg'
import numpy as np
from sklearn import linear_model # datasets can be imported if needed
import pandas as pd

# Assume housing.csv has 'lotsize' and 'price' columns
# For demonstration, create dummy data if file not present
try:
    df = pd.read_csv('housing.csv') # Make sure 'housing.csv' is in the same directory or provide full path
except FileNotFoundError:
    print("housing.csv not found, using dummy data for demonstration.")
    data = {'lotsize': np.random.randint(500, 2000, 100) + np.random.rand(100)*500, 
            'price': 50000 + (150 * (np.random.randint(500, 2000, 100) + np.random.rand(100)*500)) + np.random.normal(0, 50000, 100)}
    df = pd.DataFrame(data)
    df = df[df['price'] > 0] # Ensure price is positive

X = df[['lotsize']] 
Y = df['price']

# Reshape for scikit-learn (if using a single feature)
X_np = X.values.reshape(-len(X), 1) # Use -len(X) for automatic row detection with 1 column
Y_np = Y.values

# Split data (simplified for example: first 80 for train, rest for test)
split_idx = int(len(X_np) * 0.8)
X_train, X_test = X_np[:split_idx], X_np[split_idx:]
Y_train, Y_test = Y_np[:split_idx], Y_np[split_idx:]


# Train Linear Regression model
regr = linear_model.LinearRegression()
regr.fit(X_train, Y_train)

# Plot predictions
plt.figure(figsize=(10, 6))
plt.scatter(X_test, Y_test, color='black', label='Actual Data', alpha=0.7)
plt.plot(X_test, regr.predict(X_test), color='red', linewidth=2, label='Regression Line')
plt.xlabel('Lot Size (sq ft)')
plt.ylabel('Price ($)')
plt.title('Linear Regression: Housing Price vs. Lot Size')
plt.legend()
plt.grid(True)
plt.show()
                </code></pre>
                <div class="diagram">
                    <div class="diagram-placeholder">Image: Scatter plot with regression line (Output of the Python code
                        showing price vs. size).</div>
                </div>
            </div>
            <h5>Applications of Regression:</h5> Predicting prices (houses, stocks), forecasting trends (sales),
            identifying risk factors (medical data), making decisions (stock recommendations).
            <h5>Advantages of Regression:</h5> Easy to understand and interpret, robust to outliers (some advanced
            types), can handle both linear relationships easily.
            <h5>Disadvantages of Regression:</h5> Assumes linearity (for basic linear regression), sensitive to
            multicollinearity (highly correlated independent variables), may not be suitable for highly complex
            non-linear relationships without modifications.

            <h5>Practical Examples of Supervised Learning:</h5>
            <ul>
                <li><strong>Fraud Detection in Banking:</strong> Uses historical transaction data (labeled
                    legitimate/fraudulent) to predict fraud patterns.</li>
                <li><strong>Parkinson Disease Prediction:</strong> Models trained on patient data to predict onset or
                    progression.</li>
                <li><strong>Customer Churn Prediction:</strong> Analyzes historical customer data to identify features
                    associated with churn.</li>
                <li><strong>Cancer Cell Classification:</strong> Classifies cells as 'malignant' or 'benign' based on
                    features.</li>
                <li><strong>Stock Price Prediction:</strong> Predicts signals indicating whether buying a stock will be
                    helpful.</li>
            </ul>
            <h5>Common Supervised Learning Algorithms (Summary):</h5>
            Linear Regression, Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM),
            K-Nearest Neighbors (KNN), Gradient Boosting, Naive Bayes.
            <h5>Advantages and Disadvantages of Supervised Learning (Overall):</h5>
            <p><strong>Advantages:</strong> Accurately predicts patterns and makes data-driven decisions. Labeled data
                enables learning input-output relationships. Encompasses classification and regression for complex
                problems. Established evaluation metrics are available.</p>
            <p><strong>Disadvantages:</strong> Overfitting. Feature Engineering can be complex and time-consuming. Bias
                in models if data is biased. Heavy dependence on labeled training data, which can be costly and
                time-consuming to obtain.</p>

            <h3>2. Unsupervised Learning</h3>
            <p>Unsupervised learning is a branch of machine learning that deals with unlabeled data. Unlike supervised
                learning, where the data is labeled with a specific category or outcome, unsupervised learning
                algorithms are tasked with finding patterns and relationships within the data without any prior
                knowledge of the data's meaning. These algorithms find hidden patterns and data without any human
                intervention; we don’t give output to our model. The model only has input parameter values and discovers
                patterns on its own.</p>
            <div class="diagram">
                <h4>Unsupervised Learning Workflow</h4>
                <div class="diagram-placeholder">Image: Input Raw Data (e.g., animals) → Interpretation (Unknown Output,
                    No Training Data Set) → Algorithm → Processing → Output (Clustered data, e.g., animals grouped by
                    species).</div>
            </div>
            <h4>How Unsupervised Learning Works:</h4>
            <p>It works by analyzing unlabeled data to identify patterns and relationships. The data is not labeled with
                any predefined categories or outcomes, so the algorithm must find these patterns and relationships on
                its own. For example, using mall data that contains information about its clients, unsupervised learning
                techniques can easily group clients based on parameters like age, income, and spending score.</p>
            <ul>
                <li><strong>Unstructured data:</strong> May contain noisy (meaningless) data, missing values, or unknown
                    data.</li>
                <li><strong>Unlabeled data:</strong> Data only contains values for input parameters; there is no
                    targeted value (output). It is easy to collect compared to labeled data in the Supervised approach.
                </li>
            </ul>
            <h4>Types of Unsupervised Learning Algorithms:</h4>
            <ul>
                <li><strong>Clustering:</strong> Grouping similar data points together (e.g., K-Means, Hierarchical
                    Clustering, DBSCAN).</li>
                <li><strong>Association Rule Learning:</strong> Discovering interesting relations between variables in
                    large databases (e.g., Apriori, Eclat - "customers who bought X also bought Y").</li>
                <li><strong>Dimensionality Reduction:</strong> Reducing the number of random variables under
                    consideration by obtaining a set of principal variables (e.g., Principal Component Analysis (PCA),
                    t-distributed Stochastic Neighbor Embedding (t-SNE)).</li>
            </ul>
            <h4>Challenges of Unsupervised Learning:</h4>
            Noisy Data, Assumption Dependence, Overfitting Risk, Limited Guidance, Cluster Interpretability, Sensitivity
            to Parameters, Lack of Ground Truth for evaluation.
            <h4>Applications of Unsupervised Learning:</h4>
            Customer Segmentation, Anomaly Detection, Recommendation Systems, Image and Text Clustering, Social Network
            Analysis, Astronomy and Climate Science.

            <h3>3. Reinforcement Learning (RL)</h3>
            <p>Reinforcement Learning (RL) is a branch of machine learning that focuses on how agents can learn to make
                decisions through trial and error to maximize cumulative rewards. RL allows machines to learn by
                interacting with an environment and receiving feedback based on their actions. This feedback comes in
                the form of rewards or penalties.</p>
            <div class="diagram">
                <h4>Reinforcement Learning Loop</h4>
                <div class="diagram-placeholder">Image: Agent sends Action to Environment; Environment returns State and
                    Reward to Agent (possibly via an Interpreter).</div>
            </div>
            <p>RL revolves around the idea that an agent (the learner or decision-maker) interacts with an environment
                to achieve a goal. The agent performs actions and receives feedback to optimize its decision-making over
                time.</p>
            <h4>Key Components:</h4>
            <ul>
                <li><strong>Agent:</strong> The decision-maker that performs actions.</li>
                <li><strong>Environment:</strong> The world or system in which the agent operates.</li>
                <li><strong>State:</strong> The current situation or condition the agent is currently in.</li>
                <li><strong>Action:</strong> The possible moves or decisions the agent can make.</li>
                <li><strong>Reward:</strong> The feedback or result from the environment based on the agent's action.
                </li>
                <li><strong>Policy:</strong> A strategy that the agent uses to determine the next action based on the
                    current state.</li>
                <li><strong>Value Function:</strong> Estimates the future cumulative rewards the agent will receive from
                    a given state.</li>
                <li><strong>Model of the Environment (optional):</strong> A representation of the environment that
                    predicts future states and rewards, aiding in planning.</li>
            </ul>
            <div class="callout info">
                <h5>Example: Reinforcement Learning - Navigating a Maze</h5>
                <p>Imagine a robot navigating a maze to reach a diamond while avoiding fire hazards. The goal is to find
                    the optimal path with the least number of hazards while maximizing the reward:</p>
                <ul>
                    <li>Each time the robot moves correctly, it receives a reward.</li>
                    <li>If the robot takes the wrong path, it loses points.</li>
                </ul>
                <p>The robot's learning process can be summarized as:</p>
                <ol>
                    <li><strong>Exploration:</strong> The robot starts by exploring all possible paths.</li>
                    <li><strong>Feedback:</strong> After each move, the robot receives feedback (reward/penalty).</li>
                    <li><strong>Adjusting Behavior:</strong> Based on feedback, the robot adjusts its behavior to
                        maximize cumulative reward.</li>
                    <li><strong>Optimal Path:</strong> Eventually, the robot discovers the optimal path.</li>
                </ol>
                <div class="diagram">
                    <div class="diagram-placeholder">Image: Grid maze with robot, fire hazards, and a diamond.</div>
                </div>
            </div>
            <h4>Types of Reinforcements in RL:</h4>
            <ol>
                <li><strong>Positive Reinforcement:</strong> Defined as when an event, occurring due to a particular
                    behavior, increases the strength and frequency of that behavior.</li>
                <li><strong>Negative Reinforcement:</strong> Defined as strengthening of behavior because a negative
                    condition is stopped or avoided.</li>
            </ol>
            <h4>Applications of Reinforcement Learning:</h4>
            Robotics, Game Playing (e.g., AlphaGo), Industrial Control, Personalized Training Systems.
            <h4>Advantages of Reinforcement Learning:</h4>
            Solving highly complex problems, error correction through continuous learning, direct interaction with the
            environment, handling non-deterministic environments.
            <h4>Disadvantages of Reinforcement Learning:</h4>
            Often overkill for simple problems, high computational requirements, strong dependency on reward function
            design, difficulty in debugging and interpretation.
        </section>

        <section id="ml-workflow">
            <h2>The Machine Learning Workflow (Lifecycle)</h2>
            <p>The Machine learning lifecycle is a process that guides development and deployment of machine learning
                models in a structured way. It consists of various steps. Each step plays a crucial role in ensuring the
                success and effectiveness of the machine learning model. By following the machine learning lifecycle we
                can solve complex problems, can get data-driven insights and create scalable and sustainable models. The
                steps are:</p>
            <div class="diagram">
                <h4>Machine Learning Lifecycle</h4>
                <div class="diagram-placeholder">Image: Circular diagram of ML Lifecycle: Problem Definition → Data
                    Collection → Data Cleaning and Preprocessing → Exploratory Data Analysis (EDA) → Feature Engineering
                    and Selection → Model Selection → Model Training → Model Evaluation and Tuning → Model Deployment →
                    Model Monitoring and Maintenance (looping back).</div>
            </div>
            <h4>Steps in the ML Lifecycle:</h4>
            <ol>
                <li><strong>Problem Definition:</strong> Identify and frame the business problem. Crucial elements such
                    as project objectives, desired outcomes and the scope of the task are carefully designed during this
                    stage.</li>
                <li><strong>Data Collection:</strong> Systematic collection of datasets relevant to the defined problem,
                    ensuring quality, diversity, and necessary features.</li>
                <li><strong>Data Cleaning and Preprocessing:</strong> Raw data is often messy. This involves addressing
                    missing values, outliers, inconsistencies, standardizing formats, scaling values, and encoding
                    categorical variables to create a consistent and well-organized dataset.</li>
                <li><strong>Exploratory Data Analysis (EDA):</strong> Use statistical and visual tools to uncover
                    insights, patterns, trends, and understand the dataset's structure. EDA helps in feature engineering
                    and model selection.</li>
                <li><strong>Feature Engineering and Selection:</strong> A transformative process involving selecting
                    only relevant features, creating new features, or transforming existing ones to enhance model
                    efficiency and effectiveness.</li>
                <li><strong>Model Selection:</strong> Choose an ML algorithm that aligns with the defined problem and
                    dataset characteristics, considering complexity, performance, interpretability, and scalability.
                </li>
                <li><strong>Model Training:</strong> Expose the selected model to historical data, allowing it to learn
                    patterns, relationships, and dependencies. This is an iterative process where the algorithm adjusts
                    its parameters to minimize errors and enhance predictive accuracy.</li>
                <li><strong>Model Evaluation and Tuning:</strong> Rigorous testing against validation or test datasets
                    to check accuracy on new unseen data. Use metrics like accuracy, precision, recall, and F1 score. If
                    performance is unsatisfactory, tune hyperparameters or revisit previous steps.</li>
                <li><strong>Model Deployment:</strong> Integrate the trained and validated model into existing systems
                    or processes for real-world application and informed decision-making.</li>
                <li><strong>Model Monitoring and Maintenance:</strong> Continuously monitor model performance in
                    production and make adjustments or retrain as necessary to maintain effectiveness over time.</li>
            </ol>
        </section>

        <section id="data-preprocessing-feature-engineering">
            <h2>Data Preprocessing and Feature Engineering</h2>
            <h3>Data Preprocessing</h3>
            <p>Data preprocessing involves transforming raw data into a clean and usable format. Raw data is often
                messy, containing missing values, outliers, noise, and inconsistencies. Preprocessing aims to resolve
                these issues to ensure that your data is suitable for analysis and model building.</p>
            <h4>Main Steps in Data Preprocessing:</h4>
            <ul>
                <li><strong>Data Cleaning:</strong> Handling missing values, correcting errors, and addressing outliers.
                </li>
                <li><strong>Data Transformation:</strong> Normalizing, scaling, or encoding data to make it suitable for
                    the machine learning model.</li>
                <li><strong>Data Reduction:</strong> Reducing the complexity of the data by selecting important features
                    or dimensionality reduction.</li>
            </ul>
            <h4>Data Cleaning Techniques:</h4>
            <ol>
                <li><strong>Handling Missing Data:</strong>
                    <ul>
                        <li>Removing Missing Data: If minimal, remove rows/columns. Not ideal if significant.</li>
                        <li>Imputing Missing Values: Replace with mean, median, or mode. Advanced techniques like KNN
                            imputation can be used.</li>
                        <li>Using Algorithms that Handle Missing Data: Some algorithms (e.g., decision trees) can handle
                            missing data directly.</li>
                    </ul>
                </li>
                <li><strong>Handling Outliers:</strong>
                    <ul>
                        <li>Removing Outliers: Using statistical methods like Z-score or Interquartile Range (IQR).</li>
                        <li>Capping or Clipping: Replace outliers with the nearest value within a predefined range.</li>
                        <li>Transforming Data: Log transformation or scaling can help reduce the impact of outliers.
                        </li>
                    </ul>
                </li>
                <li><strong>Data Encoding (for categorical data):</strong>
                    <ul>
                        <li>Label Encoding: Assigns a unique integer to each category. Can introduce unintended ordinal
                            relationships.</li>
                        <li>One-Hot Encoding: Creates binary columns for each category. Avoids ordinal issues.</li>
                        <li>Target Encoding: Replaces categories with the mean of the target variable for that category.
                        </li>
                    </ul>
                </li>
            </ol>
            <h4>Data Transformation Techniques:</h4>
            <ol>
                <li><strong>Feature Scaling:</strong> Crucial for algorithms sensitive to input scale (e.g., SVM, KNN).
                    <ul>
                        <li>Normalization: Rescales data to a range of [0, 1]. Useful when maintaining distribution.
                        </li>
                        <li>Standardization: Centers data around the mean with a unit variance. Useful when data follows
                            a normal distribution.</li>
                    </ul>
                </li>
                <li><strong>Log Transformation:</strong> Helps reduce skewness and stabilize variance, useful for skewed
                    distributions or outliers.</li>
                <li><strong>Binning:</strong> Divides continuous features into discrete intervals or bins. Reduces
                    impact of small fluctuations and can improve robustness.</li>
            </ol>

            <h3>Feature Engineering</h3>
            <p>Feature engineering is the process of creating new features or transforming existing features to improve
                the performance of a machine-learning model. It involves selecting relevant information from raw data
                and transforming it into a format that can be easily understood by a model. The goal is to improve model
                accuracy and relevance.</p>
            <div class="diagram">
                <h4>Feature Engineering Process</h4>
                <div class="diagram-placeholder">Image: Flow: Raw Data → Feature Engineering (looping process of
                    transforming/creating) → Features → Modeling.</div>
            </div>
            <h4>What is a Feature?</h4>
            <p>A feature (also known as a variable or attribute) is an individual measurable property or characteristic
                of a data point that is used as input for a machine learning algorithm. Features can be numerical,
                categorical, or text-based.</p>
            <h4>Need for Feature Engineering:</h4>
            Improve User Experience, Gain Competitive Advantage, Meet Customer Needs, Increase Revenue, Future-Proofing.
            <h4>Processes Involved in Feature Engineering:</h4>
            Feature engineering consists of mainly 5 processes: Feature Creation, Feature Transformation, Feature
            Extraction, Feature Selection, and Feature Scaling.
            <ol>
                <li><strong>Feature Creation:</strong> Generating new features based on domain knowledge or by observing
                    patterns in the data (Domain-Specific, Data-Driven, Synthetic). Improves model performance,
                    robustness, interpretability, and flexibility.</li>
                <li><strong>Feature Transformation:</strong> Transforming features into a more suitable representation
                    for the model (Normalization, Scaling, Encoding, Mathematical operations). Improves performance,
                    robustness, computational efficiency, and interpretability.</li>
                <li><strong>Feature Extraction:</strong> Creating new features from existing ones to provide more
                    relevant information (Dimensionality Reduction like PCA, t-SNE; Feature Combination; Feature
                    Aggregation). Improves performance, reduces overfitting, improves efficiency and interpretability.
                </li>
                <li><strong>Feature Selection:</strong> Selecting a subset of relevant features from the dataset (Filter
                    Method, Wrapper Method, Embedded Method). Reduces overfitting, improves performance, decreases
                    computational costs, improves interpretability.</li>
                <li><strong>Feature Scaling:</strong> Transforming features so they have a similar scale (Min-Max
                    Scaling, Standard Scaling, Robust Scaling). Improves performance, increases robustness, improves
                    computational efficiency, improves interpretability.</li>
            </ol>
            <h4>Common Feature Engineering Techniques:</h4>
            One-Hot Encoding, Binning, Scaling, Feature Splitting, Text Preprocessing (stop words, stemming,
            lemmatization, vectorization).
            <h4>Feature Engineering Tools:</h4>
            Featuretools, TPOT, DataRobot, Alteryx, H2O.ai.
            <h4>Best Practices in Data Preprocessing and Feature Engineering:</h4>
            Understand Your Data (EDA), Document Every Step, Avoid Data Leakage (from test set to training), Test
            Multiple Techniques, Feature Importance Analysis.
        </section>

        <section id="model-evaluation">
            <h2>Model Evaluation Metrics</h2>
            <p>In a classification task, our main task is to predict the target variable, which is in the form of
                discrete values. To evaluate the performance of such a model, the following are the commonly used
                evaluation metrics:</p>
            <ul>
                <li><strong>Accuracy:</strong> Formula: (Number of correct predictions) / (Total number of input
                    samples). It works great if there are an equal number of samples for each class. However, it can
                    give a false positive sense of high accuracy with imbalanced classes.</li>
                <li><strong>Logarithmic Loss (Log Loss):</strong> Penalizes the false (false positive) classification.
                    It usually works well with multi-class classification. A log loss near 0 indicates high accuracy.
                    <br>Formula: Logarithmic Loss = -<sup>1</sup>⁄<sub>N</sub> Σ<sup>N</sup><sub>i=1</sub>
                    Σ<sup>M</sup><sub>j=1</sub> y<sub>ij</sub> · log(p<sub>ij</sub>)
                    <br>where y<sub>ij</sub> indicates if sample i belongs to class j, and p<sub>ij</sub> is the
                    probability of sample i belonging to class j.
                </li>
                <li><strong>Area Under Curve (AUC) - ROC Curve:</strong> Widely used for binary classification. The AUC
                    of a classifier is defined as its ability to rank a randomly chosen positive example higher than a
                    negative example.
                    <ul>
                        <li>True Positive Rate (TPR) / Sensitivity: TP / (TP + FN)</li>
                        <li>True Negative Rate (TNR) / Specificity: TN / (TN + FP)</li>
                        <li>False Positive Rate (FPR): FP / (FP + TN)</li>
                    </ul>
                    An ROC curve plots TPR vs FPR at different classification thresholds. Greater AUC = better model.
                    <div class="diagram">
                        <h4>Receiver Operating Characteristic (ROC) Curve Example</h4>
                        <div class="diagram-placeholder">Image: ROC Curve plot (TPR vs FPR) with AUC value indicated.
                        </div>
                    </div>
                </li>
                <li><strong>Precision:</strong> Formula: TP / (TP + FP). Measures a model's performance in terms of how
                    many of the positive predictions made by the model are actually correct.</li>
                <li><strong>Recall:</strong> Formula: TP / (TP + FN). The ratio of correctly predicted positive
                    instances to the total actual positive instances. It measures how well the model captures all
                    relevant positive cases.</li>
                <li><strong>F1-Score:</strong> Formula: 2 * (Precision * Recall) / (Precision + Recall). A harmonic mean
                    between recall and precision. It usually tells us how precise (correctly classifies how many
                    instances) and robust (does not miss any significant number of instances) our classifier is.</li>
                <li><strong>Confusion Matrix:</strong> Creates an N x N matrix, where N is the number of classes. For
                    binary classification (N=2), it shows:
                    <ul>
                        <li>True Positives (TP): Predicted Yes, Actual Yes.</li>
                        <li>True Negatives (TN): Predicted No, Actual No.</li>
                        <li>False Positives (FP): Predicted Yes, Actual No (Type I error).</li>
                        <li>False Negatives (FN): Predicted No, Actual Yes (Type II error).</li>
                    </ul>
                    <div class="diagram">
                        <h4>Example Confusion Matrix (n=165)</h4>
                        <div class="diagram-placeholder">Table: Predicted NO/YES vs Actual NO/YES (Actual NO: Pred NO
                            50, Pred YES 10; Actual YES: Pred NO 5, Pred YES 100). Accuracy calculation: (50+100)/165 =
                            0.91.</div>
                    </div>
                </li>
            </ul>
        </section>

        <section id="overfitting-underfitting-bias-variance">
            <h2>ML | Underfitting and Overfitting / Bias-Variance Tradeoff</h2>
            <p>Machine learning models aim to perform well on both training data and new, unseen data and is considered
                "good" if:
            <ol>
                <li>It learns patterns effectively from the training data.</li>
                <li>It generalizes well to new, unseen data.</li>
                <li>It avoids memorizing the training data (overfitting) or failing to capture relevant patterns
                    (underfitting).</li>
            </ol>
            To evaluate how well a model learns and generalizes, we monitor its performance on both the training data
            and a separate validation or test dataset. Two common issues are overfitting and underfitting.</p>
            <h3>1. Overfitting in Machine Learning</h3>
            <p>Overfitting happens when a model learns too much from the training data, including details that don't
                matter (like noise or outliers). For example, fitting a very complicated curve to a set of points might
                go through every point, but it won't represent the actual pattern. As a result, the model works great on
                training data but fails when tested on new data. Overfitting models are like students who memorize
                answers instead of understanding the topic.</p>
            <h4>Reasons for Overfitting:</h4>
            <ol>
                <li>High variance and low bias.</li>
                <li>The model is too complex.</li>
                <li>The size of the training data is too small or not representative.</li>
            </ol>
            <h3>2. Underfitting in Machine Learning</h3>
            <p>Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what's
                going on in the data. For example, drawing a straight line to fit points that actually follow a curve.
                In this case, the model doesn't work well on either the training or testing data. Underfitting models
                are like students who don't study enough. The underfitting model has High bias and low variance.</p>
            <h4>Reasons for Underfitting:</h4>
            <ol>
                <li>The model is too simple.</li>
                <li>The input features used are not adequate representations.</li>
                <li>The size of the training dataset used is not enough.</li>
                <li>Excessive regularization used to prevent overfitting constrains the model too much.</li>
                <li>Features are not scaled.</li>
            </ol>
            <div class="diagram">
                <h4>Visualizing Underfitting, Proper Fitting, and Overfitting</h4>
                <div class="diagram-placeholder">Image: Three graphs showing data points with a line/curve for High Bias
                    (Underfitting), Low Bias/Low Variance (Goodfitting), High Variance (Overfitting). θ<sub>0</sub> +
                    θ<sub>1</sub>x; θ<sub>0</sub> + θ<sub>1</sub>x + θ<sub>2</sub>x<sup>2</sup>; θ<sub>0</sub> +
                    θ<sub>1</sub>x + ... + θ<sub>4</sub>x<sup>4</sup>.</div>
            </div>
            <h3>Balance Between Bias and Variance</h3>
            <p>The relationship between bias and variance is often referred to as the <strong
                    class="highlight">bias-variance tradeoff</strong>, which highlights the need for balance:</p>
            <ul>
                <li>Increasing model complexity reduces bias but increases variance (risk of overfitting).</li>
                <li>Simplifying the model reduces variance but increases bias (risk of underfitting).</li>
            </ul>
            <div class="diagram">
                <h4>Underfitting, Appropriate-fitting, Overfitting (Decision Boundaries)</h4>
                <div class="diagram-placeholder">Image: Three plots showing decision boundaries for underfitting (too
                    simple), appropriate-fitting, and overfitting (too complex, memorizing noise).</div>
            </div>
            <h4>What is Bias?</h4>
            <p>Bias is the difference between the prediction of the values by the ML model and the correct value. High
                bias gives a large error in training as well as testing data (underfitting).</p>
            <div class="diagram">
                <div class="diagram-placeholder">Image: Graph showing data points with a straight line failing to fit a
                    curved trend (High Bias in the Model).</div>
            </div>
            <h4>What is Variance?</h4>
            <p>Variance tells us the spread of our data or model prediction. A model with high variance has a very
                complex fit to the training data and thus is not able to fit accurately on data it hasn't seen before
                (overfitting).</p>
            <div class="diagram">
                <div class="diagram-placeholder">Image: Graph showing data points with a highly complex curve fitting
                    all training points including noise (High Variance in the Model).</div>
            </div>
            <h4>Bias-Variance Tradeoff Graph</h4>
            <p>The best fit will be given by the hypothesis on the tradeoff point. Total Error = Bias<sup>2</sup> +
                Variance + Irreducible Error.</p>
            <div class="diagram">
                <div class="diagram-placeholder">Image: Graph showing error vs. model complexity, with curves for bias
                    squared, variance, training error, generalization/test error, and total error, indicating
                    underfitting and overfitting zones and the optimal complexity.</div>
            </div>
            <h3>How to Address Overfitting and Underfitting?</h3>
            <h4>Techniques to Reduce Underfitting:</h4>
            <ol>
                <li>Increase model complexity.</li>
                <li>Increase the number of features, performing feature engineering.</li>
                <li>Remove noise from the data.</li>
                <li>Increase the number of epochs or increase the duration of training.</li>
            </ol>
            <h4>Techniques to Reduce Overfitting:</h4>
            <ol>
                <li>Improving the quality of training data.</li>
                <li>Increase the training data.</li>
                <li>Reduce model complexity.</li>
                <li>Early stopping during the training phase.</li>
                <li>Ridge Regularization and Lasso Regularization.</li>
                <li>Use dropout for neural networks.</li>
            </ol>
        </section>

        <section id="advantages-disadvantages-ml">
            <h2>Advantages and Disadvantages of Machine Learning</h2>
            <h4>Advantages:</h4>
            <ol>
                <li><strong>Automation of Repetitive Tasks:</strong> Excels at automating time-consuming tasks like data
                    entry or spam filtering.</li>
                <li><strong>Improved Decision-Making:</strong> Analyzes vast data quickly for actionable insights.</li>
                <li><strong>Pattern Recognition:</strong> Identifies trends invisible to humans, useful for
                    recommendations or customer behavior analysis.</li>
                <li><strong>Continuous Improvement:</strong> Models improve over time with more data and experience.
                </li>
                <li><strong>Scalability:</strong> Handles large datasets for complex applications like sensor data
                    interpretation in autonomous vehicles.</li>
                <li><strong>Cost Efficiency:</strong> Automates workflows, reduces errors, minimizing operational costs.
                </li>
                <li><strong>Innovation Enablement:</strong> Drives innovation via technologies like virtual assistants,
                    facial recognition.</li>
            </ol>
            <h4>Disadvantages:</h4>
            <ol>
                <li><strong>Data Dependency:</strong> Performance heavily relies on the quality and quantity of training
                    data; poor data leads to poor outcomes.</li>
                <li><strong>High Computational Costs:</strong> Requires significant investment in infrastructure and
                    skilled professionals.</li>
                <li><strong>Complexity and Interpretability:</strong> Many models (especially deep learning) act as
                    "black boxes," making decisions hard to explain.</li>
                <li><strong>Risk of Bias:</strong> Biases in training data can be perpetuated or amplified by the model.
                </li>
                <li><strong>Job Displacement:</strong> Automation can lead to job losses in roles involving repetitive
                    tasks.</li>
            </ol>
        </section>

        <section id="interactive-quiz-ml">
            <h2>Mini Quiz: ML Fundamentals</h2>
            <div class="quiz-container">
                <div id="quiz">
                    <div class="quiz-question" data-question="1">
                        <p><strong>1. Which type of machine learning uses labeled data for training?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q1" value="a"> Unsupervised Learning</label>
                            <label><input type="radio" name="q1" value="b"> Supervised Learning</label>
                            <label><input type="radio" name="q1" value="c"> Reinforcement Learning</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q1"></p>
                    </div>
                    <div class="quiz-question" data-question="2">
                        <p><strong>2. Predicting house prices based on features like size and location is an example
                                of:</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q2" value="a"> Classification</label>
                            <label><input type="radio" name="q2" value="b"> Regression</label>
                            <label><input type="radio" name="q2" value="c"> Clustering</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q2"></p>
                    </div>
                    <div class="quiz-question" data-question="3">
                        <p><strong>3. When a model performs very well on training data but poorly on new, unseen data,
                                it is likely:</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q3" value="a"> Underfitting</label>
                            <label><input type="radio" name="q3" value="b"> Experiencing high bias</label>
                            <label><input type="radio" name="q3" value="c"> Overfitting</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q3"></p>
                    </div>
                    <button id="submitQuiz">Submit Answers</button>
                    <p id="quizScore" style="margin-top:15px; font-weight:bold;"></p>
                </div>
            </div>
        </section>

        <footer class="new-footer-container">
            <div class="new-footer-buttons-wrapper">
                <a href="topic-5.html" class="new-footer-button">
                    ← Previous Topic: Knowledge Representation & Reasoning
                </a>
                <a href="topic-7.html" class="new-footer-button">
                    Next Topic: Supervised Learning Algorithms →
                </a>
            </div>
            <a href="#" onclick="window.print(); return false;" class="new-footer-print-link">Print this page</a>
            <p style="font-size: 0.9rem; color: var(--text-color); margin-top: 0.5rem;">© <span id="currentYear"></span>
                Machine Learning Fundamentals. All rights reserved.</p>
        </footer>

    </main>

    <script>
        // JS for theme toggle, TOC, quiz, and footer year (Same as previous optimized versions)
        // Dark/Light Mode Toggle
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        const prefersDarkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        function setTheme(theme) { if (theme === 'dark') { body.classList.add('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-sun"></i>'; localStorage.setItem('theme', 'dark'); } else { body.classList.remove('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-moon"></i>'; localStorage.setItem('theme', 'light'); } }
        const localTheme = localStorage.getItem('theme'); if (localTheme) { setTheme(localTheme); } else { setTheme(prefersDarkScheme.matches ? 'dark' : 'light'); }
        themeToggle.addEventListener('click', () => { setTheme(body.classList.contains('dark-mode') ? 'light' : 'dark'); });
        prefersDarkScheme.addEventListener('change', (e) => { if (!localStorage.getItem('theme')) { setTheme(e.matches ? 'dark' : 'light'); } });

        // Table of Contents
        const tocContainer = document.getElementById('toc');
        const mainSections = Array.from(document.querySelectorAll('main section[id]'));
        let tocLinkElements = [];
        function throttle(func, limit) { let inThrottle; return function () { const args = arguments; const context = this; if (!inThrottle) { func.apply(context, args); inThrottle = true; setTimeout(() => inThrottle = false, limit); } } }
        function updateTocAndSectionData() { const navbar = document.querySelector('.navbar'); const navbarHeight = navbar ? navbar.offsetHeight : 0; tocContainer.innerHTML = ''; tocLinkElements = []; mainSections.forEach(section => { const sectionTitleElement = section.querySelector('h2'); if (sectionTitleElement) { const listItem = document.createElement('li'); const link = document.createElement('a'); link.textContent = sectionTitleElement.textContent; link.href = `#${section.id}`; link.dataset.sectionId = section.id; listItem.appendChild(link); tocContainer.appendChild(listItem); tocLinkElements.push(link); } section.dataset.effectiveOffsetTop = section.offsetTop - navbarHeight - 30; }); }
        function highlightActiveTocLink() { const scrollPosition = window.scrollY; let currentlyActiveSectionId = null; for (let i = mainSections.length - 1; i >= 0; i--) { const section = mainSections[i]; if (scrollPosition >= parseInt(section.dataset.effectiveOffsetTop || '0')) { currentlyActiveSectionId = section.id; break; } } tocLinkElements.forEach(link => { if (link.dataset.sectionId === currentlyActiveSectionId) { link.classList.add('active'); } else { link.classList.remove('active'); } }); }
        if (mainSections.length > 0) { updateTocAndSectionData(); highlightActiveTocLink(); window.addEventListener('scroll', throttle(highlightActiveTocLink, 100)); window.addEventListener('resize', throttle(() => { updateTocAndSectionData(); highlightActiveTocLink(); }, 150)); }

        // Mini Quiz
        const submitQuizButtonTopic6 = document.getElementById('submitQuiz');
        const quizQuestionElementsTopic6 = document.querySelectorAll('#interactive-quiz-ml .quiz-question');
        const quizCorrectAnswersTopic6 = {
            q1: { value: 'b', text: 'Supervised Learning' },
            q2: { value: 'b', text: 'Regression' },
            q3: { value: 'c', text: 'Overfitting' }
        };
        if (submitQuizButtonTopic6) {
            submitQuizButtonTopic6.addEventListener('click', () => {
                let score = 0;
                quizQuestionElementsTopic6.forEach(questionElement => {
                    const questionId = 'q' + questionElement.dataset.question;
                    const selectedOption = document.querySelector(`#interactive-quiz-ml input[name="${questionId}"]:checked`);
                    const feedbackElement = document.getElementById(`feedback-${questionId}`);
                    const correctAnswerInfo = quizCorrectAnswersTopic6[questionId];
                    if (correctAnswerInfo && feedbackElement) {
                        if (selectedOption) {
                            if (selectedOption.value === correctAnswerInfo.value) {
                                score++;
                                feedbackElement.textContent = "Correct!";
                                feedbackElement.style.color = "var(--secondary-color)";
                            } else {
                                feedbackElement.textContent = `Incorrect. The correct answer is: ${correctAnswerInfo.text}.`;
                                feedbackElement.style.color = "var(--accent-color)";
                            }
                        } else {
                            feedbackElement.textContent = "Please select an answer.";
                            feedbackElement.style.color = "var(--accent-color)";
                        }
                    }
                });
                const quizScoreEl = document.getElementById('quizScore');
                if (quizScoreEl) {
                    quizScoreEl.textContent = `You scored ${score} out of ${Object.keys(quizCorrectAnswersTopic6).length}.`;
                }
            });
        }

        // Footer current year
        const currentYearSpan = document.getElementById('currentYear');
        if (currentYearSpan) {
            currentYearSpan.textContent = new Date().getFullYear();
        }
    </script>
</body>

</html>