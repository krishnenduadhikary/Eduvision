<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision</title>
    <meta name="description"
        content="Explore Computer Vision: image processing, feature extraction (SIFT, SURF, HOG), image classification, object detection (YOLO, SSD), image segmentation, face recognition, and OCR.">
    <meta name="keywords"
        content="Computer Vision, Image Processing, Feature Extraction, SIFT, SURF, HOG, Image Classification, Object Detection, YOLO, SSD, Image Segmentation, Face Recognition, OCR, AI">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <style>
        /* CSS from topic-1.html - For brevity, imagine all the CSS from the previous example is here */
        /* Key structural CSS will be included, specific element styles might be summarized */
        :root {
            --primary-color-light: #3498db;
            /* Blue */
            --secondary-color-light: #2ecc71;
            /* Green */
            --accent-color-light: #e67e22;
            /* Orange */
            --background-color-light: #f4f7f6;
            --text-color-light: #333;
            --card-bg-light: #ffffff;
            --border-color-light: #e0e0e0;
            --code-bg-light: #eef;

            --primary-color-dark: #5dade2;
            /* Lighter Blue */
            --secondary-color-dark: #58d68d;
            /* Lighter Green */
            --accent-color-dark: #f5b041;
            /* Lighter Orange */
            --background-color-dark: #1e272e;
            --text-color-dark: #f0f0f0;
            --card-bg-dark: #2c3a47;
            --border-color-dark: #444;
            --code-bg-dark: #2a2a40;

            --primary-color: var(--primary-color-light);
            --secondary-color: var(--secondary-color-light);
            --accent-color: var(--accent-color-light);
            --background-color: var(--background-color-light);
            --text-color: var(--text-color-light);
            --card-bg: var(--card-bg-light);
            --border-color: var(--border-color-light);
            --code-bg: var(--code-bg-light);

            --font-sans: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            --font-mono: 'Courier New', Courier, monospace;
            --font-logo: 'Nunito', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-mode {
            --primary-color: var(--primary-color-dark);
            --secondary-color: var(--secondary-color-dark);
            --accent-color: var(--accent-color-dark);
            --background-color: var(--background-color-dark);
            --text-color: var(--text-color-dark);
            --card-bg: var(--card-bg-dark);
            --border-color: var(--border-color-dark);
            --code-bg: var(--code-bg-dark);
        }

        /* Eduvision Logo (Same as topic-1) */
        .eduvishion-logo {
            position: absolute;
            top: 18px;
            left: 18px;
            z-index: 1050;
            text-shadow: 0 2px 12px #6a82fb33;
        }

        .eduvishion-logo .text-2xl {
            font-family: var(--font-logo);
            font-size: 1.7rem;
            font-weight: 900;
            display: flex;
            align-items: center;
            gap: 2px;
            letter-spacing: 0.01em;
        }

        .eduvishion-logo .text-white {
            color: #fff !important;
        }

        .eduvishion-logo .text-yellow-300 {
            color: #fde047 !important;
        }

        .eduvishion-logo .group:hover .text-yellow-300 {
            color: #fef08a !important;
        }

        .eduvishion-logo a {
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 2px;
        }

        .eduvishion-logo svg {
            display: inline-block;
            vertical-align: middle;
            height: 1.5em;
            width: 1.5em;
            margin: 0 2px;
        }

        /* Navbar (Same as topic-1) */
        .navbar {
            background-color: var(--card-bg);
            color: var(--text-color);
            padding: 1rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-left-spacer {
            flex-basis: 50px;
            flex-shrink: 0;
        }

        .navbar-brand {
            font-size: 1.8rem;
            font-weight: bold;
            color: var(--primary-color);
            text-decoration: none;
            text-align: center;
            flex-grow: 1;
        }

        .navbar-brand i {
            margin-right: 0.5rem;
        }

        .theme-toggle {
            cursor: pointer;
            font-size: 1.5rem;
            background: none;
            border: none;
            color: var(--text-color);
            flex-basis: 50px;
            flex-shrink: 0;
            text-align: right;
        }

        /* Sidebar (Same as topic-1) */
        .sidebar {
            position: fixed;
            top: 77px;
            left: 0;
            width: 280px;
            height: calc(100vh - 77px);
            background-color: var(--card-bg);
            padding: 20px;
            overflow-y: auto;
            border-right: 1px solid var(--border-color);
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.05);
        }

        .sidebar h3 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.2rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }

        .sidebar ul {
            list-style: none;
            padding: 0;
        }

        .sidebar ul li a {
            display: block;
            padding: 8px 0;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.2s, padding-left 0.2s, background-color 0.2s;
            border-radius: 4px;
        }

        .sidebar ul li a.sub-item {
            padding-left: 15px;
            font-size: 0.9rem;
        }

        .sidebar ul li a.sub-item:hover {
            padding-left: 25px;
        }

        .sidebar ul li a.sub-item.active {
            padding-left: 25px;
        }

        .sidebar ul li a:hover {
            color: var(--accent-color);
            padding-left: 10px;
        }

        .sidebar ul li a.active {
            color: var(--accent-color);
            padding-left: 10px;
            font-weight: bold;
            background-color: rgba(0, 0, 0, 0.05);
        }

        .dark-mode .sidebar ul li a.active {
            background-color: rgba(255, 255, 255, 0.08);
        }

        /* Main Content Area (Same as topic-1) */
        .main-content {
            margin-left: 300px;
            padding: 2rem 3rem;
        }

        .hero-section {
            text-align: center;
            padding: 4rem 1rem;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .hero-section h1 {
            font-size: 3.5rem;
            margin-bottom: 0.5rem;
        }

        .hero-section p {
            font-size: 1.3rem;
            opacity: 0.9;
        }

        /* Syllabus Bar */
        .syllabus-bar-container {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 0.75rem;
            background-color: var(--card-bg);
            border-radius: 0.5rem;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.07);
            border: 1px solid var(--border-color);
        }

        .syllabus-bar-back-link {
            display: flex;
            align-items: center;
            font-size: 0.875rem;
            color: #2563eb;
            font-weight: 500;
            padding: 0.5rem 0.75rem;
            border-radius: 0.375rem;
            text-decoration: none;
            transition: background-color 0.15s, color 0.15s;
            margin-bottom: 0.5rem;
        }

        .dark-mode .syllabus-bar-back-link {
            color: #5dade2;
        }

        .dark-mode .syllabus-bar-back-link:hover {
            color: #8ecae6;
            background-color: rgba(255, 255, 255, 0.1);
        }

        .syllabus-bar-back-link:hover {
            color: #1d4ed8;
            background-color: #eff6ff;
        }

        .syllabus-bar-back-link svg {
            height: 1.25rem;
            width: 1.25rem;
            margin-right: 0.375rem;
            fill: currentColor;
        }

        .syllabus-bar-topic-badge {
            background-color: #2563eb;
            color: white;
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.375rem 1rem;
            border-radius: 9999px;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }

        @media (min-width: 640px) {
            .syllabus-bar-container {
                flex-direction: row;
            }

            .syllabus-bar-back-link {
                margin-bottom: 0;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.875rem;
            }
        }

        /* General Content Styles (Same as topic-1) */
        section {
            margin-bottom: 3rem;
            padding-top: 70px;
            margin-top: -70px;
        }

        h2 {
            /* Main sections: Clustering, Association, Dimensionality */
            font-size: 2.2rem;
            color: var(--primary-color);
            border-bottom: 3px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            /* Algorithm names: K-Means, Apriori, PCA */
            font-size: 1.8rem;
            /* Slightly larger for main algorithm names */
            color: var(--secondary-color);
            /* Using secondary for algorithm names */
            margin-top: 2.5rem;
            /* More space before a new algorithm */
            margin-bottom: 1.2rem;
            border-bottom: 1px dashed var(--border-color);
            padding-bottom: 0.4rem;
        }

        h4 {
            /* Sub-headings within an algorithm: How it works, Python Implementation */
            font-size: 1.4rem;
            color: var(--accent-color);
            margin-top: 1.8rem;
            margin-bottom: 1rem;
        }

        h5 {
            /* Further sub-headings: Step 1, Pros/Cons */
            font-size: 1.2rem;
            color: var(--primary-color);
            opacity: 0.9;
            margin-top: 1.2rem;
            margin-bottom: 0.7rem;
        }

        .formula {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 0.8rem 1rem;
            border-radius: 4px;
            margin: 1rem auto;
            display: block;
            text-align: center;
            font-size: 1em;
            overflow-x: auto;
        }


        p,
        li {
            font-size: 1.05rem;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        ul,
        ol {
            padding-left: 25px;
            margin-bottom: 1rem;
        }

        ul li,
        ol li {
            margin-bottom: 0.5rem;
        }

        .callout {
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 5px solid var(--accent-color);
            background-color: var(--card-bg);
            border-radius: 0 5px 5px 0;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
        }

        .callout.info {
            border-left-color: var(--primary-color);
        }

        .callout.success {
            border-left-color: var(--secondary-color);
        }

        .callout.warning {
            border-left-color: #f39c12;
        }

        details {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 5px;
            margin-bottom: 1rem;
            padding: 0.5rem 1rem;
        }

        summary {
            font-weight: bold;
            cursor: pointer;
            color: var(--primary-color);
            padding: 0.5rem 0;
        }

        summary::marker {
            color: var(--accent-color);
        }

        pre {
            background-color: var(--code-bg);
            color: var(--text-color);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9em;
            /* Slightly smaller for better fit */
            border: 1px solid var(--border-color);
            margin-top: 0.5rem;
            margin-bottom: 1rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
        }

        code {
            /* Inline code */
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 0.1em 0.3em;
            border-radius: 3px;
            font-size: 0.9em;
        }


        /* Diagram Styles (similar to agent-diagram) */
        .algorithm-diagram,
        .output-plot {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 15px;
            margin: 1.5rem auto;
            max-width: 600px;
            /* Can adjust per diagram */
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.07);
        }

        .algorithm-diagram img,
        .output-plot img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-top: 10px;
            border: 1px solid var(--border-color);
        }

        .algorithm-diagram p,
        .output-plot p {
            font-size: 0.9em;
            margin-top: 0.5em;
            color: var(--text-color);
            opacity: 0.8;
        }

        .diagram-placeholder {
            border: 2px dashed var(--border-color);
            padding: 20px;
            min-height: 100px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--text-color);
            opacity: 0.7;
            font-style: italic;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            font-size: 0.95em;
        }

        th,
        td {
            border: 1px solid var(--border-color);
            padding: 8px 10px;
            text-align: left;
        }

        th {
            background-color: var(--code-bg);
            /* Light background for headers */
            font-weight: bold;
        }

        /* Responsive table */
        .table-container {
            overflow-x: auto;
            margin-bottom: 1.5rem;
        }


        /* New Footer Styles (Same as topic-1, with link updates) */
        .new-footer-container {
            margin-top: 4rem;
            padding-top: 2.5rem;
            border-top: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            padding-bottom: 1rem;
        }

        .new-footer-buttons-wrapper {
            display: flex;
            flex-direction: column;
            width: 100%;
            align-items: center;
        }

        .new-footer-button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 0.75rem 1.5rem;
            background-color: #2563eb;
            color: white !important;
            border-radius: 9999px;
            text-decoration: none;
            transition: background-color 0.15s, box-shadow 0.15s;
            font-weight: 500;
            font-size: 0.875rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            min-width: 280px;
            text-align: center;
            margin-bottom: 0.5rem;
        }

        .new-footer-button:hover {
            background-color: #1d4ed8;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        .new-footer-button:focus {
            outline: 2px solid #3b82f6;
            outline-offset: 2px;
        }

        @media (min-width: 640px) {
            .new-footer-buttons-wrapper {
                flex-direction: row;
                justify-content: space-between;
                gap: 1rem;
            }

            .new-footer-button {
                margin-bottom: 0;
            }
        }

        /* Quiz Styles (Same as topic-1) */
        .quiz-container {
            background-color: var(--card-bg);
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .quiz-question {
            margin-bottom: 15px;
        }

        .quiz-question p strong {
            color: var(--text-color);
        }


        .quiz-options label {
            display: block;
            margin-bottom: 8px;
            cursor: pointer;
        }

        .quiz-options input {
            margin-right: 8px;
        }

        .quiz-feedback {
            margin-top: 10px;
            font-weight: bold;
        }

        /* Responsive Adjustments (Same as topic-1) */
        @media (max-width: 992px) {
            .sidebar {
                width: 100%;
                height: auto;
                position: static;
                border-right: none;
                border-bottom: 1px solid var(--border-color);
                box-shadow: none;
                top: auto;
            }

            .main-content {
                margin-left: 0;
                padding: 1.5rem;
            }

            .navbar {
                padding: 0.8rem 1rem;
            }

            .navbar-brand {
                font-size: 1.5rem;
            }

            .nav-left-spacer,
            .theme-toggle {
                flex-basis: 40px;
            }
        }

        @media (max-width: 600px) {
            .eduvishion-logo {
                top: 12px;
                left: 12px;
            }

            .eduvishion-logo .text-2xl {
                font-size: 1.3rem;
            }

            .eduvishion-logo svg {
                height: 1.2em;
                width: 1.2em;
            }

            .navbar {
                padding: 1rem;
                justify-content: center;
                position: relative;
            }

            .nav-left-spacer {
                display: none;
            }

            .navbar-brand {
                flex-grow: 0;
                margin-right: auto;
                margin-left: 50px;
                /* Adjusted for Eduvision logo space */
                font-size: 1.3rem;
            }

            .theme-toggle {
                position: absolute;
                right: 1rem;
                top: 50%;
                transform: translateY(-50%);
                flex-basis: auto;
                font-size: 1.2rem;
            }

            .main-content {
                padding: 1rem;
            }

            .hero-section h1 {
                font-size: 2.5rem;
            }

            .hero-section p {
                font-size: 1.1rem;
            }

            h2 {
                font-size: 1.8rem;
            }

            h3 {
                font-size: 1.5rem;
            }

            h4 {
                font-size: 1.2rem;
            }

            h5 {
                font-size: 1.1rem;
            }


            .syllabus-bar-container {
                padding: 0.5rem;
            }

            .syllabus-bar-back-link {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.7rem;
                padding: 0.3rem 0.8rem;
            }

            .new-footer-button {
                font-size: 0.8rem;
                padding: 0.6rem 1.2rem;
                min-width: auto;
                width: 90%;
            }

            .new-footer-buttons-wrapper {
                flex-direction: column;
            }

            .new-footer-buttons-wrapper .new-footer-button:first-child {
                margin-bottom: 0.5rem;
            }

            .algorithm-diagram,
            .output-plot {
                max-width: 100%;
                padding: 10px;
            }
        }

        /* Print Styles (Same as topic-1) */
        @media print {
            body {
                font-size: 10pt;
                color: #000 !important;
                background-color: #fff !important;
            }

            .navbar,
            .sidebar,
            .theme-toggle,
            .hero-section .btn,
            .quiz-container,
            .new-footer-container,
            details summary::marker,
            .eduvishion-logo,
            .syllabus-bar-container,
            .new-footer-print-link {
                display: none;
            }

            .main-content {
                margin-left: 0;
                padding: 0;
            }

            section {
                padding-top: 0;
                margin-top: 0;
                margin-bottom: 1.5rem;
                page-break-after: auto;
            }

            h1,
            h2,
            h3,
            h4,
            h5 {
                color: #000 !important;
                border: none !important;
                page-break-after: avoid;
            }

            .callout {
                border-left: 3px solid #ccc !important;
                background-color: #f9f9f9 !important;
            }

            a {
                text-decoration: none;
                color: #000 !important;
            }

            a[href^="http"]:after {
                content: " (" attr(href) ")";
            }

            pre,
            .formula {
                background-color: #f0f0f0 !important;
                border: 1px solid #ccc !important;
                color: #000 !important;
                page-break-inside: avoid;
                white-space: pre-wrap;
                /* Ensure code wraps in print */
                word-wrap: break-word;
            }

            table,
            .algorithm-diagram,
            .output-plot {
                page-break-inside: avoid;
            }

            .algorithm-diagram img,
            .output-plot img {
                max-width: 80% !important;
                /* Control image size in print */
            }
        }
    </style>
</head>

<body>

    <div class="eduvishion-logo">
        <div class="text-2xl font-bold">
            <a href="../../../index.html" class="flex items-center group">
                <span class="text-white">Edu</span>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"
                    fill="none">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
                    <path stroke-linecap="round" stroke-linejoin="round"
                        d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z" />
                </svg>
                <span class="text-white">ision</span>
            </a>
        </div>
    </div>

    <nav class="navbar">
        <div class="nav-left-spacer"></div>
        <a href="#" class="navbar-brand"><i class="fas fa-eye"></i> Computer Vision</a>
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
            <i class="fas fa-moon"></i>
        </button>
    </nav>

    <aside class="sidebar" id="sidebar">
        <h3>Table of Contents</h3>
        <ul id="toc"></ul>
    </aside>

    <main class="main-content">
        <header class="hero-section">
            <h1>Computer Vision</h1>
            <p>Enabling computers to "see" and interpret the visual world around them.</p>
        </header>

        <div class="syllabus-bar-container">
            <a href="../ai.html" class="syllabus-bar-back-link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20">
                    <path fill-rule="evenodd"
                        d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z"
                        clip-rule="evenodd" />
                </svg>
                Back to Syllabus
            </a>
            <div class="syllabus-bar-topic-badge">
                Topic 12
            </div>
        </div>

        <section id="intro-cv">
            <h2>Introduction to Computer Vision</h2>
            <p>Computer Vision is a field of artificial intelligence (AI) that enables computers and systems to derive
                meaningful information from digital images, videos, and other visual inputs — and take actions or make
                recommendations based on that information. If AI enables computers to think, computer vision enables
                them to see, observe, and understand.</p>
            <p>Image processing refers to the techniques used to enhance, transform, and analyze images. This can
                involve a range of operations, from simple tasks like adjusting brightness to complex procedures like
                object detection. The main aim is to prepare images for further analysis or improve their visual
                quality, allowing computers to interpret them more effectively.</p>
        </section>

        <section id="image-processing-fundamentals">
            <h2>Fundamentals of Image Processing</h2>
            <h4>Types of Images</h4>
            <p>It is important to understand the types of images for effective image processing. The main types includes
                –</p>
            <ul>
                <li><strong>Grayscale Images:</strong> Contain shades of gray, ranging from black to white. Each pixel
                    is represented by a single value that represents its intensity. They are simpler to process and
                    often used in applications like edge detection and feature extraction.</li>
                <li><strong>Color Images:</strong> Consist of three channels: red, green, and blue (RGB). Each pixel is
                    represented by three values, one for each channel, which combine to create a wide range of colors.
                    They provide more information but require more complex processing.</li>
                <li><strong>Binary Images:</strong> A binary image has only two pixel values, generally 0 (black) and 1
                    (white). These images are often used in image segmentation and object detection tasks, where it is
                    important to differentiate between the foreground and background.</li>
            </ul>
            <h4>Basic Operations in Image Processing</h4>
            <p>Image processing involves many basic operations that form the basis for more complex tasks. Here are some
                key operations –</p>
            <h5>Image Enhancement</h5>
            <p>Image enhancement techniques focuses on improving the visual quality of an image, making it easier to
                analyze. Some common methods are as follows –</p>
            <ul>
                <li><strong>Brightness Adjustment:</strong> This operation increases or decreases the overall brightness
                    of an image, making details more visible.</li>
                <li><strong>Contrast Enhancement:</strong> Adjusting the contrast makes differences between light and
                    dark areas more pronounced. Higher contrast can reveal more details, making it easier to identify
                    features in the image.</li>
                <li><strong>Histogram Equalization:</strong> This method enhances the contrast of an image by
                    redistributing pixel intensities. By spreading out the most frequent intensity values, histogram
                    equalization helps to improve the visibility of features, particularly in poorly lit images.</li>
            </ul>
            <h5>Image Filtering</h5>
            <p>Filtering techniques are used to remove noise or enhance specific features in an image. Two common types
                of filters are: <strong>Low-Pass Filters</strong> and <strong>High-Pass Filters</strong>.</p>
            <ul>
                <li><strong>Low-Pass Filters:</strong> Reduce high-frequency noise while preserving low-frequency
                    information, effectively smoothing out the image. This operation helps to eliminate graininess that
                    may distract from the important features.</li>
                <li><strong>High-Pass Filters:</strong> Enhance edges and fine details by increasing high-frequency
                    components. They are useful for edge detection and feature extraction, helping to identify the
                    boundaries of objects within an image.</li>
            </ul>
            <h5>Image Transformation</h5>
            <p>Image transformation involves altering the geometry or structure of an image. Few common transformation
                techniques are –</p>
            <ul>
                <li><strong>Scaling:</strong> Changes the size of an image while maintaining its aspect ratio. Resizing
                    images is important for matching the requirements of various applications, such as machine learning
                    models or display screens.</li>
                <li><strong>Rotation:</strong> Rotating an image by a specific angle is necessary for image alignment or
                    to correct orientation. This operation helps standardize images for analysis, especially in datasets
                    where images may not be uniformly oriented.</li>
                <li><strong>Translation:</strong> Shifting an image horizontally or vertically allows for alignment of
                    images in a sequence. This operation is particularly useful when processing a series of images to
                    track movement or changes over time.</li>
            </ul>
        </section>

        <section id="feature-extraction">
            <h2>Feature Extraction Techniques</h2>
            <p>Feature extraction involves identifying and extracting distinctive characteristics or features from an
                image that can be used for tasks like object recognition, image matching, and classification. Some
                prominent techniques include:</p>

            <h3 id="sift">Scale-Invariant Feature Transform (SIFT)</h3>
            <p>The Scale-Invariant Feature Transform (SIFT) is a popular feature extraction algorithm that aims to
                detect and describe robust and distinctive local features in an image. These features, known as SIFT
                keypoints or interest points, are invariant to changes in scale, rotation, and illumination, making them
                suitable for various computer vision tasks such as object recognition, image matching, and 3D
                reconstruction.</p>
            <h4>The SIFT algorithm consists of several key steps:</h4>
            <ol>
                <li><strong>Scale-Space Extrema Detection:</strong> Constructing a scale-space representation by
                    convolving the image with Gaussian filters of different scales. Keypoints are identified as local
                    extrema in the Difference-of-Gaussian (DoG) pyramid.</li>
                <li><strong>Keypoint Localization:</strong> Refining potential keypoints to improve accuracy and
                    eliminate low-contrast or poorly localized points (e.g., on edges).</li>
                <li><strong>Orientation Assignment:</strong> Assigning a dominant orientation to each keypoint based on
                    local image gradients. This ensures rotation invariance.</li>
                <li><strong>Keypoint Descriptor Calculation:</strong> Computing a descriptor for each keypoint based on
                    the local image gradients around it, normalized for illumination changes. SIFT descriptors are
                    typically histograms of gradient orientations.</li>
                <li><strong>Keypoint Matching:</strong> Comparing descriptors between images to find corresponding
                    keypoints, often using nearest neighbor search and a ratio test to ensure uniqueness.</li>
            </ol>

            <h3 id="surf">Speeded Up Robust Features (SURF)</h3>
            <p>The Speeded Up Robust Features (SURF) algorithm is a feature extraction method that shares similarities
                with SIFT but is designed to be computationally efficient. SURF aims to detect and describe local image
                features that are robust to changes in scale, rotation, and affine transformations.</p>
            <h4>The key steps of the SURF algorithm are as follows:</h4>
            <ol>
                <li><strong>Scale-Space Construction:</strong> Builds a scale-space representation using box filters
                    (approximations of Gaussian filters) and integral images for fast convolution.</li>
                <li><strong>Keypoint Detection:</strong> Detects keypoints by identifying local maxima and minima in the
                    Difference of Gaussians (DoG) pyramid (approximated using box filters).</li>
                <li><strong>Orientation Assignment:</strong> Computes Haar wavelet responses in multiple orientations
                    around each keypoint and selects the dominant orientation.</li>
                <li><strong>Keypoint Description:</strong> Based on the distribution of gradient orientations (using
                    Haar wavelets) within a local neighborhood.</li>
                <li><strong>Keypoint Matching:</strong> Uses distance metrics like Euclidean distance or Hamming
                    distance between descriptors. Robust matching techniques like RANSAC can be used to refine matches.
                </li>
            </ol>

            <h3 id="hog">Histogram of Oriented Gradients (HOG)</h3>
            <p>The Histogram of Oriented Gradients (HOG) is a feature extraction technique that focuses on capturing
                local gradient information from an image. It is particularly effective in object detection and
                pedestrian detection tasks.</p>
            <h4>The main steps of the HOG algorithm are as follows:</h4>
            <ol>
                <li><strong>Gradient Computation:</strong> Computing the image’s gradient magnitude and orientation
                    using gradient filters (e.g., Sobel, Scharr).</li>
                <li><strong>Cell Formation:</strong> Dividing the image into small cells, and for each cell, a histogram
                    of gradient orientations is computed.</li>
                <li><strong>Block Normalization:</strong> Grouping neighboring cells into blocks and normalizing their
                    histograms to improve robustness to illumination variations.</li>
                <li><strong>Descriptor Formation:</strong> Concatenating the normalized histograms from all blocks to
                    form the final HOG descriptor.</li>
                <li><strong>Training and Classification:</strong> HOG descriptors can be used as input features for
                    machine learning algorithms like SVMs or Neural Networks.</li>
            </ol>

            <h3 id="gloh">Gradient Location-Orientation Histograms (GLOH)</h3>
            <p>Gradient Location-Orientation Histograms (GLOH) is an extension of the SIFT algorithm that aims to
                improve the robustness and discriminative power of local image descriptors. GLOH captures additional
                information about the spatial distribution of gradients and their orientations.</p>
            <h4>The key steps of the GLOH algorithm are as follows:</h4>
            <ol>
                <li><strong>Interest Point Detection:</strong> Detected using methods similar to SIFT (e.g., DoG).</li>
                <li><strong>Orientation Assignment:</strong> More complex than SIFT, involving computing local
                    orientation histograms at different scales.</li>
                <li><strong>GLOH Descriptor Calculation:</strong> Encodes the spatial distribution of gradients around
                    each keypoint.</li>
                <li><strong>Descriptor Normalization:</strong> Techniques like L2-Norm or contrast normalization are
                    applied.</li>
                <li><strong>Keypoint Matching and Applications:</strong> Similar to SIFT or SURF, for tasks like image
                    alignment, object recognition, and image retrieval.</li>
            </ol>
            <p>In summary, SIFT, SURF, HOG, and GLOH are all feature extraction algorithms that aim to capture and
                describe distinctive local features in images. While SIFT and SURF focus on scale-invariant keypoints,
                HOG and GLOH emphasize gradient information.</p>
        </section>

        <section id="image-classification">
            <h2>Image Classification</h2>
            <p>Image classification is the task of assigning a label or category to an entire image based on its visual
                content. For example, classifying an image as containing a "cat," "dog," or "car." It's a fundamental
                problem in computer vision with numerous applications.</p>
            <h4>Concept of Image Classification</h4>
            <ul>
                <li>Image classification involves assigning pixels in the image to categories or classes of interest.
                    Examples: built-up areas, waterbody, green vegetation, bare soil, rocky areas, cloud, shadow.</li>
                <li>It is a process of mapping numbers (pixel values) to symbols (class labels). Formally, f(x): x → Δ,
                    where x ∈ R<sup>n</sup>, Δ = {c<sub>1</sub>, c<sub>2</sub>, ..., c<sub>L</sub>}.</li>
                <li>To achieve this by computer, the computer must be trained. Training is key to the success of
                    classification.</li>
            </ul>
            <h4>Types of Learning in Image Classification</h4>
            <ul>
                <li><strong>Supervised Learning:</strong> Learning process designed to form a mapping from one set of
                    variables (data) to another set of variables (information classes). A "teacher" (labeled data) is
                    involved.</li>
                <li><strong>Unsupervised Learning:</strong> Learning happens without a teacher. Exploration of the data
                    space to discover underlying patterns or structures.</li>
            </ul>
            <p>Supervised classification generally performs better than unsupervised classification IF good quality
                training data is available. Unsupervised classifiers are often used for preliminary analysis.</p>
            <h4>Role of Image Classifier</h4>
            <p>The image classifier performs the role of a <strong>discriminant</strong> – discriminates one class
                against others. A discriminant function g(c<sub>k</sub>, x) relates a feature vector x and class
                c<sub>k</sub>.
            <ul>
                <li><strong>Multiclass Case:</strong> g<sub>k</sub>(x) > g<sub>l</sub>(x), for all l ≠ k, if x ∈
                    c<sub>k</sub>.</li>
                <li><strong>Two Class Case:</strong> g(x) > 0 if x ∈ c<sub>1</sub>; g(x) < 0 if x ∈ c<sub>2</sub>.</li>
            </ul>
            </p>
            <h4>Supervised Classification Algorithms (Examples)</h4>
            <ul>
                <li>Minimum Distance from Mean (MDM)</li>
                <li>Parallelepiped Classifier</li>
                <li>Maximum Likelihood (ML) Classifier</li>
                <li>Support Vector Machines (SVM)</li>
                <li>Artificial Neural Networks (ANN), especially CNNs</li>
            </ul>
        </section>

        <section id="object-detection">
            <h2>Object Detection</h2>
            <p>Object detection is a computer vision technique for identifying and locating objects within an image or
                video. It involves not only classifying what objects are present but also determining their positions,
                typically by drawing bounding boxes around them. Object detection is a key output of deep learning and
                machine learning algorithms.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Figure 5.4: Object recognition (left image with a palm tree) vs. object
                    detection (right image with a person inside a bounding box).</div>
                <p>Figure 5.4: Object recognition (left) and object detection (right).</p>
            </div>

            <h3 id="yolo">YOLO (You Only Look Once)</h3>
            <p>YOLO (You Only Look Once) is a groundbreaking single-stage object detection algorithm that redefined the
                field by processing images in a single pass, achieving unprecedented speed without sacrificing accuracy
                significantly. Developed by Joseph Redmon and Ali Farhadi, YOLO's "look once" philosophy transformed
                object detection from a multi-step puzzle into a unified, end-to-end process.</p>
            <h4>Core Architecture: How YOLO Achieves Speed and Simplicity</h4>
            <ul>
                <li><strong>Grid Division:</strong> The input image is divided into an S×S grid (e.g., 7×7 in YOLOv1).
                    Each grid cell predicts B bounding boxes and their associated confidence scores.</li>
                <li><strong>Unified Prediction:</strong> YOLO predicts bounding boxes and class probabilities
                    simultaneously in a single forward pass.</li>
                <li><strong>Loss Function:</strong> Combines localization loss (errors in box coordinates), confidence
                    loss (object presence), and classification loss.</li>
                <li><strong>Post-Processing:</strong> Non-Max Suppression (NMS) merges overlapping boxes, retaining only
                    the most confident predictions.</li>
            </ul>
            <h4>Evolution of YOLO: From v1 to YOLOv8 and Beyond</h4>
            <p>Since 2016, YOLO has undergone iterative improvements, balancing speed, accuracy, and versatility. Key
                versions include YOLOv1 to YOLOv8, each introducing innovations like anchor boxes, multi-scale
                prediction, self-training, and anchor-free detection.</p>
            <p><strong>Strengths:</strong> Blazing speed, simplicity, scalability.
                <br><strong>Limitations:</strong> Accuracy trade-offs with crowded/small objects, localization errors in
                early versions.
            </p>

            <h3 id="ssd">SSD (Single Shot MultiBox Detector)</h3>
            <p>A Single Shot Detector (SSD) is an innovative object detection algorithm in computer vision. It stands
                out for its ability to swiftly and accurately detect and locate objects within images or video frames.
                What sets SSD apart is its capacity to accomplish this in a single pass of a deep neural network, making
                it exceptionally efficient and ideal for real-time applications.</p>
            <h4>Key Features of SSD:</h4>
            <ul>
                <li><strong>Single Shot:</strong> Performs object detection in a single pass, directly predicting
                    bounding boxes and class scores.</li>
                <li><strong>MultiBox:</strong> Uses a set of default bounding boxes (anchor boxes) of different scales
                    and aspect ratios at multiple locations in the input image.</li>
                <li><strong>Multi-Scale Detection:</strong> Operates on multiple feature maps with different
                    resolutions, allowing it to detect objects of various sizes.</li>
                <li><strong>Class Scores:</strong> Assigns class scores to each default box, indicating the likelihood
                    of an object belonging to a specific category.</li>
                <li><strong>Hard Negative Mining:</strong> Focuses on challenging negative examples during training to
                    improve accuracy.</li>
            </ul>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">SSD Architecture Diagram: VGG-16 base network, followed by extra
                    feature layers, with predictions made from multiple feature maps.</div>
                <p>SSD Architecture</p>
            </div>
            <p>SSD achieves its efficiency by employing anchor boxes of various aspect ratios at multiple locations in
                feature maps. These anchor boxes enable it to handle objects of different sizes and shapes effectively.
            </p>
            <h4>Applications of SSDs:</h4>
            <ul>
                <li>Autonomous Vehicles</li>
                <li>Surveillance Systems</li>
                <li>Retail Analytics</li>
                <li>Industrial Automation</li>
                <li>Drone Applications</li>
            </ul>
            <h4>Challenges and Limitations of SSDs:</h4>
            <p>SSD's primary limitation is its difficulty in accurately detecting tiny objects, heavily occluded
                objects, or objects with extreme aspect ratios. Other challenges include complex backgrounds and the
                trade-off between speed and accuracy.</p>
        </section>

        <section id="image-segmentation">
            <h2>Image Segmentation</h2>
            <p>Image segmentation is the process of dividing an digital image into multiple segments (sets of pixels,
                also known as image objects). The goal of segmentation is to simplify and/or change the representation
                of an image into something that is more meaningful and easier to analyze. This is essential for
                separating objects within an image.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Image Segmentation Flow: Image -> Region of interest using segmentation
                    -> Processed by ML model -> Output.</div>
            </div>
            <h4>Types of Image Segmentation Approaches</h4>
            <ol>
                <li><strong>Similarity Detection (Region Approach):</strong> Relies on detecting similar pixels in an
                    image – based on a threshold, region growing, region spreading, and region merging.</li>
                <li><strong>Discontinuity Detection (Boundary Approach):</strong> Searches for discontinuity. Image
                    Segmentation algorithms like Edge Detection, Point Detection, Line Detection follows this approach.
                </li>
            </ol>
            <h4>Image Segmentation Techniques</h4>
            <ul>
                <li><strong>Thresholding:</strong> A simple method that converts grayscale images into binary images by
                    setting a specific intensity level as a threshold.
                    <ul>
                        <li><strong>Global Thresholding:</strong> Uses a single threshold value for the entire image.
                        </li>
                        <li><strong>Manual Thresholding:</strong> User chooses the threshold.</li>
                        <li><strong>Adaptive Thresholding:</strong> Image is divided into subregions, and thresholding
                            is applied to each.</li>
                        <li><strong>Optimal Thresholding (e.g., Otsu's Method):</strong> Minimizes misclassification of
                            pixels.</li>
                    </ul>
                </li>
                <li><strong>Edge-Based Segmentation:</strong> Identifying boundaries (edges) within an image helps
                    distinguish objects from the background. Algorithms like the Canny edge detector efficiently locate
                    edges.</li>
                <li><strong>Region-Based Segmentation:</strong> Grouping pixels based on their similarity in color or
                    intensity allows for the identification of distinct regions.
                    <ul>
                        <li><strong>Region Growing:</strong> Starts with seed pixels and iteratively adds adjacent
                            similar pixels.</li>
                        <li><strong>Region Splitting and Merging:</strong> Divides the image into regions and then
                            merges or splits them based on similarity criteria.</li>
                    </ul>
                </li>
                <li><strong>Clustering-Based Segmentation:</strong> Grouping pixels based on their color or intensity
                    values using clustering algorithms (e.g., K-means) allows for the identification of distinct
                    regions.</li>
                <li><strong>Watershed Based Method:</strong> Based on topological interpretation of image boundaries.
                </li>
                <li><strong>Artificial Neural Network Based Segmentation:</strong> Using deep learning algorithms,
                    especially CNNs (e.g., U-Net, Mask R-CNN), for semantic or instance segmentation.</li>
            </ul>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Technique</th>
                            <th>Description</th>
                            <th>Advantages</th>
                            <th>Disadvantages</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Thresholding Method</td>
                            <td>Focuses on finding peak values based on the histogram of the image to find similar
                                pixels</td>
                            <td>Doesn't require complicated pre-processing, simple</td>
                            <td>Many details can get omitted, threshold errors are common</td>
                        </tr>
                        <tr>
                            <td>Edge Based Method</td>
                            <td>based on discontinuity detection unlike similarity detection</td>
                            <td>Good for images having better contrast between objects.</td>
                            <td>Not suitable for noisy images</td>
                        </tr>
                        <tr>
                            <td>Region-Based Method</td>
                            <td>based on partitioning an image into homogeneous regions</td>
                            <td>Works really well for images with a considerate amount of noise, can take user markers
                                for fasted evaluation</td>
                            <td>Time and memory consuming</td>
                        </tr>
                        <tr>
                            <td>Clustering (Traditional Segmentation Algorithms)</td>
                            <td>Divides image into k number of homogenous, mutually exclusive clusters – hence obtaining
                                objects</td>
                            <td>Proven methods, reinforced with fuzzy logic and more useful for real-time application.
                            </td>
                            <td>Determining cost function for minimization can be difficult.</td>
                        </tr>
                        <tr>
                            <td>Watershed Method</td>
                            <td>based on topological interpretation of image boundaries</td>
                            <td>segments obtained are more stable, detected boundaries are distinct</td>
                            <td>Gradient calculation for ridges is complex.</td>
                        </tr>
                        <tr>
                            <td>Neural Networks</td>
                            <td>based on deep learning algorithms – Convolutional Neural Networks</td>
                            <td>easy implementation, no need for following any complicated algorithms, ready-made
                                libraries available in Python, more practical applications</td>
                            <td>Training the model for custom and business images is time consuming and resource costly.
                            </td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="cv-applications">
            <h2>Key Applications of Computer Vision</h2>
            <h3 id="face-recognition">Face Recognition</h3>
            <p>Face recognition is a method of identifying or verifying the identity of an individual using their face.
                Face recognition systems can be used to identify people in photos, video, or in real-time. Law
                enforcement may also use mobile devices to identify people during police stops.</p>
            <div class="algorithm-diagram">
                <div class="diagram-placeholder">Figure 5.2: Face Recognition steps - 1. Image captured, 2. Eye
                    locations determined, 3. Image converted to grayscale/cropped, 4. Image converted to template, 5.
                    Image searched/matched, 6. Duplicate licenses investigated.</div>
                <p>Figure 5.2: Face Recognition</p>
            </div>
            <p>Face recognition systems use computer algorithms to pick out specific, distinctive details about a
                person’s face. These details, such as distance between the eyes or shape of the chin, are then converted
                into a mathematical representation and compared to data on other faces collected in a face recognition
                database.</p>

            <h3 id="ocr">Optical Character Recognition (OCR)</h3>
            <p>Optical Character Recognition (OCR) is a technique to extract text from printed or scanned photos,
                handwritten text images and convert them into a digital format that can be editable and searchable.</p>
            <h4>Applications of OCR:</h4>
            <ul>
                <li>Passport recognition in Airports</li>
                <li>Automation of Data Entry</li>
                <li>Number plates recognition</li>
                <li>Text detection from business cards</li>
                <li>Converting handwritten documents into electronic images</li>
                <li>Creating Searchable PDFs</li>
            </ul>
            <h4>OCR Technology & Innovations:</h4>
            <p>Traditional OCR struggled with poor-quality images and complex fonts. Today, AI-powered OCR systems
                leverage deep learning (CNNs, RNNs, Transformers) to improve recognition in various languages, fonts,
                and even handwriting. OCR combined with Natural Language Processing (NLP) enables automatic
                summarization, classification, and sentiment analysis of extracted text.</p>
            <h5>Build sample OCR Script (using Tesseract and OpenCV):</h5>
            <pre><code>import cv2
import pytesseract

# Set tesseract path (example for Windows, adjust for your system)
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# Reading a sample Image
img = cv2.imread("image.jpg") # Ensure image.jpg is in the same directory or provide full path

# If needed, resize the image
# img = cv2.resize(img, (400, 400))

# Display the image
cv2.imshow("Image", img)

# Converting Image to String
text = pytesseract.image_to_string(img)
print("Extracted Text:")
print(text)

cv2.waitKey(0)
cv2.destroyAllWindows()</code></pre>
            <div class="output-plot">
                <div class="diagram-placeholder">Output: Image window showing a sample image with text, and console
                    output showing the extracted text.</div>
                <p>Sample OCR output.</p>
            </div>
        </section>

        <section id="interactive-quiz-cv">
            <h2>Mini Quiz: Test Your Computer Vision Knowledge!</h2>
            <div class="quiz-container">
                <div id="quiz">
                    <div class="quiz-question" data-question="1">
                        <p><strong>1. Which image processing operation is primarily used to remove high-frequency noise
                                and smooth an image?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q1" value="a"> High-Pass Filtering</label>
                            <label><input type="radio" name="q1" value="b"> Low-Pass Filtering</label>
                            <label><input type="radio" name="q1" value="c"> Histogram Equalization</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q1"></p>
                    </div>
                    <div class="quiz-question" data-question="2">
                        <p><strong>2. SIFT and SURF are techniques primarily used for:</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q2" value="a"> Image Segmentation</label>
                            <label><input type="radio" name="q2" value="b"> Feature Extraction</label>
                            <label><input type="radio" name="q2" value="c"> Image Compression</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q2"></p>
                    </div>
                    <div class="quiz-question" data-question="3">
                        <p><strong>3. What is the main advantage of YOLO and SSD object detection algorithms over
                                two-stage detectors like Faster R-CNN?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q3" value="a"> Higher accuracy on small objects.</label>
                            <label><input type="radio" name="q3" value="b"> Simpler training process.</label>
                            <label><input type="radio" name="q3" value="c"> Faster inference speed, suitable for
                                real-time applications.</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q3"></p>
                    </div>
                    <div class="quiz-question" data-question="4">
                        <p><strong>4. Which image segmentation technique converts an image into binary (black and white)
                                based on pixel intensity?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q4" value="a"> Region Growing</label>
                            <label><input type="radio" name="q4" value="b"> Thresholding</label>
                            <label><input type="radio" name="q4" value="c"> Edge Detection</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q4"></p>
                    </div>
                    <button id="submitQuiz">Submit Answers</button>
                    <p id="quizScore" style="margin-top:15px; font-weight:bold;"></p>
                </div>
            </div>
        </section>


        <footer class="new-footer-container">
            <div class="new-footer-buttons-wrapper">
                <a href="topic-11.html" class="new-footer-button">
                    ← Previous Topic: Natural Language Processing (NLP)
                </a>
                <a href="topic-13.html" class="new-footer-button">
                    Next Topic: Advanced AI Topics & Ethics →
                </a>
            </div>
            <p style="font-size: 0.9rem; color: var(--text-color); margin-top: 0.5rem;">© <span id="currentYear"></span>
                Computer Vision. All rights reserved.</p>
        </footer>

    </main>

    <script>
        // JavaScript from topic-1.html (Theme toggle, Throttled TOC highlighting, Footer year)
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        const prefersDarkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        function setTheme(theme) { if (theme === 'dark') { body.classList.add('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-sun"></i>'; localStorage.setItem('theme', 'dark'); } else { body.classList.remove('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-moon"></i>'; localStorage.setItem('theme', 'light'); } }
        const localTheme = localStorage.getItem('theme'); if (localTheme) { setTheme(localTheme); } else { setTheme(prefersDarkScheme.matches ? 'dark' : 'light'); }
        themeToggle.addEventListener('click', () => { setTheme(body.classList.contains('dark-mode') ? 'light' : 'dark'); });
        prefersDarkScheme.addEventListener('change', (e) => { if (!localStorage.getItem('theme')) { setTheme(e.matches ? 'dark' : 'light'); } });

        // Table of Contents Generation & Active Scrolling
        const tocContainer = document.getElementById('toc');
        const mainContent = document.querySelector('.main-content');
        const mainSections = Array.from(mainContent.querySelectorAll('section[id]'));
        let tocLinkElements = [];

        function throttle(func, limit) { let inThrottle; return function () { const args = arguments; const context = this; if (!inThrottle) { func.apply(context, args); inThrottle = true; setTimeout(() => inThrottle = false, limit); } } }

        function updateTocAndSectionData() {
            const navbar = document.querySelector('.navbar');
            const navbarHeight = navbar ? navbar.offsetHeight : 0;
            tocContainer.innerHTML = '';
            tocLinkElements = [];

            mainSections.forEach(section => {
                const sectionTitleElement = section.querySelector('h2');
                if (sectionTitleElement) {
                    const listItem = document.createElement('li');
                    const link = document.createElement('a');
                    link.textContent = sectionTitleElement.textContent;
                    link.href = `#${section.id}`;
                    link.dataset.sectionId = section.id;
                    listItem.appendChild(link);
                    tocContainer.appendChild(listItem);
                    tocLinkElements.push(link);

                    const subSectionsH3 = Array.from(section.querySelectorAll('h3[id]'));
                    if (subSectionsH3.length > 0) {
                        const subListH3 = document.createElement('ul');
                        subSectionsH3.forEach(subSectionH3 => {
                            const subListItemH3 = document.createElement('li');
                            const subLinkH3 = document.createElement('a');
                            subLinkH3.textContent = subSectionH3.textContent;
                            subLinkH3.href = `#${subSectionH3.id}`;
                            subLinkH3.dataset.sectionId = subSectionH3.id;
                            subLinkH3.classList.add('sub-item');
                            subListItemH3.appendChild(subLinkH3);
                            subListH3.appendChild(subListItemH3);
                            tocLinkElements.push(subLinkH3);
                        });
                        listItem.appendChild(subListH3);
                    }
                }
                const allNavigableElements = Array.from(section.querySelectorAll('h2[id], h3[id]'));
                allNavigableElements.forEach(el => {
                    el.dataset.effectiveOffsetTop = el.getBoundingClientRect().top + window.scrollY - navbarHeight - 30;
                });
            });
        }

        function highlightActiveTocLink() {
            const scrollPosition = window.scrollY;
            let currentlyActiveSectionId = null;
            const allLinkableElements = tocLinkElements.map(link => document.getElementById(link.dataset.sectionId)).filter(el => el);

            for (let i = allLinkableElements.length - 1; i >= 0; i--) {
                const element = allLinkableElements[i];
                if (element && scrollPosition >= parseFloat(element.dataset.effectiveOffsetTop || '0')) {
                    currentlyActiveSectionId = element.id;
                    break;
                }
            }

            tocLinkElements.forEach(link => {
                if (link.dataset.sectionId === currentlyActiveSectionId) {
                    link.classList.add('active');
                    if (link.classList.contains('sub-item')) {
                        const parentLi = link.closest('ul').closest('li');
                        if (parentLi) {
                            const parentLink = parentLi.querySelector('a:not(.sub-item)');
                            if (parentLink) parentLink.classList.add('active');
                        }
                    }
                } else {
                    link.classList.remove('active');
                }
            });
        }

        if (mainSections.length > 0) { updateTocAndSectionData(); highlightActiveTocLink(); window.addEventListener('scroll', throttle(highlightActiveTocLink, 100)); window.addEventListener('resize', throttle(() => { updateTocAndSectionData(); highlightActiveTocLink(); }, 150)); }

        // Mini Quiz Logic
        const submitQuizButton = document.getElementById('submitQuiz');
        const quizQuestionElements = document.querySelectorAll('#interactive-quiz-cv .quiz-question');
        const quizCorrectAnswers = {
            q1: { value: 'b', text: 'Low-Pass Filtering' },
            q2: { value: 'b', text: 'Feature Extraction' },
            q3: { value: 'c', text: 'Faster inference speed, suitable for real-time applications.' },
            q4: { value: 'b', text: 'Thresholding' },
        };

        if (submitQuizButton) {
            submitQuizButton.addEventListener('click', () => {
                let score = 0;
                quizQuestionElements.forEach(questionElement => {
                    const questionId = 'q' + questionElement.dataset.question;
                    const selectedOption = document.querySelector(`#interactive-quiz-cv input[name="${questionId}"]:checked`);
                    const feedbackElement = document.getElementById(`feedback-${questionId}`);
                    const correctAnswerInfo = quizCorrectAnswers[questionId];

                    if (selectedOption) {
                        if (selectedOption.value === correctAnswerInfo.value) {
                            score++;
                            feedbackElement.textContent = "Correct!";
                            feedbackElement.style.color = "var(--secondary-color)";
                        } else {
                            feedbackElement.textContent = `Incorrect. The correct answer is: ${correctAnswerInfo.text}.`;
                            feedbackElement.style.color = "var(--accent-color)";
                        }
                    } else {
                        feedbackElement.textContent = "Please select an answer.";
                        feedbackElement.style.color = "var(--accent-color)";
                    }
                });
                const quizScoreEl = document.getElementById('quizScore');
                if (quizScoreEl) {
                    quizScoreEl.textContent = `You scored ${score} out of ${quizQuestionElements.length}.`;
                }
            });
        }

        // Footer current year
        const currentYearSpan = document.getElementById('currentYear');
        if (currentYearSpan) {
            currentYearSpan.textContent = new Date().getFullYear();
        }
    </script>
</body>

</html>