<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning Algorithms</title>
    <meta name="description"
        content="Detailed exploration of Supervised Learning algorithms: Regression (Linear, Logistic) and Classification (Decision Trees, KNN, SVM, Naive Bayes), Ensemble Methods, Cross-Validation, and Hyperparameter Tuning.">
    <meta name="keywords"
        content="Supervised Learning, Linear Regression, Logistic Regression, Decision Trees, KNN, SVM, Naive Bayes, Random Forest, Gradient Boosting, Cross-Validation, Hyperparameter Tuning">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <style>
        /* === CSS from previous topic pages - Assume it's all here === */
        :root {
            --primary-color-light: #3498db;
            --secondary-color-light: #2ecc71;
            --accent-color-light: #e67e22;
            --background-color-light: #f4f7f6;
            --text-color-light: #333;
            --card-bg-light: #ffffff;
            --border-color-light: #e0e0e0;
            --code-bg-light: #eef;
            --primary-color-dark: #5dade2;
            --secondary-color-dark: #58d68d;
            --accent-color-dark: #f5b041;
            --background-color-dark: #1e272e;
            --text-color-dark: #f0f0f0;
            --card-bg-dark: #2c3a47;
            --border-color-dark: #444;
            --code-bg-dark: #2a2a40;
            --primary-color: var(--primary-color-light);
            --secondary-color: var(--secondary-color-light);
            --accent-color: var(--accent-color-light);
            --background-color: var(--background-color-light);
            --text-color: var(--text-color-light);
            --card-bg: var(--card-bg-light);
            --border-color: var(--border-color-light);
            --code-bg: var(--code-bg-light);
            --font-sans: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            --font-mono: 'Courier New', Courier, monospace;
            --font-logo: 'Nunito', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-mode {
            --primary-color: var(--primary-color-dark);
            --secondary-color: var(--secondary-color-dark);
            --accent-color: var(--accent-color-dark);
            --background-color: var(--background-color-dark);
            --text-color: var(--text-color-dark);
            --card-bg: var(--card-bg-dark);
            --border-color: var(--border-color-dark);
            --code-bg: var(--code-bg-dark);
        }

        .eduvishion-logo {
            position: absolute;
            top: 18px;
            left: 18px;
            z-index: 1050;
            text-shadow: 0 2px 12px #6a82fb33;
        }

        .eduvishion-logo .text-2xl {
            font-family: var(--font-logo);
            font-size: 1.7rem;
            font-weight: 900;
            display: flex;
            align-items: center;
            gap: 2px;
            letter-spacing: 0.01em;
        }

        .eduvishion-logo .text-white {
            color: #fff !important;
        }

        .eduvishion-logo .text-yellow-300 {
            color: #fde047 !important;
        }

        .eduvishion-logo .group:hover .text-yellow-300 {
            color: #fef08a !important;
        }

        .eduvishion-logo a {
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 2px;
        }

        .eduvishion-logo svg {
            display: inline-block;
            vertical-align: middle;
            height: 1.5em;
            width: 1.5em;
            margin: 0 2px;
        }

        .navbar {
            background-color: var(--card-bg);
            color: var(--text-color);
            padding: 1rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-left-spacer {
            flex-basis: 50px;
            flex-shrink: 0;
        }

        .navbar-brand {
            font-size: 1.8rem;
            font-weight: bold;
            color: var(--primary-color);
            text-decoration: none;
            text-align: center;
            flex-grow: 1;
        }

        .navbar-brand i {
            margin-right: 0.5rem;
        }

        .theme-toggle {
            cursor: pointer;
            font-size: 1.5rem;
            background: none;
            border: none;
            color: var(--text-color);
            flex-basis: 50px;
            flex-shrink: 0;
            text-align: right;
        }

        .sidebar {
            position: fixed;
            top: 77px;
            left: 0;
            width: 280px;
            height: calc(100vh - 77px);
            background-color: var(--card-bg);
            padding: 20px;
            overflow-y: auto;
            border-right: 1px solid var(--border-color);
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.05);
        }

        .sidebar h3 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.2rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 0.5rem;
        }

        .sidebar ul {
            list-style: none;
            padding: 0;
        }

        .sidebar ul li a {
            display: block;
            padding: 8px 0;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.2s, padding-left 0.2s, background-color 0.2s;
            border-radius: 4px;
        }

        .sidebar ul li a:hover {
            color: var(--accent-color);
            padding-left: 10px;
        }

        .sidebar ul li a.active {
            color: var(--accent-color);
            padding-left: 10px;
            font-weight: bold;
            background-color: rgba(0, 0, 0, 0.05);
        }

        .dark-mode .sidebar ul li a.active {
            background-color: rgba(255, 255, 255, 0.08);
        }

        .main-content {
            margin-left: 300px;
            padding: 2rem 3rem;
        }

        .hero-section {
            text-align: center;
            padding: 4rem 1rem;
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .hero-section h1 {
            font-size: 3.5rem;
            margin-bottom: 0.5rem;
        }

        .hero-section p {
            font-size: 1.3rem;
            opacity: 0.9;
        }

        .syllabus-bar-container {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding: 0.75rem;
            background-color: var(--card-bg);
            border-radius: 0.5rem;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.07);
            border: 1px solid var(--border-color);
        }

        .syllabus-bar-back-link {
            display: flex;
            align-items: center;
            font-size: 0.875rem;
            color: #2563eb;
            font-weight: 500;
            padding: 0.5rem 0.75rem;
            border-radius: 0.375rem;
            text-decoration: none;
            transition: background-color 0.15s, color 0.15s;
            margin-bottom: 0.5rem;
        }

        .dark-mode .syllabus-bar-back-link {
            color: #5dade2;
        }

        .dark-mode .syllabus-bar-back-link:hover {
            color: #8ecae6;
            background-color: rgba(255, 255, 255, 0.1);
        }

        .syllabus-bar-back-link:hover {
            color: #1d4ed8;
            background-color: #eff6ff;
        }

        .syllabus-bar-back-link svg {
            height: 1.25rem;
            width: 1.25rem;
            margin-right: 0.375rem;
            fill: currentColor;
        }

        .syllabus-bar-topic-badge {
            background-color: #2563eb;
            color: white;
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.375rem 1rem;
            border-radius: 9999px;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }

        @media (min-width: 640px) {
            .syllabus-bar-container {
                flex-direction: row;
            }

            .syllabus-bar-back-link {
                margin-bottom: 0;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.875rem;
            }
        }

        section {
            margin-bottom: 3rem;
            padding-top: 70px;
            margin-top: -70px;
        }

        h2 {
            font-size: 2.2rem;
            color: var(--primary-color);
            border-bottom: 3px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            font-size: 1.7rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        h4 {
            font-size: 1.3rem;
            color: var(--accent-color);
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
        }

        h5 {
            font-size: 1.1rem;
            color: var(--primary-color);
            opacity: 0.9;
            margin-top: 1.2rem;
            margin-bottom: 0.7rem;
        }

        p,
        li {
            font-size: 1.05rem;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        ul,
        ol {
            padding-left: 25px;
        }

        .callout {
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 5px solid var(--accent-color);
            background-color: var(--card-bg);
            border-radius: 0 5px 5px 0;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
        }

        .callout.info {
            border-left-color: var(--primary-color);
        }

        .callout.success {
            border-left-color: var(--secondary-color);
        }

        .callout.warning {
            border-left-color: #f39c12;
        }

        pre {
            background-color: var(--code-bg);
            color: var(--text-color);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            font-family: var(--font-mono);
            font-size: 0.9em;
            border: 1px solid var(--border-color);
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
        }

        .highlight {
            background-color: var(--accent-color);
            color: white;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-weight: bold;
        }

        .diagram {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin: 1.5rem auto;
            max-width: 600px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.07);
        }

        .diagram img,
        .diagram-placeholder {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-top: 10px;
        }

        .diagram-placeholder {
            border: 2px dashed var(--border-color);
            padding: 20px;
            min-height: 150px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--text-color);
            opacity: 0.7;
        }

        .algorithm-summary-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95em;
        }

        .algorithm-summary-table th,
        .algorithm-summary-table td {
            border: 1px solid var(--border-color);
            padding: 10px;
            text-align: left;
        }

        .algorithm-summary-table th {
            background-color: var(--primary-color);
            color: white;
        }

        .algorithm-summary-table tr:nth-child(even) {
            background-color: var(--background-color);
        }

        .formula {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 0.5em;
            border-radius: 4px;
            display: inline-block;
            margin: 0.5em 0;
        }

        .new-footer-container {
            margin-top: 4rem;
            padding-top: 2.5rem;
            border-top: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            gap: 1rem;
            padding-bottom: 1rem;
        }

        .new-footer-buttons-wrapper {
            display: flex;
            flex-direction: column;
            width: 100%;
            align-items: center;
        }

        .new-footer-button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 0.75rem 1.5rem;
            background-color: #2563eb;
            color: white !important;
            border-radius: 9999px;
            text-decoration: none;
            transition: background-color 0.15s, box-shadow 0.15s;
            font-weight: 500;
            font-size: 0.875rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            min-width: 280px;
            text-align: center;
            margin-bottom: 0.5rem;
        }

        .new-footer-button:hover {
            background-color: #1d4ed8;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        .new-footer-button:focus {
            outline: 2px solid #3b82f6;
            outline-offset: 2px;
        }


        @media (min-width: 640px) {
            .new-footer-buttons-wrapper {
                flex-direction: row;
                justify-content: space-between;
                gap: 1rem;
            }

            .new-footer-button {
                margin-bottom: 0;
            }
        }

        .quiz-container {
            background-color: var(--card-bg);
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .quiz-question {
            margin-bottom: 15px;
        }

        .quiz-options label {
            display: block;
            margin-bottom: 8px;
            cursor: pointer;
        }

        .quiz-options input {
            margin-right: 8px;
        }

        .quiz-feedback {
            margin-top: 10px;
            font-weight: bold;
        }

        @media (max-width: 992px) {
            .sidebar {
                width: 100%;
                height: auto;
                position: static;
                border-right: none;
                border-bottom: 1px solid var(--border-color);
                box-shadow: none;
                top: auto;
            }

            .main-content {
                margin-left: 0;
                padding: 1.5rem;
            }

            .navbar {
                padding: 0.8rem 1rem;
            }

            .navbar-brand {
                font-size: 1.5rem;
            }

            .nav-left-spacer,
            .theme-toggle {
                flex-basis: 40px;
            }
        }

        @media (max-width: 600px) {
            .eduvishion-logo {
                top: 12px;
                left: 12px;
            }

            .eduvishion-logo .text-2xl {
                font-size: 1.3rem;
            }

            .eduvishion-logo svg {
                height: 1.2em;
                width: 1.2em;
            }

            .navbar {
                padding: 1rem;
                justify-content: center;
                position: relative;
            }

            .nav-left-spacer {
                display: none;
            }

            .navbar-brand {
                flex-grow: 0;
                margin-right: auto;
                margin-left: 50px;
                font-size: 1.3rem;
            }

            .theme-toggle {
                position: absolute;
                right: 1rem;
                top: 50%;
                transform: translateY(-50%);
                flex-basis: auto;
                font-size: 1.2rem;
            }

            .main-content {
                padding: 1rem;
            }

            h2 {
                font-size: 1.8rem;
            }

            h3 {
                font-size: 1.5rem;
            }

            .syllabus-bar-container {
                padding: 0.5rem;
            }

            .syllabus-bar-back-link {
                font-size: 0.8rem;
                padding: 0.4rem 0.6rem;
            }

            .syllabus-bar-topic-badge {
                font-size: 0.7rem;
                padding: 0.3rem 0.8rem;
            }

            .new-footer-button {
                font-size: 0.8rem;
                padding: 0.6rem 1.2rem;
                min-width: auto;
                width: 90%;
            }

            .new-footer-buttons-wrapper {
                flex-direction: column;
            }

            .new-footer-buttons-wrapper .new-footer-button:first-child {
                margin-bottom: 0.5rem;
            }
        }

        @media print {
            body {
                font-size: 10pt;
                color: #000 !important;
                background-color: #fff !important;
            }

            .navbar,
            .sidebar,
            .theme-toggle,
            .hero-section .btn,
            .quiz-container,
            .new-footer-container,
            .eduvishion-logo,
            .syllabus-bar-container,
            .new-footer-print-link {
                display: none;
            }

            .main-content {
                margin-left: 0;
                padding: 0;
            }

            section {
                padding-top: 0;
                margin-top: 0;
                margin-bottom: 1.5rem;
                page-break-after: auto;
            }

            h1,
            h2,
            h3,
            h4 {
                color: #000 !important;
                border: none !important;
                page-break-after: avoid;
            }

            .callout {
                border-left: 3px solid #ccc !important;
                background-color: #f9f9f9 !important;
            }

            a {
                text-decoration: none;
                color: #000 !important;
            }

            a[href^="http"]:after {
                content: " (" attr(href) ")";
            }

            pre {
                background-color: #f0f0f0 !important;
                border: 1px solid #ccc !important;
                color: #000 !important;
            }

            .diagram,
            .diagram img,
            .algorithm-summary-table {
                page-break-inside: avoid;
            }
        }
    </style>
</head>

<body>

    <div class="eduvishion-logo">
        <div class="text-2xl font-bold">
            <a href="../../index.html" class="flex items-center group">
                <span class="text-white">Edu</span>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"
                    fill="none">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
                    <path stroke-linecap="round" stroke-linejoin="round"
                        d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z" />
                </svg>
                <span class="text-white">ision</span>
            </a>
        </div>
    </div>

    <nav class="navbar">
        <div class="nav-left-spacer"></div>
        <a href="#" class="navbar-brand"><i class="fas fa-lightbulb"></i> Supervised Learning Algorithms</a>
        <!-- Icon suggestion -->
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
            <i class="fas fa-moon"></i>
        </button>
    </nav>

    <aside class="sidebar" id="sidebar">
        <h3>Table of Contents</h3>
        <ul id="toc"></ul>
    </aside>

    <main class="main-content">
        <header class="hero-section">
            <h1>Supervised Learning Algorithms</h1>
            <p>Exploring Regression, Classification, and Ensemble methods in Supervised Machine Learning.</p>
        </header>

        <div class="syllabus-bar-container">
            <a href="../ai.html" class="syllabus-bar-back-link"> <!-- Ensure this path is correct -->
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20">
                    <path fill-rule="evenodd"
                        d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z"
                        clip-rule="evenodd" />
                </svg>
                Back to Syllabus
            </a>
            <div class="syllabus-bar-topic-badge">
                Topic 7
            </div>
        </div>

        <section id="intro-supervised-algos">
            <h2>Introduction to Supervised Learning Algorithms</h2>
            <p>Supervised learning can be further divided into several different types, each with its own unique
                characteristics and applications. These algorithms learn from labeled data, where each data point is
                tagged with a correct output or target variable.</p>
            <p>Here's a summary of some common supervised machine learning algorithms:</p>
            <table class="algorithm-summary-table">
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Type (Regression/Classification)</th>
                        <th>Purpose</th>
                        <th>Method</th>
                        <th>Use Cases</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Linear Regression</td>
                        <td>Regression</td>
                        <td>Predict continuous output values</td>
                        <td>Linear equation minimizing sum of squares of residuals</td>
                        <td>Predicting continuous values (e.g., house prices, stock prices)</td>
                    </tr>
                    <tr>
                        <td>Logistic Regression</td>
                        <td>Classification</td>
                        <td>Predict binary output variable (probability)</td>
                        <td>Logistic function transforming linear relationship</td>
                        <td>Binary classification tasks (e.g., spam detection, medical diagnosis yes/no)</td>
                    </tr>
                    <tr>
                        <td>Decision Trees</td>
                        <td>Both</td>
                        <td>Model decisions and outcomes</td>
                        <td>Tree-like structure with decisions and outcomes</td>
                        <td>Classification and Regression tasks, good for interpretability</td>
                    </tr>
                    <tr>
                        <td>Random Forests</td>
                        <td>Both</td>
                        <td>Improve classification and regression accuracy, reduce overfitting</td>
                        <td>Combining multiple decision trees (ensemble)</td>
                        <td>Complex classification/regression, improving prediction accuracy</td>
                    </tr>
                    <tr>
                        <td>Support Vector Machine (SVM)</td>
                        <td>Both</td>
                        <td>Create hyperplane for classification or predict continuous values</td>
                        <td>Maximizing margin between classes or minimizing error for regression</td>
                        <td>Classification and Regression tasks, effective in high-dimensional spaces</td>
                    </tr>
                    <tr>
                        <td>K-Nearest Neighbors (KNN)</td>
                        <td>Both</td>
                        <td>Predict class or value based on k closest neighbors</td>
                        <td>Finding k closest neighbors and predicting based on majority or average</td>
                        <td>Classification and Regression tasks, sensitive to noisy data and choice of 'k'</td>
                    </tr>
                    <tr>
                        <td>Gradient Boosting</td>
                        <td>Both</td>
                        <td>Combine weak learners to create strong model</td>
                        <td>Iteratively correcting errors with new models</td>
                        <td>Classification and Regression tasks to improve prediction accuracy, handles complex data
                        </td>
                    </tr>
                    <tr>
                        <td>Naive Bayes</td>
                        <td>Classification</td>
                        <td>Predict class based on feature independence assumption</td>
                        <td>Bayes' theorem with feature independence assumption</td>
                        <td>Text classification, spam filtering, sentiment analysis, medical diagnosis</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="regression-models">
            <h2>Regression Models</h2>
            <p>Regression models are used when the target variable (the one you want to predict) is continuous. The goal
                is to find a mathematical function that best maps input features to this continuous output.</p>

            <h3>Linear Regression</h3>
            <p>Linear regression is a type of supervised machine-learning algorithm that learns from the labelled
                datasets and maps the data points with most optimized linear functions which can be used for prediction
                on new datasets. It assumes that there is a linear relationship between the input and output, meaning
                the output changes at a constant rate as the input changes. This relationship is represented by a
                straight line. For example, we want to predict a student's exam score based on how many hours they
                studied.</p>
            <ul>
                <li><strong>Independent variable (input):</strong> Hours studied (factor we control or observe).</li>
                <li><strong>Dependent variable (output):</strong> Exam score (factor we predict).</li>
            </ul>
            <div class="diagram">
                <h4>What is Linear Regression?</h4>
                <div class="diagram-placeholder">Image: Formula ŷ = θ₀ + θ₁x with explanation of terms, and an
                    illustrative graphic (e.g., people with salary increasing with experience).</div>
            </div>
            <h4>How Linear Regression Works (Cost Function):</h4>
            <p>The model learns by finding the line (or hyperplane in multiple dimensions) that best fits the data. This
                is typically done by minimizing a cost function, often the sum of squared errors (SSE) or mean squared
                error (MSE) between the predicted values and actual values.</p>
            <p class="formula">min (1/n) Σ<sup>n</sup><sub>i=1</sub> (Ȳ<sub>i</sub> - Y<sub>i</sub>)<sup>2</sup></p>
            <p>Where: Ȳ<sub>i</sub> is the predicted value, Y<sub>i</sub> is the actual value, and n is the number of
                observations. The model adjusts parameters (θ<sub>0</sub>, θ<sub>1</sub>, ..., θ<sub>n</sub>) to
                minimize this error using techniques like Gradient Descent or Ordinary Least Squares.</p>

            <h4>Types of Linear Regression:</h4>
            <ul>
                <li><strong>Simple Linear Regression:</strong> Predicts the dependent variable using a single
                    independent variable.</li>
                <li><strong>Multiple Linear Regression:</strong> Uses two or more independent variables to predict the
                    dependent variable.</li>
            </ul>
            <div class="diagram">
                <h4>Simple vs. Multiple Linear Regression</h4>
                <div class="diagram-placeholder">Image: Side-by-side plots: 2D scatter plot with a line for Simple LR,
                    and a 3D scatter plot with a plane/surface for Multiple LR.</div>
            </div>

            <h4>Assumptions of Linear Regression:</h4>
            <ol>
                <li><strong>Linearity:</strong> A linear relationship exists between independent variable(s) X and the
                    dependent variable Y.</li>
                <li><strong>Homoscedasticity:</strong> The variance of residual is the same for any value of X (equal
                    variance).</li>
                <li><strong>Multivariate Normality:</strong> For any fixed value of X, Y is normally distributed. Error
                    terms are normally distributed.</li>
                <li><strong>Independence:</strong> Observations are independent of each other (no autocorrelation).</li>
                <li><strong>Lack of Multicollinearity:</strong> Independent variables are not highly correlated with
                    each other (for multiple regression).</li>
                <li><strong>Absence of Endogeneity:</strong> No correlation between predictors and error terms.</li>
            </ol>
            <div class="diagram">
                <h4>Assumptions of Linear Regression - Visualized</h4>
                <div class="diagram-placeholder">Image: Plots showing examples that meet vs. violate each of the 6
                    assumptions.</div>
            </div>

            <h4>Why Linear Regression is Important:</h4>
            <ul>
                <li><strong>Simplicity and Interpretability:</strong> Easy to understand and interpret.</li>
                <li><strong>Predictive Ability:</strong> Helps predict future outcomes.</li>
                <li><strong>Basis for Other Models:</strong> Concepts are foundational for more advanced algorithms.
                </li>
                <li><strong>Efficiency:</strong> Computationally efficient for linear relationships.</li>
                <li><strong>Widely Used:</strong> Common in statistics and ML.</li>
                <li><strong>Analysis:</strong> Provides insights into variable relationships.</li>
            </ul>
            <h4>Real-World Use Cases:</h4>
            Stock Market Prediction, Real Estate Price Prediction, Medical Risk Prediction, Sales Forecasting.
            <div class="diagram">
                <h4>Use Cases of Linear Regression</h4>
                <div class="diagram-placeholder">Image: Infographic with 4 boxes: Stock Market, Real Estate, Medical
                    Risk, Sales Forecasting.</div>
            </div>
            <!-- Further details on learning, variations, data prep, advantages/disadvantages from images -->
            <h5>Python Code Example: Linear Regression</h5>
            <p>This example demonstrates a simple linear regression using Scikit-learn, predicting housing prices.</p>
            <pre><code>
import matplotlib.pyplot as plt
import matplotlib # Import base matplotlib
matplotlib.use('TkAgg') # Set backend before importing pyplot if issues arise, or 'Agg' for non-GUI
import numpy as np
from sklearn import linear_model
import pandas as pd

# Try to load actual data, fall back to dummy data if not found
try:
    # Ensure 'housing.csv' is in the same directory or provide the full path
    df = pd.read_csv('housing.csv') 
except FileNotFoundError:
    print("housing.csv not found, using dummy data for demonstration.")
    # Create more realistic dummy data
    np.random.seed(42) # for reproducibility
    lotsize_base = np.random.uniform(500, 5000, 100)
    price_base = 50000 + 100 * lotsize_base 
    noise = np.random.normal(0, 50000, 100) # Add some noise
    price_final = price_base + noise
    # Ensure prices are positive
    price_final[price_final < 10000] = 10000 
    data = {'lotsize': lotsize_base, 'price': price_final}
    df = pd.DataFrame(data)

X = df[['lotsize']] 
Y = df['price']

# Reshape for scikit-learn (if using a single feature)
X_np = X.values.reshape(-1, 1) 
Y_np = Y.values

# Split data (e.g., 80% train, 20% test)
split_idx = int(len(X_np) * 0.8)
X_train, X_test = X_np[:split_idx], X_np[split_idx:]
Y_train, Y_test = Y_np[:split_idx], Y_np[split_idx:]

# Train Linear Regression model
regr = linear_model.LinearRegression()
regr.fit(X_train, Y_train)

# Plot predictions
plt.figure(figsize=(10, 6))
plt.scatter(X_test, Y_test, color='black', label='Actual Test Data', alpha=0.7)
plt.scatter(X_train, Y_train, color='blue', label='Training Data', alpha=0.3) # Optionally plot training data
plt.plot(X_test, regr.predict(X_test), color='red', linewidth=2, label='Regression Line')
plt.xlabel('Lot Size (sq ft)')
plt.ylabel('Price ($)')
plt.title('Linear Regression: Housing Price vs. Lot Size')
plt.legend()
plt.grid(True)
plt.show()
            </code></pre>
            <div class="diagram">
                <div class="diagram-placeholder">Image: Output scatter plot of Price vs. Lot Size with the regression
                    line.</div>
            </div>
            <p>This linear regression line provides valuable insights into the relationship between the two variables.
                It represents the best-fitting line that captures the overall trend of how a dependent variable (Y)
                changes in response to an independent variable (X).</p>


            <h3>Logistic Regression</h3>
            <p>Logistic regression is a supervised machine learning algorithm used for classification tasks where the
                goal is to predict the probability that an instance belongs to a given class or not. It's primarily used
                for <strong class="highlight">binary classification</strong> (where the output is one of two classes,
                e.g., 0 or 1, Yes or No, True or False) but can be extended to multiclass problems.</p>
            <p>It's referred to as regression because its underlying technique is similar to linear regression, but it
                uses a <strong class="highlight">sigmoid function</strong> (or logistic function) to transform the
                output of a linear equation into a probability value between 0 and 1.</p>
            <h4>Key Points:</h4>
            <ul>
                <li>Predicts the output of a categorical dependent variable.</li>
                <li>Instead of giving an exact value like 0 or 1, it gives probabilistic values between 0 and 1.</li>
                <li>Fits an "S" shaped logistic function.</li>
            </ul>
            <h4>Types of Logistic Regression:</h4>
            <ol>
                <li><strong>Binomial:</strong> Dependent variable has only two possible types (e.g., 0/1, Pass/Fail).
                </li>
                <li><strong>Multinomial:</strong> Dependent variable has 3 or more possible unordered types (e.g., cat,
                    dog, sheep).</li>
                <li><strong>Ordinal:</strong> Dependent variable has 3 or more possible ordered types (e.g., low,
                    medium, high).</li>
            </ol>
            <h4>How Logistic Regression Works:</h4>
            <p>The model transforms the linear regression function's continuous value output into a categorical value
                output using a sigmoid function.
                <br>If input features X, linear combination z = w · X + b.
                <br>The sigmoid function σ(z) = 1 / (1 + e<sup>-z</sup>) maps any real-valued z into a value between 0
                and 1.
            </p>
            <div class="diagram">
                <h4>Sigmoid Function</h4>
                <div class="diagram-placeholder">Image: S-shaped curve of the sigmoid function.</div>
            </div>
            <p>The probability of belonging to a class can be measured as: P(y=1) = σ(z) and P(y=0) = 1 - σ(z).</p>
            <!-- Further details on equations, likelihood, gradient, terminologies from images -->
            <div class="callout info">
                <h4>Linear Regression vs. Logistic Regression</h4>
                <table class="algorithm-summary-table">
                    <thead>
                        <tr>
                            <th>Feature</th>
                            <th>Linear Regression</th>
                            <th>Logistic Regression</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Output</td>
                            <td>Continuous</td>
                            <td>Categorical (Probability for classes)</td>
                        </tr>
                        <tr>
                            <td>Problem Type</td>
                            <td>Regression</td>
                            <td>Classification</td>
                        </tr>
                        <tr>
                            <td>Relationship Line</td>
                            <td>Best fit straight line</td>
                            <td>S-Curve (Sigmoid)</td>
                        </tr>
                        <tr>
                            <td>Estimation Method</td>
                            <td>Least Squares</td>
                            <td>Maximum Likelihood Estimation</td>
                        </tr>
                        <tr>
                            <td>Output Value</td>
                            <td>e.g., price, age</td>
                            <td>e.g., 0/1, Yes/No</td>
                        </tr>
                        <tr>
                            <td>Linear Relationship</td>
                            <td>Required</td>
                            <td>Not necessarily required between X and Y (but between X and log-odds of Y)</td>
                        </tr>
                        <tr>
                            <td>Collinearity</td>
                            <td>Can be an issue</td>
                            <td>Less sensitive but still a consideration</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section id="classification-algorithms">
            <h2>Other Classification Algorithms</h2>
            <!-- Content for Decision Trees, KNN, SVM, Naive Bayes from images, without <details> -->
            <h3>Decision Trees</h3>
            <p>A decision tree is a simple diagram that shows different choices and their possible results helping you
                make decisions easily. It has a hierarchical tree structure with a main question at the top (root node)
                which further branches out into different possible outcomes.</p>
            <ul>
                <li><strong>Root Node:</strong> Starting point representing the entire dataset.</li>
                <li><strong>Branches:</strong> Lines connecting nodes, showing flow from one decision to another.</li>
                <li><strong>Internal Nodes:</strong> Points where decisions are made based on input features.</li>
                <li><strong>Leaf Nodes:</strong> Terminal nodes representing final outcomes or predictions.</li>
            </ul>
            <div class="diagram">
                <h4>Decision Tree Structure</h4>
                <div class="diagram-placeholder">Image: Diagram showing Root Node branching to Internal Nodes, then to
                    Leaf Nodes.</div>
            </div>
            <div class="diagram">
                <h4>Example Decision Tree (Time of Day)</h4>
                <div class="diagram-placeholder">Image: Decision tree for "Drink Coffee?" based on Time of Day, Tired?,
                    etc.</div>
            </div>
            <h4>Types of Decision Trees:</h4>
            Classification trees (predict categorical outcomes) and regression trees (predict continuous values).
            <h4>How Decision Trees Work:</h4>
            Starts with a root node question. Asks a series of yes/no questions to split data into subsets based on
            attributes until a final outcome (leaf node) is reached.
            <h4>Advantages:</h4> Simplicity, interpretability, no need for feature scaling, handles non-linear
            relationships.
            <h4>Disadvantages:</h4> Overfitting, instability (slight input variations can change tree), bias towards
            features with more levels.
            <h4>Applications:</h4> Loan approval, medical diagnosis, predicting exam results.
            <div class="callout info">
                <h5>Python Code Example: Decision Tree Classifier</h5>
                <pre><code>
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt

# Example: Using a public dataset (Balance Scale)
# Data Import and Exploration
def importdata():
    balance_data = pd.read_csv(
'https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data',
    sep=',', header=None)
    # Adding headers for clarity
    balance_data.columns = ['Class Name', 'Left-Weight', 'Left-Distance', 'Right-Weight', 'Right-Distance']
    print("Dataset Length: ", len(balance_data))
    print("Dataset Shape: ", balance_data.shape)
    print("Dataset: ", balance_data.head())
    return balance_data

# Separating the target variable
def splitdataset(balance_data):
    X = balance_data.values[:, 1:5] # Features
    Y = balance_data.values[:, 0]   # Target
    X_train, X_test, y_train, y_test = train_test_split(
        X, Y, test_size=0.3, random_state=100)
    return X, Y, X_train, X_test, y_train, y_test

# Training with Gini Index
def train_using_gini(X_train, y_train):
    clf_gini = DecisionTreeClassifier(criterion="gini",
                                      random_state=100, max_depth=3, min_samples_leaf=5)
    clf_gini.fit(X_train, y_train)
    return clf_gini

# Prediction and Evaluation
def prediction(X_test, clf_object):
    y_pred = clf_object.predict(X_test)
    print("Predicted values:", y_pred)
    return y_pred

def cal_accuracy(y_test, y_pred):
    print("Confusion Matrix: ", confusion_matrix(y_test, y_pred))
    print("Accuracy : ", accuracy_score(y_test, y_pred)*100)
    print("Report : ", classification_report(y_test, y_pred))

# Main execution
if __name__ == '__main__':
    data = importdata()
    X, Y, X_train, X_test, y_train, y_test = splitdataset(data)
    clf_gini = train_using_gini(X_train, y_train)
    
    print("\\nResults Using Gini Index:")
    y_pred_gini = prediction(X_test, clf_gini)
    cal_accuracy(y_test, y_pred_gini)

    # Plotting the tree
    plt.figure(figsize=(15,10))
    plot_tree(clf_gini, filled=True, feature_names=['LW', 'LD', 'RW', 'RD'], 
              class_names=np.unique(Y).astype(str), rounded=True)
    plt.title("Decision Tree - Gini Index")
    plt.show()
                </code></pre>
                <div class="diagram">
                    <div class="diagram-placeholder">Output: Data info and plotted Decision Tree using Gini Index.</div>
                </div>
            </div>


            <h3>K-Nearest Neighbors (KNN)</h3>
            <p>KNN is a supervised ML algorithm used for classification and regression. It works by finding the 'k'
                closest data points (neighbors) to a given input and makes predictions based on the majority class
                (classification) or average value (regression) of these neighbors. It's non-parametric and
                instance-based (a "lazy learner" as it stores the dataset and performs action at classification time).
            </p>
            <div class="diagram">
                <h4>KNN: Finding Neighbors & Voting</h4>
                <div class="diagram-placeholder">Image: Steps: Initial Data → Calculate Data (show new point) → Finding
                    Neighbors & Voting for Labels (K=3 circle around new point).</div>
            </div>
            <h4>Distance Metrics Used in KNN:</h4>
            <ol>
                <li><strong>Euclidean Distance:</strong> Straight-line distance. Formula: √Σ(x<sub>i</sub> -
                    y<sub>i</sub>)<sup>2</sup></li>
                <li><strong>Manhattan Distance:</strong> Sum of absolute differences. Formula: Σ|x<sub>i</sub> -
                    y<sub>i</sub>|</li>
                <li><strong>Minkowski Distance:</strong> Generalization. Formula: ( Σ|x<sub>i</sub> -
                    y<sub>i</sub>|<sup>p</sup> )<sup>1/p</sup> (Euclidean if p=2, Manhattan if p=1)</li>
            </ol>
            <h4>Working of KNN:</h4>
            Selecting optimal K, calculating distance, finding K nearest neighbors, voting/averaging.
            <div class="diagram">
                <h4>Working of KNN Algorithm Steps</h4>
                <div class="diagram-placeholder">Image: Three-panel diagram showing target point, distance calculation
                    to neighbors, and classification based on K neighbors.</div>
            </div>

            <h3>Support Vector Machines (SVM)</h3>
            <p>The SVM algorithm creates a hyperplane or set of hyperplanes in an n-dimensional space to separate data
                points into classes or predict continuous values. The goal is to find the hyperplane that maximizes the
                margin (distance) between the classes. The data points closest to the hyperplane that influence its
                position are called support vectors.</p>
            <p>For non-linear data, SVMs use kernel functions (e.g., linear, polynomial, Radial Basis Function (RBF),
                sigmoid) to map data into higher-dimensional spaces where a linear separator might be found.</p>
            <!-- Add placeholder for SVM diagram if available -->
            <div class="diagram">
                <div class="diagram-placeholder">Image: Diagram illustrating SVM hyperplane and margin with support
                    vectors.</div>
            </div>

            <h3>Naive Bayes</h3>
            <p>The Naive Bayes algorithm is a supervised machine learning algorithm based on applying Bayes' Theorem
                with the "naive" assumption that features are independent of each other given the class label. Despite
                this often unrealistic assumption, Naive Bayes classifiers can perform well, especially in text
                classification and spam filtering.</p>
            <p>It calculates the probability of each class given the input features and selects the class with the
                highest probability.
                <br>Formula: P(A|B) = (P(B|A) * P(A)) / P(B)
            </p>
            <!-- Add placeholder for Naive Bayes diagram if available -->
            <div class="diagram">
                <div class="diagram-placeholder">Image: Conceptual diagram of Naive Bayes applying Bayes' Theorem.</div>
            </div>
        </section>

        <section id="ensemble-methods">
            <h2>Ensemble Methods</h2>
            <p>Ensemble methods combine multiple learning algorithms to obtain better predictive performance than could
                be obtained from any of the constituent learning algorithms alone.</p>
            <h3>Random Forests</h3>
            <p>Random Forests are an ensemble of Decision Trees. Each tree in the forest is trained on a random subset
                of the training data (bagging) and considers only a random subset of features for splitting at each
                node. The final prediction is made by aggregating the predictions of all trees (majority vote for
                classification, average for regression). This helps reduce overfitting and improve generalization.</p>

            <h3>Bagging (Bootstrap Aggregating)</h3>
            <p>A general technique where multiple models are trained on different bootstrap samples (random samples with
                replacement) of the training data. Predictions are then combined.</p>

            <h3>Boosting</h3>
            <p>Boosting algorithms build models sequentially, where each new model attempts to correct the errors made
                by previous models.</p>
            <ul>
                <li><strong>AdaBoost (Adaptive Boosting):</strong> Iteratively trains weak learners (e.g., shallow
                    decision trees). In each iteration, it increases the weights of misclassified instances so that
                    subsequent learners focus more on these hard-to-classify examples.</li>
                <li><strong>Gradient Boosting:</strong> Builds models sequentially where each new model fits the
                    negative gradient (residuals) of the loss function with respect to the predictions of the previous
                    ensemble. Popular implementations include XGBoost, LightGBM, and CatBoost.</li>
            </ul>
        </section>

        <section id="cross-validation-hyperparameter-tuning">
            <h2>Cross-Validation and Hyperparameter Tuning</h2>
            <h3>Cross-Validation</h3>
            <p>Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data
                sample. The goal is to estimate how the model is expected to perform on unseen data. A common method is
                <strong class="highlight">k-fold cross-validation</strong>, where the original sample is randomly
                partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the
                validation data for testing the model, and the remaining k-1 subsamples are used as training data. The
                cross-validation process is then repeated k times, with each of the k subsamples used exactly once as
                the validation data. The k results can then be averaged to produce a single estimation.
            </p>
            <h3>Hyperparameter Tuning</h3>
            <p>Hyperparameters are parameters whose values are set before the learning process begins (e.g., the 'k' in
                KNN, the depth of a decision tree, learning rate in gradient descent). Hyperparameter tuning is the
                process of finding the optimal combination of hyperparameters that maximizes the model's performance on
                an independent dataset. Common techniques include:</p>
            <ul>
                <li><strong>Grid Search:</strong> Exhaustively searches through a manually specified subset of the
                    hyperparameter space.</li>
                <li><strong>Random Search:</strong> Samples hyperparameter combinations randomly from a given
                    distribution. Often more efficient than grid search.</li>
                <li><strong>Bayesian Optimization:</strong> Uses a probabilistic model to select the most promising
                    hyperparameters to evaluate next.</li>
            </ul>
        </section>

        <section id="interactive-quiz-supervised">
            <h2>Mini Quiz: Supervised Learning Algorithms</h2>
            <div class="quiz-container">
                <div id="quiz">
                    <div class="quiz-question" data-question="1">
                        <p><strong>1. Which algorithm is primarily used for binary classification by transforming a
                                linear output using a sigmoid function?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q1" value="a"> Linear Regression</label>
                            <label><input type="radio" name="q1" value="b"> Logistic Regression</label>
                            <label><input type="radio" name="q1" value="c"> K-Nearest Neighbors</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q1"></p>
                    </div>
                    <div class="quiz-question" data-question="2">
                        <p><strong>2. An ensemble method that builds multiple decision trees on random subsets of data
                                and features is called:</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q2" value="a"> Support Vector Machine</label>
                            <label><input type="radio" name="q2" value="b"> Naive Bayes</label>
                            <label><input type="radio" name="q2" value="c"> Random Forest</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q2"></p>
                    </div>
                    <div class="quiz-question" data-question="3">
                        <p><strong>3. Which of these is NOT an assumption of classical Linear Regression?</strong></p>
                        <div class="quiz-options">
                            <label><input type="radio" name="q3" value="a"> Linearity</label>
                            <label><input type="radio" name="q3" value="b"> Features must be categorical</label>
                            <label><input type="radio" name="q3" value="c"> Homoscedasticity</label>
                        </div>
                        <p class="quiz-feedback" id="feedback-q3"></p>
                    </div>
                    <button id="submitQuiz">Submit Answers</button>
                    <p id="quizScore" style="margin-top:15px; font-weight:bold;"></p>
                </div>
            </div>
        </section>

        <footer class="new-footer-container">
            <div class="new-footer-buttons-wrapper">
                <a href="topic-6.html" class="new-footer-button">
                    ← Previous Topic: Machine Learning Fundamentals
                </a>
                <a href="topic-8.html" <!-- Update this link when topic-8 is ready -->
                    class="new-footer-button">
                    Next Topic: Unsupervised Learning Algorithms →
                </a>
            </div>

            <p style="font-size: 0.9rem; color: var(--text-color); margin-top: 0.5rem;">© <span id="currentYear"></span>
                Supervised Learning Algorithms. All rights reserved.</p>
        </footer>

    </main>

    <script>
        // JS for theme toggle, TOC, quiz, and footer year (Same as previous optimized versions)
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        const prefersDarkScheme = window.matchMedia("(prefers-color-scheme: dark)");
        function setTheme(theme) { if (theme === 'dark') { body.classList.add('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-sun"></i>'; localStorage.setItem('theme', 'dark'); } else { body.classList.remove('dark-mode'); themeToggle.innerHTML = '<i class="fas fa-moon"></i>'; localStorage.setItem('theme', 'light'); } }
        const localTheme = localStorage.getItem('theme'); if (localTheme) { setTheme(localTheme); } else { setTheme(prefersDarkScheme.matches ? 'dark' : 'light'); }
        themeToggle.addEventListener('click', () => { setTheme(body.classList.contains('dark-mode') ? 'light' : 'dark'); });
        prefersDarkScheme.addEventListener('change', (e) => { if (!localStorage.getItem('theme')) { setTheme(e.matches ? 'dark' : 'light'); } });

        const tocContainer = document.getElementById('toc');
        const mainSections = Array.from(document.querySelectorAll('main section[id]'));
        let tocLinkElements = [];
        function throttle(func, limit) { let inThrottle; return function () { const args = arguments; const context = this; if (!inThrottle) { func.apply(context, args); inThrottle = true; setTimeout(() => inThrottle = false, limit); } } }
        function updateTocAndSectionData() { const navbar = document.querySelector('.navbar'); const navbarHeight = navbar ? navbar.offsetHeight : 0; tocContainer.innerHTML = ''; tocLinkElements = []; mainSections.forEach(section => { const sectionTitleElement = section.querySelector('h2'); if (sectionTitleElement) { const listItem = document.createElement('li'); const link = document.createElement('a'); link.textContent = sectionTitleElement.textContent; link.href = `#${section.id}`; link.dataset.sectionId = section.id; listItem.appendChild(link); tocContainer.appendChild(listItem); tocLinkElements.push(link); } section.dataset.effectiveOffsetTop = section.offsetTop - navbarHeight - 30; }); }
        function highlightActiveTocLink() { const scrollPosition = window.scrollY; let currentlyActiveSectionId = null; for (let i = mainSections.length - 1; i >= 0; i--) { const section = mainSections[i]; if (scrollPosition >= parseInt(section.dataset.effectiveOffsetTop || '0')) { currentlyActiveSectionId = section.id; break; } } tocLinkElements.forEach(link => { if (link.dataset.sectionId === currentlyActiveSectionId) { link.classList.add('active'); } else { link.classList.remove('active'); } }); }
        if (mainSections.length > 0) { updateTocAndSectionData(); highlightActiveTocLink(); window.addEventListener('scroll', throttle(highlightActiveTocLink, 100)); window.addEventListener('resize', throttle(() => { updateTocAndSectionData(); highlightActiveTocLink(); }, 150)); }

        const submitQuizButtonTopic7 = document.getElementById('submitQuiz');
        const quizQuestionElementsTopic7 = document.querySelectorAll('#interactive-quiz-supervised .quiz-question');
        const quizCorrectAnswersTopic7 = {
            q1: { value: 'b', text: 'Logistic Regression' },
            q2: { value: 'c', text: 'Random Forest' },
            q3: { value: 'b', text: 'Features must be categorical' }
        };
        if (submitQuizButtonTopic7) {
            submitQuizButtonTopic7.addEventListener('click', () => {
                let score = 0;
                quizQuestionElementsTopic7.forEach(questionElement => {
                    const questionId = 'q' + questionElement.dataset.question;
                    const selectedOption = document.querySelector(`#interactive-quiz-supervised input[name="${questionId}"]:checked`);
                    const feedbackElement = document.getElementById(`feedback-${questionId}`);
                    const correctAnswerInfo = quizCorrectAnswersTopic7[questionId];
                    if (correctAnswerInfo && feedbackElement) {
                        if (selectedOption) {
                            if (selectedOption.value === correctAnswerInfo.value) {
                                score++;
                                feedbackElement.textContent = "Correct!";
                                feedbackElement.style.color = "var(--secondary-color)";
                            } else {
                                feedbackElement.textContent = `Incorrect. The correct answer is: ${correctAnswerInfo.text}.`;
                                feedbackElement.style.color = "var(--accent-color)";
                            }
                        } else {
                            feedbackElement.textContent = "Please select an answer.";
                            feedbackElement.style.color = "var(--accent-color)";
                        }
                    }
                });
                const quizScoreEl = document.getElementById('quizScore');
                if (quizScoreEl) {
                    quizScoreEl.textContent = `You scored ${score} out of ${Object.keys(quizCorrectAnswersTopic7).length}.`;
                }
            });
        }

        const currentYearSpan = document.getElementById('currentYear');
        if (currentYearSpan) {
            currentYearSpan.textContent = new Date().getFullYear();
        }
    </script>
</body>

</html>