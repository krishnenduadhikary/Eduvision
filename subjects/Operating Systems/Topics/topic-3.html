<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Threads & Concurrency - EduVision OS Guide</title>
    <link
        href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&family=Nunito:wght@700&display=swap"
        rel="stylesheet">
    <style>
        :root {
            --primary-purple: #7A5CFA;
            --primary-purple-dark: #5D3FD3;
            --primary-purple-light: #A48DFB;
            --light-lavender-bg: #F8F7FF;
            --card-bg-color: #FFFFFF;
            --text-color: #23272f;
            --subtle-text-color: #6b7280;
            --border-color: #E2E8F0;
            --code-bg-color: #181c24;
            --code-text-color: #e0e6f1;
            --tooltip-bg: #23272f;
            --tooltip-text: #F7FAFC;
            --success-color: #48BB78;
            --danger-color: #F56565;
            --info-blue: #3B82F6;
            --info-blue-dark: #2563EB;
            --info-blue-light: #EFF6FF;
            --font-sans: 'Poppins', 'Segoe UI', Arial, sans-serif;
            --font-mono: 'Fira Mono', 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.06);
            --shadow-md: 0 4px 16px 0 rgba(80, 70, 140, 0.10);
            --shadow-lg: 0 12px 32px 0 rgba(80, 70, 140, 0.13);
            --shadow-subtle: 0 2px 4px 0 rgba(107, 114, 128, 0.08);
            --border-radius-sm: 0.25rem;
            --border-radius-md: 0.5rem;
            --border-radius-lg: 1rem;
        }

        html,
        body {
            height: 100%;
        }

        body {
            font-family: var(--font-sans);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            min-height: 100vh;
            color: var(--text-color);
            background: linear-gradient(120deg, #f8f7ff 0%, #e9e6f7 100%);
        }

        .main-header {
            background: linear-gradient(90deg, var(--primary-purple-dark) 0%, var(--primary-purple) 100%);
            color: white;
            padding: 28px 40px 22px 40px;
            text-align: center;
            box-shadow: var(--shadow-md);
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        .main-header h1 {
            margin: 0;
            font-size: 2.5em;
            font-weight: 800;
            letter-spacing: -1px;
        }

        .main-header .subtitle {
            font-size: 1.1em;
            font-weight: 400;
            opacity: 0.92;
            margin-top: 0.25rem;
            letter-spacing: 0.01em;
        }

        .eduvishion-logo {
            position: absolute;
            top: 18px;
            left: 18px;
            z-index: 1001;
        }

        .eduvishion-logo .text-2xl {
            font-size: 1.6rem;
            font-weight: 700;
            display: flex;
            align-items: center;
            gap: 2px;
            font-family: 'Nunito', sans-serif;
        }

        .eduvishion-logo .text-white {
            color: #fff !important;
        }

        .eduvishion-logo .text-yellow-300 {
            color: #fde047 !important;
        }

        .eduvishion-logo .group:hover .text-yellow-300 {
            color: #fef08a !important;
        }

        .eduvishion-logo a {
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 2px;
        }

        .eduvishion-logo svg {
            display: inline-block;
            vertical-align: middle;
            height: 1.5em;
            width: 1.5em;
            margin: 0 2px;
        }

        @media (max-width: 768px) {
            .main-header {
                position: static;
            }

            .eduvishion-logo {
                top: 15px !important;
                left: 15px !important;
            }

            .eduvishion-logo .text-2xl {
                font-size: 1.2rem !important;
            }
        }

        @media (max-width: 400px) {
            .eduvishion-logo {
                display: none;
            }
        }

        .page-container {
            display: flex;
            width: 100%;
            margin-top: 0;
        }

        #sidebar {
            width: 270px;
            min-width: 220px;
            background: linear-gradient(135deg, #f5f3fe 80%, #e9e6f7 100%);
            padding: 32px 18px 32px 18px;
            height: calc(100vh - 85px);
            position: sticky;
            top: 85px;
            overflow-y: auto;
            border-right: 1.5px solid var(--border-color);
            box-shadow: 2px 0 12px 0 rgba(80, 70, 140, 0.04);
            transition: width 0.3s;
        }

        #sidebar h2 {
            color: var(--primary-purple-dark);
            text-align: left;
            margin-top: 0;
            margin-bottom: 22px;
            font-size: 1.3em;
            font-weight: 700;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-purple);
            letter-spacing: 0.01em;
        }

        #sidebar ul {
            list-style-type: none;
            padding: 0;
        }

        #sidebar ul li a {
            color: var(--subtle-text-color);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 11px 18px;
            margin-bottom: 7px;
            border-radius: var(--border-radius-md);
            font-weight: 500;
            font-size: 1.04em;
            transition: background 0.18s, color 0.18s, transform 0.13s;
            border-left: 4px solid transparent;
            position: relative;
        }

        #sidebar ul li a:hover,
        #sidebar ul li a.active {
            background: linear-gradient(90deg, #ece9fa 60%, #e0d7f3 100%);
            color: var(--primary-purple-dark);
            border-left: 4px solid var(--primary-purple);
            font-weight: 700;
            transform: translateX(4px) scale(1.03);
        }

        #main-content {
            flex-grow: 1;
            padding: 38px 48px 48px 48px;
            max-width: 100vw;
            min-width: 0;
            background: transparent;
            display: flex;
            flex-direction: column;
            align-items: stretch;
        }

        .main-content-float {
            background: var(--card-bg-color);
            border-radius: var(--border-radius-lg);
            box-shadow: var(--shadow-lg);
            padding: 36px 36px 24px 36px;
            margin-bottom: 40px;
            margin-top: 0;
            min-width: 0;
        }

        .syllabus-nav-bar {
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1.5rem;
            padding: 0.75rem;
            background: var(--card-bg-color);
            border-radius: var(--border-radius-lg);
            box-shadow: var(--shadow-subtle);
        }

        .syllabus-nav-bar a.back-to-syllabus {
            display: inline-flex;
            align-items: center;
            font-size: 0.98rem;
            color: var(--info-blue);
            font-weight: 600;
            padding: 0.5rem 0.95rem;
            border-radius: var(--border-radius-md);
            text-decoration: none;
            transition: background 0.2s, color 0.2s;
            margin-bottom: 0.5rem;
        }

        .syllabus-nav-bar a.back-to-syllabus:hover {
            background: var(--info-blue-light);
            color: var(--info-blue-dark);
        }

        .syllabus-nav-bar a.back-to-syllabus svg {
            height: 1.25rem;
            width: 1.25rem;
            margin-right: 0.375rem;
        }

        .syllabus-nav-bar .topic-indicator {
            background: var(--info-blue);
            color: white;
            font-size: 0.85rem;
            font-weight: 700;
            padding: 0.375rem 1.1rem;
            border-radius: 9999px;
            box-shadow: var(--shadow-sm);
        }

        @media (min-width: 640px) {
            .syllabus-nav-bar {
                flex-direction: row;
            }

            .syllabus-nav-bar a.back-to-syllabus {
                margin-bottom: 0;
            }

            .syllabus-nav-bar .topic-indicator {
                font-size: 1rem;
            }
        }

        .topic-header {
            text-align: left;
            padding-bottom: 18px;
            margin-bottom: 32px;
            border-bottom: 1.5px solid var(--border-color);
        }

        .topic-header h1 {
            color: var(--primary-purple-dark);
            margin: 0 0 7px 0;
            font-size: 2.1em;
            font-weight: 700;
            letter-spacing: -0.5px;
        }

        .topic-header p {
            color: var(--subtle-text-color);
            font-size: 1.08em;
            margin: 0;
        }

        section {
            margin-bottom: 38px;
            padding: 30px 28px 28px 28px;
            background: var(--card-bg-color);
            border-radius: var(--border-radius-md);
            box-shadow: var(--shadow-sm);
            border: 1px solid var(--border-color);
        }

        section h2 {
            font-size: 1.6em;
            color: var(--primary-purple-dark);
            border-bottom: 2px solid var(--primary-purple);
            margin-bottom: 22px;
            padding-bottom: 8px;
            font-weight: 700;
        }

        section h3 {
            font-size: 1.22em;
            margin-top: 32px;
            margin-bottom: 14px;
            color: var(--primary-purple);
            font-weight: 600;
            padding-bottom: 6px;
            border-bottom: 1px dashed var(--border-color);
        }

        section h4 {
            font-size: 1.08em;
            margin-top: 22px;
            margin-bottom: 9px;
            color: var(--primary-purple-dark);
            font-weight: 600;
        }

        section ul {
            padding-left: 22px;
            margin-top: 0.5rem;
        }

        section ul li {
            margin-bottom: 0.5rem;
            list-style-type: disc;
        }

        section ul li::marker {
            color: var(--primary-purple);
        }

        .diagram-placeholder,
        .image-container {
            border: 1.5px dashed var(--primary-purple-light);
            padding: 22px;
            margin: 22px 0;
            text-align: center;
            background: linear-gradient(90deg, #f8f7ff 80%, #ece9fa 100%);
            border-radius: var(--border-radius-md);
            color: var(--subtle-text-color);
            font-style: italic;
            font-size: 1.01em;
        }

        .diagram-placeholder p,
        .image-container p {
            margin: 7px 0;
        }

        .image-container img {
            max-width: 100%;
            border-radius: 0.5em;
            margin-bottom: 8px;
            box-shadow: 0 2px 8px 0 rgba(80, 70, 140, 0.08);
        }

        .code-block {
            background: var(--code-bg-color);
            color: var(--code-text-color);
            padding: 1.2rem 1.2rem 1.2rem 1.5rem;
            border-radius: var(--border-radius-md);
            overflow-x: auto;
            margin: 1.2rem 0;
            position: relative;
            box-shadow: 0 2px 12px 0 rgba(30, 30, 60, 0.10);
            font-family: var(--font-mono);
            font-size: 1em;
        }

        .code-block pre {
            margin: 0;
            font-family: var(--font-mono);
            font-size: 1em;
            line-height: 1.7;
        }

        .copy-btn {
            position: absolute;
            top: 0.8rem;
            right: 1.1rem;
            background: linear-gradient(90deg, var(--primary-purple) 60%, var(--primary-purple-dark) 100%);
            color: white;
            border: none;
            padding: 0.38rem 1.1rem;
            border-radius: 1.2em;
            cursor: pointer;
            font-size: 0.93em;
            font-weight: 600;
            letter-spacing: 0.01em;
            box-shadow: 0 2px 8px 0 rgba(80, 70, 140, 0.10);
            transition: background 0.18s, box-shadow 0.18s;
        }

        .copy-btn:hover {
            background: linear-gradient(90deg, var(--primary-purple-dark) 60%, var(--primary-purple) 100%);
            box-shadow: 0 4px 16px 0 rgba(80, 70, 140, 0.13);
        }

        [data-tooltip] {
            position: relative;
            cursor: help;
            border-bottom: 1px dotted var(--primary-purple);
        }

        [data-tooltip]::before,
        [data-tooltip]::after {
            visibility: hidden;
            opacity: 0;
            pointer-events: none;
            transition: opacity 0.2s, transform 0.2s;
            position: absolute;
            left: 50%;
            z-index: 1001;
        }

        [data-tooltip]::before {
            content: attr(data-tooltip);
            background: var(--tooltip-bg);
            color: var(--tooltip-text);
            padding: 0.5rem 0.75rem;
            border-radius: var(--border-radius-md);
            font-size: 0.85em;
            white-space: nowrap;
            bottom: 140%;
            transform: translateX(-50%) translateY(5px);
        }

        [data-tooltip]::after {
            content: '';
            border-style: solid;
            border-width: 6px 6px 0 6px;
            border-color: var(--tooltip-bg) transparent transparent transparent;
            bottom: calc(140% - 6px);
            transform: translateX(-50%) translateY(5px);
        }

        [data-tooltip]:hover::before,
        [data-tooltip]:hover::after {
            visibility: visible;
            opacity: 1;
            transform: translateX(-50%) translateY(0);
        }

        .topic-navigation {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 1.5rem;
            margin-top: 32px;
            margin-bottom: 0;
        }

        .topic-navigation a {
            background: linear-gradient(90deg, var(--primary-purple) 60%, var(--primary-purple-dark) 100%);
            color: #fff;
            border-radius: 2em;
            padding: 0.8em 2.2em;
            font-size: 1.08em;
            font-weight: 700;
            text-decoration: none;
            box-shadow: 0 2px 12px 0 rgba(80, 70, 140, 0.13);
            transition: background 0.18s, box-shadow 0.18s, transform 0.13s;
            display: inline-flex;
            align-items: center;
            gap: 0.7em;
        }

        .topic-navigation a:hover {
            background: linear-gradient(90deg, var(--primary-purple-dark) 60%, var(--primary-purple) 100%);
            box-shadow: 0 6px 24px 0 rgba(80, 70, 140, 0.18);
            transform: translateY(-2px) scale(1.04);
        }

        footer {
            text-align: center;
            padding: 25px;
            background: var(--primary-purple-dark);
            color: rgba(255, 255, 255, 0.85);
            margin-top: 40px;
            font-size: 0.97em;
            letter-spacing: 0.01em;
        }

        @media (max-width: 1024px) {
            #main-content {
                padding: 20px 10px;
            }

            .main-content-float {
                padding: 18px 8px 18px 8px;
            }
        }

        @media (max-width: 768px) {
            .page-container {
                flex-direction: column;
            }

            #sidebar {
                width: 100%;
                min-width: 0;
                height: auto;
                position: static;
                border-right: none;
                border-bottom: 1.5px solid var(--border-color);
                box-shadow: none;
                top: 0;
            }

            .main-header {
                padding: 18px;
                position: static;
            }

            .main-header h1 {
                font-size: 1.5em;
            }

            .topic-header h1 {
                font-size: 1.2em;
            }

            section h2 {
                font-size: 1.1em;
            }

            section h3 {
                font-size: 1em;
            }

            section h4 {
                font-size: 0.97em;
            }

            #main-content {
                padding: 10px 2vw;
            }

            .main-content-float {
                padding: 10px 2vw;
            }

            .topic-navigation {
                flex-direction: column;
                gap: 0.7em;
            }

            .topic-navigation a {
                width: 100%;
                text-align: center;
            }
        }

        .command-prompt {
            color: var(--primary-purple);
            font-weight: 600;
        }

        .command-error {
            color: var(--danger-color);
        }
    </style>
</head>

<body>
    <header class="main-header">
        <div class="eduvishion-logo">
            <a href="../../index.html" class="flex items-center group">
                <span class="text-white">Edu</span>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"
                    fill="none" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
                    <path
                        d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z" />
                </svg>
                <span class="text-white">ision</span>
            </a>
        </div>
        <h1>Operating Systems</h1>
        <p class="subtitle">EduVision :: Full Syllabus with Concepts, Notes, and Diagrams</p>
    </header>

    <div class="page-container">
        <nav id="sidebar">
            <h2>Navigation</h2>
            <ul id="toc-list">
                <li><a href="#threads-intro" class="active">1. Threads Introduction</a></li>
                <li><a href="#multithreading-models">2. Multithreading Models</a></li>
                <li><a href="#process-vs-thread">3. Process vs. Thread</a></li>
                <li><a href="#concurrency-issues">4. Concurrency Issues</a></li>
                <li><a href="#synchronization-tools">5. Synchronization Tools</a></li>
                <li><a href="#classical-sync-problems">6. Classical Problems</a></li>
                <li><a href="#deadlocks">7. Deadlocks</a></li>
            </ul>
        </nav>

        <main id="main-content">
            <div class="syllabus-nav-bar">
                <a href="../../index.html" class="back-to-syllabus"> <!-- Update this path as needed -->
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                        <path fill-rule="evenodd"
                            d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z"
                            clip-rule="evenodd" />
                    </svg>
                    Back to Syllabus
                </a>
                <div class="topic-indicator">
                    Topic 3
                </div>
            </div>

            <div class="topic-header">
                <h1>Topic 3: Threads and Concurrency</h1>
                <p>Delving into threads, multithreading models, concurrency challenges like race conditions, and
                    synchronization mechanisms including mutexes, semaphores, monitors, and an introduction to
                    deadlocks.</p>
            </div>

            <section id="threads-intro">
                <h2>1. Threads Introduction</h2>
                <p>A <strong
                        data-tooltip="A single sequence stream within a process. Threads have their own program counter, register set, and stack space, but share address space (code, data, files) with other threads in the same process.">thread</strong>
                    is a flow of execution through the process code, with its own program counter, system registers and
                    stack. Threads are a popular way to improve application performance through parallelism. A thread is
                    sometimes called a lightweight process.</p>
                <p>Threads represent a software approach to improving performance of operating system by reducing the
                    overhead; thread is equivalent to a classical process in terms of execution flow.</p>
                <ul>
                    <li>Each thread belongs to exactly one process and no thread can exist outside a process.</li>
                    <li>Each thread represents a separate flow of control.</li>
                </ul>
                <div class="image-container">
                    <img src="placeholder_single_multi_threaded.png" alt="Single-threaded vs Multithreaded Processes">
                    <p>Figure: Single-threaded and multithreaded processes.</p>
                </div>
                <p>Threads have been successfully used in implementing network servers. They also provide a suitable
                    foundation for parallel execution of applications on shared memory multiprocessors.</p>

                <h3>Benefits of Threads</h3>
                <p>The benefits of multithreaded programming can be broken down into four major categories:</p>
                <ol>
                    <li><strong>Responsiveness:</strong> Multithreading an interactive application may allow a program
                        to continue running even if part of it is blocked or is performing a lengthy operation, thereby
                        increasing responsiveness to the user. For instance, a web browser can still interact with the
                        user while a separate thread downloads an image.</li>
                    <li><strong>Resource Sharing:</strong> Processes can only share resources through techniques such as
                        shared memory and message passing. Such techniques must be explicitly arranged by the
                        programmer. However, threads share the memory and the resources of the process to which they
                        belong by default. The benefit of sharing code and data is that it allows an application to have
                        several different threads of activity within the same address space.</li>
                    <li><strong>Economy:</strong> Allocating memory and resources for process creation is costly.
                        Because threads share the resources of the process to which they belong, it is more economical
                        to create and context-switch threads. Empirically, creating a process is about thirty times
                        slower than is creating a thread, and context switching is about five times slower between
                        processes than between threads (e.g., in Solaris).</li>
                    <li><strong>Scalability:</strong> The benefits of multithreading can be even greater in a
                        multiprocessor architecture, where threads may be running in parallel on different processing
                        cores. A single-threaded process can run on only one processor, regardless of how many are
                        available.</li>
                </ol>
                <div class="image-container">
                    <img src="placeholder_concurrent_single_core.png"
                        alt="Concurrent execution on a single-core system">
                    <p>Figure: Concurrent execution on a single-core system (T1, T2, T3, T4 interleaved).</p>
                </div>

                <h3>Types of Threads</h3>
                <p>Thread is implemented in two ways:</p>
                <ol>
                    <li>User Level Thread</li>
                    <li>Kernel Level Thread</li>
                </ol>

                <h4>1. User Level Thread</h4>
                <ul>
                    <li>In a user thread, all of the work of thread management is done by the application and the kernel
                        is not aware of the existence of threads.</li>
                    <li>The thread library contains code for creating and destroying threads, for passing message and
                        data between threads, for scheduling thread execution and for saving and restoring thread
                        contexts.</li>
                    <li>The application begins with a single thread and begins running in that thread.</li>
                    <li>User level threads are generally fast to create and manage.</li>
                </ul>
                <strong>Advantages of user level thread over Kernel level thread:</strong>
                <ul>
                    <li>Thread switching does not require Kernel mode privileges.</li>
                    <li>User level thread can run on any operating system.</li>
                    <li>Scheduling can be application specific.</li>
                    <li>User level threads are fast to create and manage.</li>
                </ul>
                <strong>Disadvantages of user level thread:</strong>
                <ul>
                    <li>In a typical operating system, most system calls are blocking.</li>
                    <li>Multithreaded application cannot take advantage of multiprocessing.</li>
                </ul>

                <h4>2. Kernel Level Threads</h4>
                <ul>
                    <li>In Kernel level thread, thread management done by the Kernel.</li>
                    <li>There is no thread management code in the application area.</li>
                    <li>Kernel threads are supported directly by the operating system.</li>
                    <li>Any application can be programmed to be multithreaded. All of the threads within an application
                        are supported within a single process.</li>
                    <li>The Kernel maintains context information for the process as a whole and for individual threads
                        within the process.</li>
                    <li>Scheduling by the Kernel is done on a thread basis.</li>
                    <li>The Kernel performs thread creation, scheduling and management in Kernel space.</li>
                    <li>Kernel threads are generally slower to create and manage than the user threads.</li>
                </ul>
                <strong>Advantages of Kernel level thread:</strong>
                <ul>
                    <li>Kernel can simultaneously schedule multiple threads from the same process on multiple
                        processors.</li>
                    <li>If one thread in a process is blocked, the Kernel can schedule another thread of the same
                        process.</li>
                    <li>Kernel routines themselves can multithreaded.</li>
                </ul>
                <strong>Disadvantages:</strong>
                <ul>
                    <li>Kernel threads are generally slower to create and manage than the user threads.</li>
                    <li>Transfer of control from one thread to another within same process requires a mode switch to the
                        Kernel.</li>
                </ul>

                <h3>Overall Benefits/Advantages of Thread</h3>
                <ul>
                    <li>Thread minimizes context switching time.</li>
                    <li>Use of threads provides concurrency within a process.</li>
                    <li>Efficient communication.</li>
                    <li>Economy- It is more economical to create and context switch threads.</li>
                    <li>Utilization of multiprocessor architectures - The benefits of multithreading can be greatly
                        increased in a multiprocessor architecture.</li>
                </ul>

                <h4>Difference between User Level & Kernel Level Thread (Table)</h4>
                <table>
                    <thead>
                        <tr>
                            <th>Feature</th>
                            <th>User Level Threads</th>
                            <th>Kernel Level Threads</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1. Creation & Management</td>
                            <td>Faster to create and manage.</td>
                            <td>Slower to create and manage.</td>
                        </tr>
                        <tr>
                            <td>2. Implementation</td>
                            <td>Implemented by a thread library at the user level.</td>
                            <td>Operating system support directly to Kernel threads.</td>
                        </tr>
                        <tr>
                            <td>3. OS Awareness</td>
                            <td>User level thread can run on any operating system (kernel unaware).</td>
                            <td>Kernel level threads are specific to the operating system.</td>
                        </tr>
                        <tr>
                            <td>4. Support</td>
                            <td>Support provided at the user level is called user level thread.</td>
                            <td>Support may be provided by kernel is called Kernel level threads.</td>
                        </tr>
                        <tr>
                            <td>5. Multiprocessing</td>
                            <td>Multithreaded application cannot take advantage of multiprocessing.</td>
                            <td>Kernel routines themselves can be multithreaded (can utilize multiprocessing).</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section id="multithreading-models">
                <h2>2. Multithreading Models</h2>
                <p>Some operating system provide a combined user level thread and Kernel level thread facility. Solaris
                    is a good example of this combined approach. In a combined system, multiple threads within the same
                    application can run in parallel on multiple processors and a blocking system call need not block the
                    entire process.</p>
                <p>Multithreading models are three types:</p>
                <ol>
                    <li>Many to many relationship.</li>
                    <li>Many to one relationship.</li>
                    <li>One to one relationship.</li>
                </ol>

                <h3>1. Many-to-Many Model</h3>
                <ul>
                    <li>In this model, many user level threads multiplexes to the Kernel thread of smaller or equal
                        numbers.</li>
                    <li>The number of Kernel threads may be specific to either a particular application or a particular
                        machine.</li>
                </ul>
                <div class="image-container">
                    <img src="placeholder_many_to_many_model.png" alt="Many-to-Many Multithreading Model">
                    <p>Figure: Many-to-many model. In this model, developers can create as many user threads as
                        necessary and the corresponding Kernel threads can run in parallels on a multiprocessor.</p>
                </div>

                <h3>2. Many-to-One Model</h3>
                <ul>
                    <li>Many to one model maps many user level threads to one Kernel level thread.</li>
                    <li>Thread management is done in user space. When thread makes a blocking system call, the entire
                        process will be blocks.</li>
                    <li>Only one thread can access the Kernel at a time, so multiple threads are unable to run in
                        parallel on multiprocessors.</li>
                </ul>
                <div class="image-container">
                    <img src="placeholder_many_to_one_model.png" alt="Many-to-One Multithreading Model">
                    <p>Figure: Many-to-one model. If the user level thread libraries are implemented in the operating
                        system, that system does not support Kernel threads use the many to one relationship modes.</p>
                </div>

                <h3>3. One-to-One Model</h3>
                <ul>
                    <li>There is one to one relationship of user level thread to the kernel level thread.</li>
                    <li>This model provides more concurrency than the many to one model.</li>
                    <li>It allows another thread to run when a thread makes a blocking system call. It supports multiple
                        threads to execute in parallel on microprocessors.</li>
                </ul>
                <div class="image-container">
                    <img src="placeholder_one_to_one_model.png" alt="One-to-One Multithreading Model">
                    <p>Figure: One-to-one model. Disadvantage of this model is that creating user thread requires the
                        corresponding Kernel thread. OS/2, Windows NT and windows 2000 use one to one relationship
                        model.</p>
                </div>
            </section>

            <section id="process-vs-thread">
                <h2>3. Difference between Process and Thread</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Process</th>
                            <th>Thread</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Process is heavy weight or resource intensive.</td>
                            <td>Thread is light weight taking lesser resources than a process.</td>
                        </tr>
                        <tr>
                            <td>Process switching needs interaction with operating system.</td>
                            <td>Thread switching does not need to interact with operating system.</td>
                        </tr>
                        <tr>
                            <td>In multiple processing environments each process executes the same code but has its own
                                memory and file resources.</td>
                            <td>All threads can share same set of open files, child processes.</td>
                        </tr>
                        <tr>
                            <td>If one process is blocked then no other process can execute until the first process is
                                unblocked.</td>
                            <td>While one thread is blocked and waiting, second thread in the same task can run.</td>
                        </tr>
                        <tr>
                            <td>Multiple processes without using threads use more resources.</td>
                            <td>Multiple threaded processes use fewer resources.</td>
                        </tr>
                        <tr>
                            <td>In multiple processes each process operates independently of the others.</td>
                            <td>One thread can read, write or change another thread's data.</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <!-- Sections for Concurrency, Synchronization, Deadlocks will follow -->
            <!-- For brevity, I'll add stubs for the remaining sections. Populate them from your images. -->

            <section id="concurrency-issues">
                <h2>4. Concurrency Issues</h2>
                <h3>Race Conditions</h3>
                <p>A situation where several processes access and manipulate the same data concurrently and the outcome
                    of the execution depends on the particular order in which the access takes place, is called a
                    <strong
                        data-tooltip="A situation where the outcome of an operation depends on the unpredictable sequence or timing of other events.">race
                        condition</strong>.
                </p>
                <ul>
                    <li>Suppose that two processes, P1 and P2, share the global variable 'a'. At some point in its
                        execution, P1 updates 'a' to the value 1, and at some point P2 updates 'a' to the value 2. Thus,
                        the two tasks are in a race to write variable 'a'.</li>
                    <li>In this example the "loser" of the race (the process that updates last) determines the final
                        value of 'a'.</li>
                </ul>
                <h4>Therefore Operating System Concerns of following things:</h4>
                <ol>
                    <li>The operating system must be able to keep track of the various processes.</li>
                    <li>The operating system must allocate and deallocate various resources for each active process.
                    </li>
                    <li>The operating system must protect the data and physical resources of each process against
                        unintended interference by other processes.</li>
                    <li>The functioning of a process, and the output it produces, must be independent of the speed at
                        which its execution is carried out relative to the speed of other concurrent processes.</li>
                </ol>

                <h3>Process Interaction can be defined as:</h3>
                <ul>
                    <li>Processes unaware of each other</li>
                    <li>Processes indirectly aware of each other</li>
                    <li>Processes directly aware of each other</li>
                </ul>
                <p>Concurrent processes come into conflict with each other when they are competing for the use of the
                    same resource.</p>
                <p>Two or more processes need to access a resource during the course of their execution. Each process is
                    unaware of the existence of the other processes. There is no exchange of information between the
                    competing processes.</p>

                <hr class="subsection-divider">
                <h3>Critical Section Problem</h3>
                <p>Consider a system consisting of n processes (P0, P1, ..., Pn-1) each process has a segment of code
                    which is known as a critical section in which the process may be changing common variable, updating
                    a table, writing a file and so on.</p>
                <ul>
                    <li>The important feature of the system is that when the process is executing in its critical
                        section no other process is to be allowed to execute in its critical section.</li>
                    <li>The execution of critical sections by the processes is a mutually exclusive.</li>
                    <li>The critical section problem is to design a protocol that the process can use to co-operate each
                        process must request permission to enter its critical section.</li>
                    <li>The section of code implementing this request is the entry section. The critical section is
                        followed on exit section. The remaining code is the remainder section.</li>
                </ul>
                <div class="code-block">
                    <pre><code>Example:
While (1) 
{
    Entry Section;

    Critical Section;

    Exit Section;

    Remainder Section;
}</code></pre>
                </div>
                <p>A solution to the critical section problem must satisfy the following three conditions:</p>
                <ol>
                    <li><strong>Mutual Exclusion:</strong> If process Pi is executing in its critical section then no
                        any other process can be executing in their critical section.</li>
                    <li><strong>Progress:</strong> If no process is executing in its critical section and some process
                        wish to enter their critical sections then only those process that are not executing in their
                        remainder section can enter its critical section next.</li>
                    <li><strong>Bounded waiting:</strong> There exists a bound on the number of times that other
                        processes are allowed to enter their critical sections after a process has made a request.</li>
                </ol>
            </section>

            <section id="synchronization-tools">
                <h2>5. Synchronization Tools</h2>
                <p>Operating-systems designers build software tools to solve the critical-section problem.</p>
                <h3>Mutex Locks</h3>
                <p>The simplest of these tools is the <strong
                        data-tooltip="Mutual Exclusion lock. A mechanism to ensure that only one thread or process can access a critical section of code or a shared resource at a time.">mutex
                        lock</strong>. (In fact, the term mutex is short for mutual exclusion.) We use the mutex lock to
                    protect critical regions and thus prevent race conditions. That is, a process must acquire the lock
                    before entering a critical section; it releases the lock when it exits the critical section. The
                    acquire() function acquires the lock, and the release() function releases the lock.</p>
                <p>A mutex lock has a boolean variable `available` whose value indicates if the lock is available or
                    not. If the lock is available, a call to acquire() succeeds, and the lock is then considered
                    unavailable. A process that attempts to acquire an unavailable lock is blocked until the lock is
                    released.</p>
                <div class="code-block">
                    <pre><code>The definition of acquire() is as follows:
acquire() {
    while (!available)
        ; /* busy wait */
    available = false;;
}

Solution to the critical-section problem using mutex locks:
do {
    acquire lock

    critical section

    release lock

    remainder section
} while (true);

The definition of release() is as follows:
release() {
    available = true;
}</code></pre>
                </div>
                <p>The main disadvantage of the implementation given here is that it requires <strong
                        data-tooltip="A process repeatedly checks a condition (e.g., lock availability) in a loop without yielding the CPU.">busy
                        waiting</strong>. While a process is in its critical section, any other process that tries to
                    enter its critical section must loop continuously in the call to acquire(). In fact, this type of
                    mutex lock is also called a <strong
                        data-tooltip="A mutex lock where a process 'spins' in a loop checking if the lock is available, rather than blocking and yielding the CPU.">spinlock</strong>
                    because the process “spins” while waiting for the lock to become available.</p>

                <hr class="subsection-divider">
                <h3>Semaphores</h3>
                <p>A <strong
                        data-tooltip="A synchronization variable that, apart from initialization, is accessed only through two standard atomic operations: wait() and signal().">semaphore</strong>
                    S is an integer variable that, apart from initialization, is accessed only through two standard
                    atomic operations: wait() and signal().</p>
                <p>The wait() operation was originally termed P (from the Dutch proberen, “to test”); signal() was
                    originally called V (from verhogen, “to increment”).</p>
                <div class="code-block">
                    <pre><code>The definition of wait() is as follows:
wait(S) {
    while (S <= 0)
        ; // busy wait
    S--;
}

The definition of signal() is as follows:
signal(S) {
    S++;
}</code></pre>
                </div>
                <p>The integer value of the semaphore in the wait and signal operations must be executed indivisibly.
                    That is, when one process modifies the semaphore value, no other process can simultaneously modify
                    that same semaphore value. In addition, in the case of the wait(S), the testing of the integer value
                    of S (S <= 0) and its possible modification (S--) must also be executed without interruption.</p>
                        <h4>Semaphore as General Synchronization Tool</h4>
                        <ul>
                            <li><strong>Counting semaphore</strong> – integer value can range over an unrestricted
                                domain.</li>
                            <li><strong>Binary semaphore</strong> – integer value can range only between 0 and 1; Can be
                                simpler to implement. Also known as mutex locks. Can implement a counting semaphore S as
                                a binary semaphore.</li>
                        </ul>
                        <div class="code-block">
                            <pre><code>Provides mutual exclusion Semaphore mutex; // initialized to 1
do {
    wait(mutex);
        // Critical Section
    signal(mutex);
} while (TRUE);</code></pre>
                        </div>
                        <h4>Semaphore Implementation</h4>
                        <ul>
                            <li>Must guarantee that no two processes can execute wait() and signal() on the same
                                semaphore at the same time.</li>
                            <li>Thus, implementation becomes the critical section problem where the wait and signal code
                                are placed in the critical section.</li>
                            <li>Could now have busy waiting in critical section implementation. But implementation code
                                is short. Little busy waiting if critical section rarely occupied.</li>
                            <li>Note that applications may spend lots of time in critical sections and therefore this is
                                not a good solution.</li>
                        </ul>
                        <h4>Semaphore Implementation with no Busy waiting:</h4>
                        <ul>
                            <li>With each semaphore there is an associated waiting queue.</li>
                            <li>Each entry in a waiting queue has two data items: value (of type integer), pointer to
                                next record in the list.</li>
                            <li>Two operations:
                                <ul>
                                    <li>block – place the process invoking the operation on the appropriate waiting
                                        queue.</li>
                                    <li>wakeup – remove one of processes in the waiting queue and place it in the ready
                                        queue.</li>
                                </ul>
                            </li>
                        </ul>
                        <div class="code-block">
                            <pre><code>Implementation of wait:
wait(semaphore *S) {
    S->value--;
    if (S->value < 0) {
        add this process to S->list;
        block();
    }
}

Implementation of signal:
signal(semaphore *S) {
    S->value++;
    if (S->value <= 0) {
        remove a process P from S->list;
        wakeup(P);
    }
}</code></pre>
                        </div>
                        <p>Semaphores are not provided by hardware. But they have several attractive properties:</p>
                        <ol>
                            <li>Semaphores are machine independent.</li>
                            <li>Semaphores are simple to implement.</li>
                            <li>Correctness is easy to determine.</li>
                            <li>Can have many different critical sections with different semaphores.</li>
                            <li>Semaphore acquires many resources simultaneously.</li>
                        </ol>
                        <h4>Drawback of Semaphore</h4>
                        <ol>
                            <li>They are essentially shared global variables.</li>
                            <li>Access to semaphores can come from anywhere in a program.</li>
                            <li>There is no control or guarantee of proper usage.</li>
                            <li>There is no linguistic connection between the semaphore and the data to which the
                                semaphore controls access.</li>
                            <li>They serve two purposes, mutual exclusion and scheduling constraints.</li>
                        </ol>

                        <hr class="subsection-divider">
                        <h3>Monitors</h3>
                        <p>Although semaphores provide a convenient and effective mechanism for process synchronization,
                            using them incorrectly can result in timing errors that are difficult to detect, since these
                            errors happen only if particular execution sequences take place and these sequences do not
                            always occur.</p>
                        <ul>
                            <li>The <strong
                                    data-tooltip="A high-level synchronization construct that provides mutual exclusion and condition variables for more structured concurrency control.">monitor</strong>
                                is a programming-language construct that provides equivalent functionality to that of
                                semaphores and that is easier to control.</li>
                            <li>The monitor construct has been implemented in a number of programming languages,
                                including Concurrent Pascal, Pascal-Plus, Modula-2, Modula-3, and Java.</li>
                            <li>An <strong
                                    data-tooltip="A data type that encapsulates data with a set of functions (operations) to operate on that data, independent of any specific implementation of the ADT.">abstract
                                    data type—or ADT</strong>—encapsulates data with a set of functions to operate on
                                that data that are independent of any specific implementation of the ADT.</li>
                            <li>A monitor type is an ADT that includes a set of programmer defined operations that are
                                provided with mutual exclusion within the monitor. The monitor type also declares the
                                variables whose values define the state of an instance of that type, along with the
                                bodies of functions that operate on those variables.</li>
                            <li>It has also been implemented as a program library.</li>
                            <li>This allows programmers to put monitor locks on any object.</li>
                        </ul>
                        <div class="code-block">
                            <pre><code>monitor monitor_name {
    /* shared variable declarations */
    function P1 (...) { ... }
    function P2 (...) { ... }
    ...
    function Pn (...) { ... }
    initialization code (...) { ... }
}</code></pre>
                            <p>Figure: Syntax of a monitor.</p>
                        </div>
                        <ul>
                            <li>Thus, a function defined within a monitor can access only those variables declared
                                locally within the monitor and its formal parameters.</li>
                            <li>Similarly, the local variables of a monitor can be accessed by only the local functions.
                            </li>
                        </ul>
                        <div class="image-container">
                            <img src="placeholder_monitor_schematic.png" alt="Schematic view of a monitor">
                            <p>Figure: Schematic view of a monitor (entry queue, shared data, operations, initialization
                                code).</p>
                        </div>
                        <ul>
                            <li>The monitor construct ensures that only one process at a time is active within the
                                monitor.</li>
                            <li>However, the monitor construct, as defined so far, is not sufficiently powerful for
                                modeling some synchronization schemes. For this purpose, we need to define additional
                                synchronization mechanisms. These mechanisms are provided by condition variables.
                                <br><code>condition x, y;</code>
                            </li>
                            <li>The only operations that can be invoked on a condition variable are wait() and signal().
                                <br><code>x.wait();</code>
                                <br>The operation means that the process invoking this operation is suspended until
                                another process invokes x.signal().
                                <br><code>x.signal();</code>
                                <br>The x.signal() operation resumes exactly one suspended process. If no process is
                                suspended, then the signal() operation has no effect;
                            </li>
                        </ul>
                        <div class="image-container">
                            <img src="placeholder_monitor_with_conditions.png" alt="Monitor with condition variables">
                            <p>Figure: Monitor with condition variables (x, y conditions associated with queues).</p>
                        </div>
                        <ul>
                            <li>Now suppose that, when the x.signal() operation is invoked by a process P, there exists
                                a suspended process Q associated with condition x. Clearly, if the suspended process Q
                                is allowed to resume its execution, the signaling process P must wait. Otherwise, both P
                                and Q would be active simultaneously within the monitor. Note, however, that
                                conceptually both processes can continue with their execution.</li>
                        </ul>
                        <p>Two possibilities exist:</p>
                        <ol>
                            <li><strong>Signal and wait.</strong> P either waits until Q leaves the monitor or waits for
                                another condition.</li>
                            <li><strong>Signal and continue.</strong> Q either waits until P leaves the monitor or waits
                                for another condition.</li>
                        </ol>
                        <ul>
                            <li>On the one hand, since P was already executing in the monitor, the signal and continue
                                method seems more reasonable. On the other, if we allow thread P to continue, then by
                                the time Q is resumed, the logical condition for which Q was waiting may no longer hold.
                                When thread P executes the signal operation, it immediately leaves the monitor. Hence, Q
                                is immediately resumed.</li>
                        </ul>
            </section>

            <section id="classical-sync-problems">
                <h2>6. Classical Problems of Synchronization</h2>
                <ul>
                    <li>Bounded-Buffer Problem</li>
                    <li>Readers and Writers Problem</li>
                    <li>Dining-Philosophers Problem</li>
                </ul>

                <h3>Bounded-Buffer Problem</h3>
                <p>The pool consists of n buffers, each capable of holding one item. The mutex semaphore provides mutual
                    exclusion for accesses to the buffer pool and is initialized to the value 1. The empty and full
                    semaphores count the number of empty and full buffers. The semaphore empty is initialized to the
                    value n; the semaphore full is initialized to the value 0.</p>
                <ul>
                    <li>N buffers, each can hold one item</li>
                    <li>Semaphore mutex initialized to the value 1</li>
                    <li>Semaphore full initialized to the value 0</li>
                    <li>Semaphore empty initialized to the value N.</li>
                </ul>
                <div class="code-block">
                    <pre><code>// The structure of the producer process
do {
    // produce an item in nextp
    wait(empty);
    wait(mutex);
        // add the item to the buffer
    signal(mutex);
    signal(full);
} while (TRUE);

// The structure of the consumer process
do {
    wait(full);
    wait(mutex);
        // remove an item from buffer to nextc
    signal(mutex);
    signal(empty);
    // consume the item in nextc
} while (TRUE);</code></pre>
                </div>
                <div class="image-container">
                    <img src="placeholder_bounded_buffer_diagram.png" alt="Bounded Buffer Diagram">
                </div>

                <hr class="subsection-divider">
                <h3>Readers-Writers Problem</h3>
                <p>Suppose that a database is to be shared among several concurrent processes. Some of these processes
                    may want only to read the database, whereas others may want to update (that is, to read and write)
                    the database. We distinguish between these two types of processes by referring to the former as
                    readers and to the latter as writers. Obviously, if two readers access the data simultaneously, no
                    adverse effects will result. However, if a writer and some other process (either a reader or a
                    writer) access the database simultaneously, chaos may ensue.</p>
                <ul>
                    <li>A data set is shared among a number of concurrent processes
                        <ul>
                            <li>Readers – only read the data set; they do not perform any updates</li>
                            <li>Writers – can both read and write</li>
                        </ul>
                    </li>
                    <li>Problem – allow multiple readers to read at the same time. Only one single writer can access the
                        shared data at the same time.</li>
                    <li>Shared Data
                        <ul>
                            <li>Data set</li>
                            <li>Semaphore mutex initialized to 1</li>
                            <li>Semaphore wrt initialized to 1</li>
                            <li>Integer readcount initialized to 0</li>
                        </ul>
                    </li>
                </ul>
                <div class="code-block">
                    <pre><code>// The structure of a writer process
do {
    wait(wrt);
        // writing is performed
    signal(wrt);
} while (TRUE);

// The structure of a reader process
do {
    wait(mutex);
    readcount++;
    if (readcount == 1)
        wait(wrt);
    signal(mutex);
        // reading is performed
    wait(mutex);
    readcount--;
    if (readcount == 0)
        signal(wrt);
    signal(mutex);
} while (TRUE);</code></pre>
                </div>
                <div class="image-container">
                    <img src="placeholder_readers_writers_diagram.png" alt="Readers-Writers Problem Diagram">
                </div>

                <hr class="subsection-divider">
                <h3>Dining-Philosophers Problem</h3>
                <p>Consider five philosophers who spend their lives thinking and eating. The philosophers share a
                    circular table surrounded by five chairs, each belonging to one philosopher. In the center of the
                    table is a bowl of rice, and the table is laid with five single chopsticks. When a philosopher
                    thinks, she does not interact with her colleagues. From time to time, a philosopher gets hungry and
                    tries to pick up the two chopsticks that are closest to her (the chopsticks that are between her and
                    her left and right neighbors). A philosopher may pick up only one chopstick at a time. Obviously,
                    she cannot pick up a chopstick that is already in the hand of a neighbor. When a hungry philosopher
                    has both her chopsticks at the same time, she eats without releasing the chopsticks. When she is
                    finished eating, she puts down both chopsticks and starts thinking again.</p>
                <ul>
                    <li>Shared data
                        <ul>
                            <li>Bowl of rice (data set)</li>
                            <li>Semaphore chopstick [5] initialized to 1</li>
                        </ul>
                    </li>
                </ul>
                <div class="code-block">
                    <pre><code>// The structure of Philosopher i:
do {
    wait(chopstick[i]);
    wait(chopstick[(i + 1) % 5]);
        // eat
    signal(chopstick[i]);
    signal(chopstick[(i + 1) % 5]);
        // think
} while (TRUE);</code></pre>
                </div>
                <div class="image-container">
                    <img src="placeholder_dining_philosophers_diagram.png" alt="Dining Philosophers Problem Diagram">
                </div>
            </section>

            <section id="deadlocks">
                <h2>7. Deadlocks</h2>
                <p><strong>Definition:</strong> In a multiprogramming environment, several processes may compete for a
                    finite number of resources. A process requests resources; if the resources are not available at that
                    time, the process enters a waiting state. Sometimes, a waiting process is never again able to change
                    state, because the resources it has requested are held by other waiting processes. This situation is
                    called a <strong
                        data-tooltip="A situation where two or more processes are unable to proceed because each is waiting for one of the others to release a resource.">deadlock</strong>.
                </p>

                <h3>System Model</h3>
                <ul>
                    <li>A system may consist of finite number of resources and is distributed among number of processes.
                        There resources are partitioned into several instances each with identical instances.</li>
                    <li>A process must request a resource before using it and it must release the resource after using
                        it. It can request any number of resources to carry out a designated task. The amount of
                        resource requested may not exceed the total number of resources available.</li>
                </ul>
                <p>A process may utilize the resources in only the following sequences:</p>
                <ol>
                    <li><strong>Request:</strong> If the request is not granted immediately then the requesting process
                        must wait it can acquire the resources.</li>
                    <li><strong>Use:</strong> The process can operate on the resource.</li>
                    <li><strong>Release:</strong> The process releases the resource after using it.</li>
                </ol>
                <p>Deadlock may involve different types of resources. Example: Consider a system with one printer and
                    one tape drive. If a process Pi currently holds a printer and a process Pj holds the tape drive. If
                    process Pi request a tape drive and process Pj request a printer then a deadlock occurs. Multithread
                    programs are good candidates for deadlock because they compete for shared resources.</p>

                <h3>Deadlock characteristics</h3>
                <p><strong>Necessary Conditions:</strong> A deadlock situation can occur if the following 4 conditions
                    occur simultaneously in a system:-</p>
                <ol>
                    <li><strong>Mutual Exclusion:</strong> Only one process must hold the resource at a time. If any
                        other process requests for the resource, the requesting process must be delayed until the
                        resource has been released.</li>
                    <li><strong>Hold and Wait:</strong> A process must be holding at least one resource and waiting to
                        acquire additional resources that are currently being held by the other process.</li>
                    <li><strong>No Preemption:</strong> Resources can’t be preempted i.e., only the process holding the
                        resources must release it after the process has completed its task.</li>
                    <li><strong>Circular Wait:</strong> A set {P0,P1......Pn} of waiting process must exist such that P0
                        is waiting for a resource i.e., held by P1, P1 is waiting for a resource i.e., held by P2. Pn-1
                        is waiting for resource held by process Pn and Pn is waiting for the resource i.e., held by P1.
                        All the four conditions must hold for a deadlock to occur.</li>
                </ol>
                <div class="image-container"
                    style="display: flex; justify-content: space-around; flex-wrap: wrap; align-items: center;">
                    <img src="placeholder_deadlock_T0_T1.png" alt="Deadlock condition with T0 and T1"
                        style="max-width: 48%; margin-bottom: 10px;">
                    <img src="placeholder_circular_wait_diagram.png" alt="State diagram of circular wait condition"
                        style="max-width: 48%;">
                </div>


                <h3>Resource Allocation Graph:</h3>
                <ol>
                    <li>Deadlocks are described by using a directed graph called system resource allocation graph. The
                        graph consists of set of vertices (v) and set of edges (e).</li>
                    <li>The set of vertices (v) can be described into two different types of nodes P={P1,P2........Pn}
                        i.e., set consisting of all active processes and R={R1,R2.........Rn} i.e., set consisting of
                        all resource types in the system.</li>
                    <li>A directed edge from process Pi to resource type Rj denoted by Pi->Ri indicates that Pi
                        requested an instance of resource Rj and is waiting. This edge is called Request edge.</li>
                    <li>A directed edge Ri-> Pj signifies that resource Rj is held by process Pi. This is called
                        Assignment edge.</li>
                    <li>If the graph contain no cycle, then no process in the system is deadlock. If the graph contains
                        a cycle then a deadlock may exist.</li>
                    <li>If each resource type has exactly one instance than a cycle implies that a deadlock has
                        occurred. If each resource has several instances then a cycle do not necessarily implies that a
                        deadlock has occurred.</li>
                </ol>
                <div class="image-container">
                    <img src="placeholder_resource_allocation_graph.png" alt="Resource Allocation Graph Example">
                    <p>Figure: Resource-allocation graph. P={P1, P2, P3}, R={R1, R2, R3, R4}.</p>
                </div>

                <h3>Methods for Handling Deadlocks:</h3>
                <p>There are three ways to deal with deadlock problem:</p>
                <ul>
                    <li>We can use a protocol to prevent deadlocks ensuring that the system will never enter into the
                        deadlock state.</li>
                    <li>We allow a system to enter into deadlock state, detect it and recover from it.</li>
                    <li>We ignore the problem and pretend that the deadlock never occur in the system. This is used by
                        most OS including UNIX.</li>
                </ul>
                <ul>
                    <li>To ensure that the deadlock never occur the system can use either deadlock avoidance or a
                        deadlock prevention.</li>
                    <li>Deadlock prevention is a set of method for ensuring that at least one of the necessary
                        conditions does not occur.</li>
                    <li>Deadlock avoidance requires the OS is given advance information about which resource a process
                        will request and use during its lifetime.</li>
                    <li>If a system does not use either deadlock avoidance or deadlock prevention then a deadlock
                        situation may occur. During this it can provide an algorithm that examines the state of the
                        system to determine whether a deadlock has occurred and algorithm to recover from deadlock.</li>
                    <li>Undetected deadlock will result in deterioration of the system performance.</li>
                </ul>

                <h4>Deadlock Prevention:</h4>
                <p>For a deadlock to occur each of the four necessary conditions must hold. If at least one of the
                    conditions does not hold then we can prevent occurrence of deadlock.</p>
                <ol>
                    <li><strong>Mutual Exclusion:</strong> This holds for non-sharable resources. Eg:-A printer can be
                        used by only one process at a time. Mutual exclusion is not possible in sharable resources and
                        thus they cannot be involved in deadlock. Read-only files are good examples for sharable
                        resources. A process never waits for accessing a sharable resource. So we cannot prevent
                        deadlock by denying the mutual exclusion condition in non-sharable resources.</li>
                    <li><strong>Hold and Wait:</strong> This condition can be eliminated by forcing a process to release
                        all its resources held by it when it request a resource i.e., not available. One protocol can be
                        used is that each process is allocated with all of its resources before its start execution.
                        Eg:-consider a process that copies the data from a tape drive to the disk, sorts the file and
                        then prints the results to a printer. If all the resources are allocated at the beginning then
                        the tape drive, disk files and printer are assigned to the process. The main problem with this
                        is it leads to low resource utilization because it requires printer at the last and is allocated
                        with it from the beginning so that no other process can use it.</li>
                    <li><strong>No Preemption:</strong> To ensure that this condition never occurs the resources must be
                        preempted. If a process is holding some resource and request another resource that cannot be
                        immediately allocated to it, then all the resources currently held by the requesting process are
                        preempted and added to the list of resources for which other processes may be waiting. The
                        process will be restarted only when it regains the old resources and the new resources that it
                        is requesting.</li>
                    <li><strong>Circular Wait:</strong> The fourth and the final condition for deadlock is the circular
                        wait condition. One way to ensure that this condition never, is to impose ordering on all
                        resource types and each process requests resource in an increasing order.</li>
                </ol>

                <h4>Deadlock Avoidance:</h4>
                <ul>
                    <li>Deadlock prevention algorithm may lead to low device utilization and reduces system throughput.
                    </li>
                    <li>Avoiding deadlocks requires additional information about how resources are to be requested. With
                        the knowledge of the complete sequences of requests and releases we can decide for each requests
                        whether or not the process should wait.</li>
                    <li>For each requests it requires to check the resources currently available, resources that are
                        currently allocated to each processes to decide whether the current requests can be satisfied or
                        must wait to avoid future possible deadlock.</li>
                    <li>A deadlock avoidance algorithm dynamically examines the resources allocation state to ensure
                        that a circular wait condition never exists. The resource allocation state is defined by the
                        number of available and allocated resources and the maximum demand of each process.</li>
                </ul>
                <strong>Safe State:</strong>
                <ul>
                    <li>A state is a safe state in which there exists at least one order in which all the process will
                        run completely without resulting in a deadlock.</li>
                    <li>A system is in safe state if there exists a safe sequence.</li>
                    <li>A sequence of processes <P1,P2,.........Pn> is a safe sequence for the current allocation state
                            if for each Pi the resources that Pi can request can be satisfied by the currently available
                            resources.</li>
                    <li>If the resources that Pi requests are not currently available then Pi can obtain all of its
                        needed resource to complete its designated task.</li>
                    <li>A safe state is not a deadlock state.</li>
                    <li>Whenever a process request a resource i.e., currently available, the system must decide whether
                        resources can be allocated immediately or whether the process must wait. The request is granted
                        only if the allocation leaves the system in safe state.</li>
                    <li>In this, if a process requests a resource i.e., currently available it must still have to wait.
                        Thus resource utilization may be lower than it would be without a deadlock avoidance algorithm.
                    </li>
                </ul>
                <strong>Resource Allocation Graph Algorithm:</strong>
                <p>This algorithm is used only if we have one instance of a resource type. In addition to the request
                    edge and the assignment edge a new edge called claim edge is used. Ex :- A claim edge Pi->Rj
                    indicates that process Pi may request Rj in future. It is represented by a dotted line. When a
                    process Pi requests the resource Rj, the claim edge is converted to a request edge. When resource Rj
                    is released by process Pi, the assignment edge Rj->Pi is replaced by the claim edge Pi->Rj. When a
                    process Pi requests Rj the request is granted only if converting the request edge Pi->Rj to as
                    assignment edge Rj->Pi do not result in a cycle. Cycle detection algorithm is used to detect the
                    cycle. If there are no cycles then the allocation of the resource to process leave the system in
                    safe state.</p>
                <div class="image-container" style="display:flex; justify-content:space-around; align-items:center;">
                    <img src="placeholder_rag_claim_edge1.png" alt="RAG with claim edge (no cycle)"
                        style="max-width: 48%;">
                    <img src="placeholder_rag_claim_edge2.png" alt="RAG with potential cycle" style="max-width: 48%;">
                </div>

                <strong>Banker’s Algorithm:</strong>
                <p>Banker’s algorithm is a deadlock avoidance algorithm. It is named so because this algorithm is used
                    in banking systems to determine whether a loan can be granted or not. Consider there are n account
                    holders in a bank and the sum of the money in all of their accounts is S. Every time a loan has to
                    be granted by the bank, it subtracts the loan amount from the total money the bank has. Then it
                    checks if that difference is greater than S. It is done because, only then, the bank would have
                    enough money even if all the n account holders draw all their money at once. Banker’s algorithm
                    works in a similar way in computers.</p>
                <p>Whenever a new process is created, it must specify the maximum instances of each resource type that
                    it needs, exactly. Let us assume that there are n processes and m resource types. Some data
                    structures that are used to implement the banker’s algorithm are:</p>
                <ol>
                    <li><strong>Available:</strong> It is an array of length m. It represents the number of available
                        resources of each type. If Available[j] = k, then there are k instances available, of resource
                        type R(j).</li>
                    <li><strong>Max:</strong> It is an n x m matrix which represents the maximum number of instances of
                        each resource that a process can request. If Max[i][j] = k, then the process P(i) can request
                        almost k instances of resource type R(j).</li>
                    <li><strong>Allocation:</strong> It is an n x m matrix which represents the number of resources of
                        each type currently allocated to each process. If Allocation[i][j] = k, then process P(i) is
                        currently allocated k instances of resource type R(j).</li>
                    <li><strong>Need:</strong> It is an n x m matrix which indicates the remaining resource needs of
                        each process. If Need[i][j] = k, then process P(i) may need k more instances of resource type
                        R(j) to complete its task. Need[i][j] = Max[i][j] - Allocation [i][j]</li>
                </ol>
                <details>
                    <summary>Safety Algorithm & Resource Allocation Algorithm (Banker's)</summary>
                    <p><strong>Safety Algorithm:</strong> This algorithm is used to find out whether or not a system is
                        in safe state or not.</p>
                    <ol>
                        <li>Let Work and Finish be vectors of length m and n, respectively. Initialize: Work =
                            Available. For i = 1,2, ..., n, if Allocation<sub>i</sub> != 0, then Finish[i] = false;
                            otherwise, Finish[i] = true.</li>
                        <li>Find an index i such that both: Finish[i] == false AND Request<sub>i</sub>
                            <= Work. If no such i exists, go to step 4.</li>
                        <li>Work = Work + Allocation<sub>i</sub>. Finish[i] = true. Go to step 2.</li>
                        <li>If Finish [i] = false, for some i, 1 <= i <=n, then the system is in a deadlock state.
                                Moreover, if Finish[i]=false, then process Pi is deadlocked.</li>
                    </ol>
                    <p><strong>Resource Allocation Algorithm:</strong> Request = request vector for process Pi. If
                        Request<sub>i</sub>[j] = k then process Pi wants k instances of resource type Rj.</p>
                    <ol>
                        <li>If Request<sub>i</sub>
                            <= Need<sub>i</sub> go to step 2. Otherwise, raise error condition, since process has
                                exceeded its maximum claim.
                        </li>
                        <li>If Request<sub>i</sub>
                            <= Available, go to step 3. Otherwise Pi must wait, since resources are not available.</li>
                        <li>Pretend to allocate requested resources to Pi by modifying the state as follows: Available =
                            Available – Request<sub>i</sub>; Allocation<sub>i</sub> = Allocation<sub>i</sub> +
                            Request<sub>i</sub>; Need<sub>i</sub> = Need<sub>i</sub> – Request<sub>i</sub>;.
                            If safe ⇒ the resources are allocated to Pi.
                            If unsafe ⇒ Pi must wait and the old resource-allocation state is restored.
                        </li>
                    </ol>
                    <p><strong>Example (from image):</strong> 5 processes P0-P4; 3 resource types A(10), B(5), C(7).
                        Snapshot at T0...</p>
                    <div class="image-container">
                        <img src="placeholder_bankers_example_tables.png" alt="Banker's Algorithm Example Tables">
                    </div>
                    <div class="image-container">
                        <img src="placeholder_bankers_safe_state_calc.png"
                            alt="Banker's Algorithm Safe State Calculation">
                    </div>
                    <p>Executing safety algorithm shows that sequence <P1, P3, P4, P0, P2> satisfies safety requirement.
                    </p>
                    <p>Can request for (3,3,0) by P4 be granted? –NO (Results Unsafe)</p>
                    <p>Can request for (0,2,0) by P0 be granted? –NO (Results Unsafe)</p>
                </details>

                <h4>Deadlock Detection:</h4>
                <p>If a system doesn’t employ either a deadlock prevention or deadlock avoidance, then deadlock
                    situation may occur. In this environment the system must provide:</p>
                <ul>
                    <li>An algorithm to recover from the deadlock.</li>
                    <li>An algorithm to remove the deadlock is applied either to a system which pertains single in
                        instance each resource type or a system which pertains several instances of a resource type.
                    </li>
                </ul>
                <strong>Single Instance of each Resource type:</strong>
                <p>If all resources only a single instance then we can define a deadlock detection algorithm which uses
                    a new form of resource allocation graph called “Wait for graph”. We obtain this graph from the
                    resource allocation graph by removing the nodes of type resource and collapsing the appropriate
                    edges.</p>
                <div class="image-container">
                    <img src="placeholder_rag_vs_waitfor_graph.png" alt="Resource Allocation Graph vs Wait-for Graph">
                    <p>Figure: Resource-Allocation Graph (a) and corresponding wait-for graph (b).</p>
                </div>
                <p>For single instance: Pi->Pj (Pi is waiting for Pj to release a resource that Pi needs). Pi->Pj exist
                    if and only if RAG contains 2 edges Pi->Rq and Rq->Pj for some resource Rq.</p>
                <strong>Several Instances of a Resource type:</strong>
                <p>The wait for graph scheme is not applicable to a resource allocation system with multiple instances
                    of each resource type. For this case the algorithm employs several data structures which are similar
                    to those used in the banker’s algorithm like Available, Allocation, Request. The algorithm is
                    similar to the Safety Algorithm in Banker's but checks for deadlock state directly.</p>
                <!-- Steps are similar to Safety Algorithm, but detects deadlock if Finish[i] is false for some process -->

                <h4>RECOVERY FROM DEADLOCK:</h4>
                <p>When a detection algorithm determines that a deadlock exists, then the system or operator is
                    responsible for handling deadlock problem. There are two options for breaking a deadlock.</p>
                <ol>
                    <li>Process Termination</li>
                    <li>Resource preemption</li>
                </ol>
                <strong>Process Termination:</strong>
                <p>There are two method to eliminate deadlocks by terminating a process as follows:</p>
                <ol>
                    <li><strong>Abort all deadlocked processes:</strong> This method will break the deadlock cycle
                        clearly by terminating all process. This method is cost effective. And it removes the partial
                        computations completed by the processes.</li>
                    <li><strong>Abort one process at a time until the deadlock cycle is eliminated:</strong> This method
                        terminates one process at a time, and invokes a deadlock-detection algorithm to determine
                        whether any processes are still deadlocked.</li>
                </ol>
                <strong>Resource Preemption:</strong>
                <p>In resource preemption, the operator or system preempts some resources from processes and give these
                    resources to other processes until the deadlock cycle is broken. If preemption is required to deal
                    with deadlocks, then three issues need to be addressed:</p>
                <ol>
                    <li><strong>Selecting a victim:</strong> The system or operator selects which resources and which
                        processes are to be preempted based on cost factor.</li>
                    <li><strong>Rollback:</strong> The system or operator must roll back the process to some safe state
                        and restart it from that state.</li>
                    <li><strong>Starvation:</strong> The system or operator should ensure that resources will not always
                        be preempted from the same process?</li>
                </ol>
            </section>

            <nav class="topic-navigation">
                <a href="topic-2.html">
                    ← Previous: Process Management
                </a>
                <a href="topic-4.html"> <!-- Assuming topic-4.html is the next one -->
                    Next: Memory Management →
                </a>
            </nav>

        </main>
    </div>

    <footer>
        <p>© 2024 EduVision OS Guide. Gold Standard Interactive Learning.</p>
        <p>Content based on provided materials and AI enhancements for clarity and engagement.</p>
    </footer>

    <script>
        // JavaScript from topic-2.html can be reused here.
        // Ensure sidebar links and section IDs match.

        document.querySelectorAll('#sidebar a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const targetElement = document.querySelector(this.getAttribute('href'));
                if (targetElement) {
                    let headerActualHeight = document.querySelector('.main-header').offsetHeight;
                    if (window.innerWidth <= 768 && getComputedStyle(document.querySelector('.main-header')).position !== 'sticky') {
                        headerActualHeight = 0;
                    }
                    const elementPosition = targetElement.getBoundingClientRect().top + window.pageYOffset;
                    const offsetPosition = elementPosition - headerActualHeight - 20;
                    window.scrollTo({ top: offsetPosition, behavior: 'smooth' });
                }
                document.querySelectorAll('#sidebar a').forEach(link => link.classList.remove('active'));
                this.classList.add('active');
            });
        });

        const sections = document.querySelectorAll('#main-content section');
        const navLi = document.querySelectorAll('#sidebar ul li a');

        window.addEventListener('scroll', () => {
            let headerActualHeight = document.querySelector('.main-header') ? document.querySelector('.main-header').offsetHeight : 0;
            if (window.innerWidth <= 768 && getComputedStyle(document.querySelector('.main-header')).position !== 'sticky') {
                headerActualHeight = 0;
            }
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop - headerActualHeight - 50;
                if (pageYOffset >= sectionTop) { current = section.getAttribute('id'); }
            });
            navLi.forEach(a => {
                a.classList.remove('active');
                if (a.getAttribute('href').substring(1) === current) { a.classList.add('active'); }
            });
            if (!current && navLi.length > 0 && sections.length > 0 && window.pageYOffset < (sections[0].offsetTop - headerActualHeight - 50)) {
                navLi.forEach(a => a.classList.remove('active'));
                navLi[0].classList.add('active');
            }
        });

        // ... (Rest of the JS functions: copyCode, simulators, etc. from topic-2.html)
        function copyCode(button) {
            const pre = button.parentElement.querySelector('pre');
            if (!pre) return;
            const code = pre.innerText;
            navigator.clipboard.writeText(code).then(() => {
                button.innerText = 'Copied!'; button.style.backgroundColor = 'var(--success-color)';
                setTimeout(() => { button.innerText = 'Copy'; button.style.backgroundColor = 'var(--primary-purple)'; }, 2000);
            }).catch(err => {
                console.error('Failed to copy: ', err); button.innerText = 'Error'; button.style.backgroundColor = 'var(--danger-color)';
                setTimeout(() => { button.innerText = 'Copy'; button.style.backgroundColor = 'var(--primary-purple)'; }, 2000);
            });
        }
        // Simulators might not be relevant for this topic, remove if not needed.
        // Command interpreter might also not be relevant here.

        function setInitialActiveLink() {
            const hash = window.location.hash;
            let activeSet = false;
            if (hash && navLi.length > 0) {
                navLi.forEach(a => {
                    if (a.getAttribute('href') === hash) { a.classList.add('active'); activeSet = true; }
                    else { a.classList.remove('active'); }
                });
            }
            if (!activeSet && navLi.length > 0) { navLi[0].classList.add('active'); }
        }
        window.addEventListener('load', setInitialActiveLink);
    </script>
</body>

</html>